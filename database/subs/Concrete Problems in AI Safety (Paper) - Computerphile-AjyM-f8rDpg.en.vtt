WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.160
Today I thought I'd talk about a paper fairly recent.

00:00:03.160 --> 00:00:04.200
It was last year

00:00:04.200 --> 00:00:06.500
A paper called "Concrete Problems in AI Safety"

00:00:06.500 --> 00:00:09.760
Which is going to be related to the stuff I was talking about

00:00:09.760 --> 00:00:13.500
before with the "Stop Button". It's got a bunch of authors; mostly from Google Brain

00:00:14.780 --> 00:00:17.780
Google's AI research department, I guess..

00:00:17.780 --> 00:00:22.940
Well a lot of it's AI research, but
specifically Google Brain and some

00:00:22.949 --> 00:00:26.039
people from Stanford and Berkeley and
opening iEARN. Whatever... it's a

00:00:26.039 --> 00:00:30.890
collaboration between a lot of different
authors

00:00:30.890 --> 00:00:38.059
The idea of the paper is trying to lay out a set
of problems that we are able to

00:00:38.059 --> 00:00:44.180
currently make progress on like if we're
concerned about this far-off sort of

00:00:44.180 --> 00:00:49.670
super intelligence stuff.. Sure; it seems
important and it's interesting and

00:00:49.670 --> 00:00:54.230
difficult and whatever, but it's quite
difficult to sit down and actually do

00:00:54.230 --> 00:00:59.150
anything about it because we don't know
very much about what a super

00:00:59.150 --> 00:01:01.489
intelligence would be like or how it
would be implemented or whatever....

00:01:01.489 --> 00:01:08.740
The idea of this paper is that it... It lays
out some problems that we can tackle now

00:01:08.740 --> 00:01:14.420
which will be helpful now and that I
think will be helpful later on as well

00:01:14.420 --> 00:01:18.880
with more advanced AI systems and making
them safe as well. It lists five problems:

00:01:18.880 --> 00:01:23.360
Avoiding negative side effects, which is
quite closely related to the stuff we've

00:01:23.360 --> 00:01:26.810
been talking about before with the stop
button or the stamp collector. A lot of

00:01:26.810 --> 00:01:31.039
the problems with that can be framed as
negative side effects. They do the thing

00:01:31.040 --> 00:01:34.580
you ask them to but in the process of
doing that they do a lot of things but

00:01:34.580 --> 00:01:37.250
you don't want them to. These are like
the robot running over the baby right?

00:01:37.250 --> 00:01:42.040
Yeah, anything where it does the thing
you wanted it to, like it makes you the

00:01:42.049 --> 00:01:44.990
cup of tea or it collects you stamps or
whatever, but in the process of doing

00:01:44.990 --> 00:01:49.909
that, it also does things you don't want
it to do. So those are your negative side

00:01:49.909 --> 00:01:56.570
effects. So that's the first of the
research areas is how do we avoid these

00:01:56.570 --> 00:02:00.500
negative side effects.. Then there's
avoiding reward hacking, which is about

00:02:00.500 --> 00:02:05.479
systems gaming their reward function. Doing something which technically counts

00:02:05.479 --> 00:02:13.040
but isn't really what you intended the
reward function to be. There's a lot of

00:02:13.040 --> 00:02:16.010
different ways that that can manifest
but this is like this is already a

00:02:16.010 --> 00:02:20.150
common problem in machine learning
systems where you come up with your

00:02:20.150 --> 00:02:23.600
evaluation function or your reward
function or whatever your objective

00:02:23.600 --> 00:02:29.150
function and the system very carefully
optimizes to exactly what you wrote and

00:02:29.150 --> 00:02:33.140
then you realize what you wrote isn't
what you meant. Scalable oversight is the

00:02:33.140 --> 00:02:36.560
next one. It's a problem that human
beings have all the time, anytime you've

00:02:36.560 --> 00:02:41.340
started a new job. You don't know what to
do and you have someone who does who's

00:02:41.340 --> 00:02:45.580
supervising you. The question is what questions do you

00:02:45.580 --> 00:02:49.690
ask and how many questions do you ask
because current machine learning systems

00:02:49.690 --> 00:02:54.040
can learn pretty well if you give them a
million examples but you don't want your

00:02:54.040 --> 00:02:59.800
robot to ask you a million questions, you
know. You want it to only ask a few

00:02:59.800 --> 00:03:03.460
questions and use that information
efficiently to learn from you. Safe

00:03:03.460 --> 00:03:07.990
exploration is the next one which is
about, well, about safely exploring the

00:03:07.990 --> 00:03:13.780
range of possible actions. So, you will
want the system to experiment, you know,

00:03:13.780 --> 00:03:18.040
try different things, try out different
approaches. That's the only way it's

00:03:18.040 --> 00:03:21.280
going to find what's going to work but
there are some things that you don't

00:03:21.280 --> 00:03:25.360
want it to try even once like the baby.
Right, right.. Yeah you don't want it to

00:03:25.360 --> 00:03:30.520
say "What happens if I run over this
baby?" Do you want certain possible things

00:03:30.520 --> 00:03:34.570
that it might consider trying to
actually not try at all because you

00:03:34.570 --> 00:03:37.510
can't afford to have them happen even
once in the real world. Like a

00:03:37.510 --> 00:03:41.680
thermonuclear war option; What happens if
I do this? You don't want it to try that.

00:03:41.680 --> 00:03:46.600
Is that the sort of thing that.. Yeah, yeah..
I'm thinking of war games.. Yes, yeah.. yeah. Global

00:03:46.600 --> 00:03:52.690
Thermal Nuclear War . It runs through a
simulation of every possible type of

00:03:52.690 --> 00:03:57.790
nuclear war, right? But it does it in
simulation. You want your system not to

00:03:57.790 --> 00:04:01.330
run through every possible type of
thermonuclear war in real life to find

00:04:01.330 --> 00:04:06.070
out it doesn't work cause you can't.. It's
too unsafe to do that even once. The last

00:04:06.070 --> 00:04:13.180
area to look into is robustness to
distributional shift. Yeah

00:04:13.180 --> 00:04:18.400
It's a complicated term but the
concept is not. It's just that the

00:04:18.400 --> 00:04:24.580
situation can change over time. So you
may end up; you may make something.

00:04:24.580 --> 00:04:29.860
You train it; it performs well and then
things change to be different from the

00:04:29.860 --> 00:04:35.290
training scenario and that is inherently
very difficult. It's something

00:04:35.290 --> 00:04:38.440
humans struggle with. You
find yourself in a situation you've

00:04:38.440 --> 00:04:42.310
never been in before
but the difference I think or one of the

00:04:42.310 --> 00:04:47.680
useful things that humans do is, notice
that there's a problem a lot of current

00:04:47.680 --> 00:04:51.580
machine learning systems. If
something changes underneath them

00:04:51.580 --> 00:04:57.039
and their training is no longer useful
they have no way of knowing that. So they

00:04:57.039 --> 00:05:00.780
continue being just as confident in
their answers that now make no sense

00:05:00.780 --> 00:05:07.659
because they haven't noticed
that there's a change. So.. if we can't

00:05:07.659 --> 00:05:12.909
make systems that can just react to
completely unforeseen circumstances, we

00:05:12.909 --> 00:05:15.930
may be able to make systems that at
least can recognize that they're in

00:05:15.930 --> 00:05:19.509
unforeseen circumstances and ask for
help and then maybe we have a scalable

00:05:19.509 --> 00:05:23.860
supervision situation there where they
recognize the problem and that's when

00:05:23.860 --> 00:05:28.240
they ask for help. I suppose a simplified
simplistic example of this is when you have

00:05:28.240 --> 00:05:33.250
an out-of-date satnav and it doesn't seem
to realize that you happen to be doing

00:05:33.250 --> 00:05:37.780
70 miles an hour over a plowed field because somebody else, you know, built a

00:05:37.780 --> 00:05:42.280
road there. Yeah, exactly. The general
tendency of unless you program them

00:05:42.280 --> 00:05:46.919
specifically not to; to just plow on with
what they think they should be doing.

00:05:46.919 --> 00:05:54.759
Yeah. It can cause problems and in a large
scale heavily depended on , you know , in

00:05:54.759 --> 00:05:57.789
this case, it's your sat-nav. So it's not
too big of a deal because it's not

00:05:57.789 --> 00:06:01.080
actually driving the car and you know
what's wrong and you can ignore it

00:06:01.080 --> 00:06:04.560
As AI systems become more
important and more integrated into

00:06:04.560 --> 00:06:06.960
everything, that kind of thing, can become
a real problem.

00:06:06.960 --> 00:06:10.040
Although, you would hope the car
doesn't take you in  plowed field in

00:06:10.040 --> 00:06:13.440
first place. Yeah. Is it an open paper or does it

00:06:13.440 --> 00:06:16.000
leave us with any answers? Yeah. So

00:06:16.000 --> 00:06:22.029
the way it does all of these
is it gives a quick outline of what the

00:06:22.029 --> 00:06:27.490
problem is. The example they usually use
is a cleaning robot like we've made this.

00:06:27.490 --> 00:06:32.020
We've made a robot it's in an office or
something and it's cleaning up and then

00:06:32.020 --> 00:06:35.440
they sort of framed the different
problems those things that could go

00:06:35.440 --> 00:06:38.770
wrong in that scenario. So it's pretty
similar to they get me a cup of tea and

00:06:38.770 --> 00:06:43.389
don't run over the baby type set up. It's
clean the office and, you know, not knock

00:06:43.389 --> 00:06:49.979
anything over or destroy anything. And
then, for each one, the paper talks about

00:06:49.979 --> 00:06:54.629
possible approaches to each problem and

00:06:55.020 --> 00:06:59.919
things we can work on, basically. Things
that we don't know how to do yet but

00:06:59.919 --> 00:07:04.850
which seem like they might be doable in
a year or two and some careful thought

00:07:04.850 --> 00:07:10.820
This paper. Is this one for people to read? Yeah, really good. It doesn't cover

00:07:10.820 --> 00:07:15.770
anything like the range of the problems
in AI safety but of the problems

00:07:15.770 --> 00:07:20.000
specifically about avoiding accidents,
because all of these are

00:07:20.000 --> 00:07:25.940
these are ways of creating possible
accidents, right? Possible causes of

00:07:25.940 --> 00:07:29.600
accidents. There's all kinds of other
problems you've been having in AI that

00:07:29.600 --> 00:07:34.430
don't fall under accidents but within
that area I think it covers everything

00:07:34.430 --> 00:07:40.250
and it's quite readable. It's quite... It doesn't
require really high-level because it's

00:07:40.250 --> 00:07:45.590
an overview paper, doesn't require
high-level AI understanding for the most

00:07:45.590 --> 00:07:49.400
part. Anyone can read it and it's on
archive so you know it's freely

00:07:49.400 --> 00:07:53.960
available. These guys now working on AI
safety, or did this then

00:07:53.960 --> 00:07:56.270
They've hung their hat up. They've
written a paper and they're hoping

00:07:56.270 --> 00:07:59.690
someone else is gonna sort it all out. These people are working on AI

00:07:59.690 --> 00:08:05.330
safety right now but they're not the
only people. This paper was released in

00:08:05.330 --> 00:08:11.240
summer of 2016, so it's been about a year
since it came out and since then there

00:08:11.240 --> 00:08:16.099
have been more advances and some of the
problems posed have had really

00:08:16.099 --> 00:08:22.190
interesting solutions or well.. Not
solutions, early work, that looks like it

00:08:22.190 --> 00:08:27.229
could become a solution or approaches
new interesting ideas about ways to

00:08:27.229 --> 00:08:31.970
tackle these problems. So I think as a
paper, it's already been successful in

00:08:31.970 --> 00:08:37.729
stirring new research and giving people
a focus to build their AI safety research on

00:08:37.729 --> 00:08:42.820
top of. So we just need to watch this space, right? Yeah, exactly..

