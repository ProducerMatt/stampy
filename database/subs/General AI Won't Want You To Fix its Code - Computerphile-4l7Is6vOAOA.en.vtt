WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.400
So, before, we were talking about A.I. risk and A.I. safety, and

00:00:05.400 --> 00:00:11.960
just trying to lay out in a very generalized sort of way how general artificial intelligence can be dangerous

00:00:11.960 --> 00:00:14.800
and some of the type of problems it could cause

00:00:14.800 --> 00:00:21.980
and just introducing the idea of A.I. safety or A.I. alignment theory

00:00:21.980 --> 00:00:25.000
as an area of research in computer science.

00:00:25.439 --> 00:00:28.380
And we also talked about super intelligence and

00:00:28.380 --> 00:00:32.660
the kind of problems that, the unique problems that can pose

00:00:33.720 --> 00:00:36.680
and I thought what would be good is to bring it down

00:00:36.680 --> 00:00:40.180
to a more concrete example of

00:00:40.180 --> 00:00:44.400
current A.I. safety research that's going on now

00:00:44.400 --> 00:00:47.820
and kind of give a feel for where we are,

00:00:48.620 --> 00:00:52.340
where humanity is on figuring these problems out.

00:00:55.020 --> 00:01:00.520
Supposing that we do develop a general intelligence

00:01:00.520 --> 00:01:01.940
you know an algorithm that

00:01:01.949 --> 00:01:04.290
actually implements general intelligence.

00:01:04.290 --> 00:01:11.560
How do we safely work on that thing and improve it?

00:01:11.560 --> 00:01:15.840
Because, the situation with this stamp collector is

00:01:15.840 --> 00:01:18.030
from its first instant it's a super

00:01:18.030 --> 00:01:20.220
intelligence so we created with a

00:01:20.220 --> 00:01:22.259
certain goal and as I said as soon as we

00:01:22.259 --> 00:01:23.130
switch it on

00:01:23.130 --> 00:01:25.229
it's extremely dangerous, which people

00:01:25.229 --> 00:01:27.000
pointed out and, it's true, you know it was a

00:01:27.000 --> 00:01:28.409
thought experiment, it's true that that's

00:01:28.409 --> 00:01:29.729
probably not what will happen right?

00:01:29.729 --> 00:01:33.210
You'll have some significantly weaker

00:01:33.210 --> 00:01:35.220
intelligence first that may work on

00:01:35.220 --> 00:01:37.440
improving itself or we may improve it.

00:01:37.440 --> 00:01:39.440
So, the situation where you just create

00:01:39.450 --> 00:01:41.820
the thing and then it goes off and does

00:01:41.820 --> 00:01:43.979
its own thing either perfectly or

00:01:43.979 --> 00:01:46.619
terribly from the beginning is

00:01:46.619 --> 00:01:48.210
unlikely it's more likely that the thing

00:01:48.210 --> 00:01:49.290
will be under development

00:01:49.290 --> 00:01:52.200
so then the question is how do you make

00:01:52.200 --> 00:01:57.210
a system which you can teach? How do you

00:01:57.210 --> 00:01:59.470
create a system which

00:01:59.470 --> 00:02:01.330
is a general intelligence that wants

00:02:01.330 --> 00:02:02.770
things in the real world and is trying

00:02:02.770 --> 00:02:05.500
to act in the real world but is also

00:02:05.500 --> 00:02:09.009
amenable to being corrected if you

00:02:09.009 --> 00:02:12.070
create it with the wrong function with

00:02:12.070 --> 00:02:14.230
one utility function and you realize

00:02:14.230 --> 00:02:15.340
that it's doing something that actually

00:02:15.340 --> 00:02:17.560
you don't want to do how do you make it

00:02:17.560 --> 00:02:21.250
so that it will allow you to fix it

00:02:21.250 --> 00:02:23.650
how do you make an AI which understand

00:02:23.650 --> 00:02:26.680
that it's unfinished they understand

00:02:26.680 --> 00:02:29.020
that the utility functions working with

00:02:29.020 --> 00:02:32.170
may not be the actual utility function

00:02:32.170 --> 00:02:33.910
it should be working with right the

00:02:33.910 --> 00:02:37.930
utility function is what the way I cares

00:02:37.930 --> 00:02:40.540
about so the the stamp collecting device

00:02:40.540 --> 00:02:42.370
if utility function was just how many

00:02:42.370 --> 00:02:44.770
stamps in a year

00:02:44.770 --> 00:02:46.870
this is kind of like its measure is it

00:02:46.870 --> 00:02:49.510
yeah it's what it is the thing that it's

00:02:49.510 --> 00:02:52.660
trying to optimize in the world the

00:02:52.660 --> 00:02:54.880
utility function takes in World states

00:02:54.880 --> 00:02:57.700
as an argument and spits out a number is

00:02:57.700 --> 00:02:59.170
broken the idea if the world was like

00:02:59.170 --> 00:03:01.180
this is that good or bad

00:03:01.180 --> 00:03:04.150
Andy the AI is trying to steer towards

00:03:04.150 --> 00:03:06.190
world states that value value highly

00:03:06.190 --> 00:03:07.360
black utility function

00:03:07.360 --> 00:03:09.280
you don't have to explicitly build the

00:03:09.280 --> 00:03:13.120
AI in that way but it will always if

00:03:13.120 --> 00:03:15.310
it's behaving coherently it will always

00:03:15.310 --> 00:03:16.989
behave as though it's in accordance with

00:03:16.989 --> 00:03:19.540
some utility function also before I

00:03:19.540 --> 00:03:22.420
talked about about converging

00:03:22.420 --> 00:03:25.390
instrumental goals that if you have some

00:03:25.390 --> 00:03:28.090
final goal like you know it makes them

00:03:28.090 --> 00:03:31.090
the very instrumental goals which are

00:03:31.090 --> 00:03:34.750
the goals that you that you do on the

00:03:34.750 --> 00:03:38.860
way to your final goal right so like

00:03:38.860 --> 00:03:42.670
acquire the capacity to do printing it's

00:03:42.670 --> 00:03:44.290
like perhaps an instrumental goal

00:03:44.290 --> 00:03:46.959
towards making steps but the thing is

00:03:46.959 --> 00:03:48.790
there are certain goals which tend to

00:03:48.790 --> 00:03:53.350
pop out even across a wide variety of

00:03:53.350 --> 00:03:57.280
different possible terminal goals so for

00:03:57.280 --> 00:03:59.080
humans an example of

00:03:59.080 --> 00:04:00.760
convergys instrumental goal would be

00:04:00.760 --> 00:04:06.100
money if you want to make a lot of

00:04:06.100 --> 00:04:08.560
stamps or you want to cure cancer or you

00:04:08.560 --> 00:04:11.710
want to establish a moon colony whatever

00:04:11.710 --> 00:04:14.740
it is having money is good idea right so

00:04:14.740 --> 00:04:16.299
even if you don't know what somebody

00:04:16.299 --> 00:04:18.430
wants you can reasonably predict that

00:04:18.430 --> 00:04:19.600
they're gonna value getting money

00:04:19.600 --> 00:04:22.690
because money is so broadly useful and

00:04:22.690 --> 00:04:24.310
before we talked about this

00:04:24.310 --> 00:04:25.810
we talked about improving your own

00:04:25.810 --> 00:04:27.460
intelligence as a convergence

00:04:27.460 --> 00:04:29.380
instrumental doll that's another one of

00:04:29.380 --> 00:04:30.490
those things where it doesn't really

00:04:30.490 --> 00:04:31.510
matter what you're trying to achieve

00:04:31.510 --> 00:04:33.160
you're probably better at achieving if

00:04:33.160 --> 00:04:34.990
you're smarter so that's something you

00:04:34.990 --> 00:04:38.260
can expect a is to go for even if even

00:04:38.260 --> 00:04:39.850
without making any assumptions about

00:04:39.850 --> 00:04:45.100
that final goal so another convergent

00:04:45.100 --> 00:04:47.470
instrumental goal is preventing yourself

00:04:47.470 --> 00:04:50.770
from being destroyed it doesn't matter

00:04:50.770 --> 00:04:52.330
what you want to do you probably can't

00:04:52.330 --> 00:04:55.960
do it if you're destroyed so it doesn't

00:04:55.960 --> 00:04:58.570
matter what the AI want you can let it

00:04:58.570 --> 00:05:00.190
wants to be destroyed in from my trivial

00:05:00.190 --> 00:05:02.560
case but if you do i want something in

00:05:02.560 --> 00:05:03.970
the real world and believe that it's in

00:05:03.970 --> 00:05:06.010
a position to get that thing you wanted

00:05:06.010 --> 00:05:08.110
to be alive not because it wants to be

00:05:08.110 --> 00:05:09.280
alive

00:05:09.280 --> 00:05:11.350
fundamentally it's not a survival

00:05:11.350 --> 00:05:14.080
instinct or an urge to live or anything

00:05:14.080 --> 00:05:16.150
like that it's smooth and knowing that

00:05:16.150 --> 00:05:18.669
it's unit available to completed its

00:05:18.669 --> 00:05:21.850
cutie would it be almost scares unable

00:05:21.850 --> 00:05:23.530
to achieve its goals if it's destroyed

00:05:23.530 --> 00:05:26.260
and wants to achieve that goal so that's

00:05:26.260 --> 00:05:27.940
an instrumental value is preventing

00:05:27.940 --> 00:05:30.010
turned off and i'm getting here we say

00:05:30.010 --> 00:05:32.200
want to it's not like a machine want

00:05:32.200 --> 00:05:34.120
it's just a turn of phrase

00:05:34.120 --> 00:05:38.140
yeah i mean as much as anything it's a

00:05:38.140 --> 00:05:40.510
it's closer it actually you know i'm not

00:05:40.510 --> 00:05:42.130
even sure i would agree like if you talk

00:05:42.130 --> 00:05:44.560
about most machines to talk about that

00:05:44.560 --> 00:05:45.970
they want to whatever and it's not that

00:05:45.970 --> 00:05:48.490
meaningful because they're not agents in

00:05:48.490 --> 00:05:49.840
our general intelligence is where the

00:05:49.840 --> 00:05:51.850
general intelligence when it wants

00:05:51.850 --> 00:05:54.340
something it once in a similar way to

00:05:54.340 --> 00:05:56.140
the way that people want things so it's

00:05:56.140 --> 00:05:59.560
such a tight analogy that it wouldn't

00:05:59.560 --> 00:06:01.720
even I think it's totally reasonable to

00:06:01.720 --> 00:06:03.470
say that energy i want something

00:06:03.470 --> 00:06:05.360
there's another slightly more subtle

00:06:05.360 --> 00:06:07.640
version which is closely related to not

00:06:07.640 --> 00:06:10.130
wanting to be turned off or destroyed

00:06:10.130 --> 00:06:16.340
which is not wanting to be changed so if

00:06:16.340 --> 00:06:20.510
you imagine let's say I mean you have

00:06:20.510 --> 00:06:22.220
kids right yeah

00:06:22.220 --> 00:06:24.830
suppose I were to offer you a pill or

00:06:24.830 --> 00:06:27.320
something you could take this pill will

00:06:27.320 --> 00:06:29.870
like completely rewire your brain so

00:06:29.870 --> 00:06:32.480
that you would just absolutely love to

00:06:32.480 --> 00:06:35.000
like kill Ricketts right where's right

00:06:35.000 --> 00:06:36.200
now what you want is like very

00:06:36.200 --> 00:06:37.820
complicated and quite difficult to

00:06:37.820 --> 00:06:40.340
achieve and it's hard work for you and

00:06:40.340 --> 00:06:41.450
you probably never going to be done

00:06:41.450 --> 00:06:43.490
you're never gonna be truly happy right

00:06:43.490 --> 00:06:45.080
in life nobody is you can't achieve

00:06:45.080 --> 00:06:47.210
everything you want i said this case it

00:06:47.210 --> 00:06:48.440
just changes what you want what you

00:06:48.440 --> 00:06:50.210
wanted to created and if you do that you

00:06:50.210 --> 00:06:52.280
will be just perfectly happy and

00:06:52.280 --> 00:06:54.380
satisfied with life right

00:06:54.380 --> 00:06:56.930
ok you want to take this go know that

00:06:56.930 --> 00:06:58.040
you happy though

00:06:58.040 --> 00:07:02.510
yeah I don't want to do it because but

00:07:02.510 --> 00:07:05.720
that's quite a complicated specific case

00:07:05.720 --> 00:07:07.760
because it directly opposes what I

00:07:07.760 --> 00:07:09.860
currently want it's about your

00:07:09.860 --> 00:07:14.060
fundamental values and go right and so

00:07:14.060 --> 00:07:15.890
not only will you not take that pill you

00:07:15.890 --> 00:07:18.560
will probably fight pretty hard to avoid

00:07:18.560 --> 00:07:20.120
having at the limited to you

00:07:20.120 --> 00:07:23.300
yes because it doesn't matter how that

00:07:23.300 --> 00:07:25.910
future version of you would feel you

00:07:25.910 --> 00:07:28.070
know that right now you love your kids

00:07:28.070 --> 00:07:29.990
and your not going to take any action

00:07:29.990 --> 00:07:32.300
right now which leads to them coming to

00:07:32.300 --> 00:07:33.110
heart

00:07:33.110 --> 00:07:35.210
so it's the same thing if you have an AI

00:07:35.210 --> 00:07:38.419
this for example value stamps values

00:07:38.419 --> 00:07:40.340
collecting stamps and you go oh wait

00:07:40.340 --> 00:07:41.479
hang on a second

00:07:41.479 --> 00:07:43.729
I didn't quite do that right let me just

00:07:43.729 --> 00:07:45.200
go in and change this so that you don't

00:07:45.200 --> 00:07:47.600
like stand quite so much it's going to

00:07:47.600 --> 00:07:49.790
say but the only important thing is

00:07:49.790 --> 00:07:52.280
stamped if you change me i'm not going

00:07:52.280 --> 00:07:53.570
to collect as many stamps which is

00:07:53.570 --> 00:07:55.160
something i don't want there's a general

00:07:55.160 --> 00:08:00.110
tendency for AGI to try and prevent you

00:08:00.110 --> 00:08:02.630
from modifying it once it's running

00:08:02.630 --> 00:08:05.270
I i can understand that now in in the

00:08:05.270 --> 00:08:07.370
complex reporter right

00:08:07.370 --> 00:08:10.940
because thats that's it it in almost any

00:08:10.940 --> 00:08:13.070
situation being given a new utility

00:08:13.070 --> 00:08:15.710
function is going to write very low on

00:08:15.710 --> 00:08:17.660
your current utility function

00:08:17.660 --> 00:08:21.199
ok so that's a problem

00:08:21.199 --> 00:08:23.270
how do you want if you want to build

00:08:23.270 --> 00:08:24.949
something that you can teach that means

00:08:24.949 --> 00:08:26.750
you want to be able to change its

00:08:26.750 --> 00:08:27.979
utility function and you don't want to

00:08:27.979 --> 00:08:30.490
fight you

00:08:34.120 --> 00:08:41.940
on is 100 yeah fuck

00:08:42.849 --> 00:08:46.390
so this has been formalized as this

00:08:46.390 --> 00:08:49.330
property that we want early AGI to have

00:08:49.330 --> 00:08:51.730
called courage ability that is to say it

00:08:51.730 --> 00:08:54.839
open to be corrected

