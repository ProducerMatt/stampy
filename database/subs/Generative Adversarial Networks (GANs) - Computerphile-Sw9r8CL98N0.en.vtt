WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:05.940
So today, I thought we talk about generative adversarial networks because they're really cool, and they've

00:00:07.330 --> 00:00:10.950
They can do a lot of really cool things people have used them for all kinds of things

00:00:11.290 --> 00:00:13.830
Things like you know you draw a sketch of a shoe

00:00:13.830 --> 00:00:16.469
And it will render you an actual picture of a shoe or a handbag

00:00:16.930 --> 00:00:21.689
They're fairly low-resolution right now, but it's very impressive the way that they can produce

00:00:22.449 --> 00:00:24.539
real quite good-looking images

00:00:27.699 --> 00:00:29.130
You could make a neural network

00:00:29.130 --> 00:00:35.460
That's a classifier right you give it lots and lots of pictures of cats and lots and lots of pictures of dogs

00:00:35.460 --> 00:00:39.180
and you say you know you present it with a picture of a cat and

00:00:39.910 --> 00:00:43.680
It says it outputs a number. Let's say between zero and one

00:00:44.289 --> 00:00:45.309
and

00:00:45.309 --> 00:00:50.969
Zero represents cats and one represents dogs and so you give it a cat and it puts out one and you say no

00:00:50.969 --> 00:00:55.349
That's not right should be zero and you keep training it until eventually it can tell the difference right?

00:00:55.870 --> 00:00:57.219
so

00:00:57.219 --> 00:00:59.219
somewhere inside that

00:01:01.090 --> 00:01:02.350
Network

00:01:02.350 --> 00:01:08.129
It's... it must have formed some model of what cats are and what dogs are, at least as far as images of

00:01:08.260 --> 00:01:10.260
images of them are concerned

00:01:11.110 --> 00:01:12.220
But

00:01:12.220 --> 00:01:15.689
That model really... you can only really use it to classify things

00:01:15.689 --> 00:01:19.979
You can't say "ok draw me a new cat picture", "draw me a cat picture I haven't seen before"

00:01:21.520 --> 00:01:27.600
It doesn't know how to do that so quite often you want a model that can generate new

00:01:28.270 --> 00:01:34.560
Samples you have so you give it a bunch of samples from a particular distribution, and you want it to

00:01:35.110 --> 00:01:40.079
Give you more samples which are also from that same distribution, so it has to learn the underlying

00:01:40.990 --> 00:01:44.699
Structure of what you've given it. And that's kind of tricky, actually.

00:01:46.390 --> 00:01:48.390
There's a lot of...

00:01:49.990 --> 00:01:52.307
Well there's a lot of challenges involved in that.

00:01:52.307 --> 00:01:53.280
Well, let's be honest

00:01:53.280 --> 00:01:55.500
I don't think as a human you can find that tricky

00:01:55.500 --> 00:02:00.569
You know if... if I know what a cat looks like but, uh, being not the greatest artist in the world

00:02:00.759 --> 00:02:06.569
I'm not sure that I could draw you a decent cat. So, you know, that this is not confined to just

00:02:06.880 --> 00:02:07.720
Computing is it? This...

00:02:07.720 --> 00:02:09.869
Yeah, that's true. That's really true.

00:02:10.929 --> 00:02:12.929
but if you take

00:02:14.460 --> 00:02:17.940
Let's do like a really simple example of a generative model

00:02:17.940 --> 00:02:21.180
say you you give your network one thing

00:02:21.180 --> 00:02:21.940
It looks like this.

00:02:21.940 --> 00:02:25.889
And then you give it another one you're like these are your training samples looks like this

00:02:26.110 --> 00:02:29.449
You give it another one that looks like this, and then...

00:02:29.449 --> 00:02:31.499
What are those dots in the systems?

00:02:33.120 --> 00:02:35.100
Instances of something on two dimensions?

00:02:35.100 --> 00:02:38.360
Yeah, I mean right now, it's literally just data. We just... it doesn't matter what it is

00:02:38.440 --> 00:02:40.740
Just some... yeah, these are these are data points

00:02:41.560 --> 00:02:45.630
And so these are the things you're giving it, and then it will learn

00:02:46.660 --> 00:02:51.510
You can train it. It will learn a model, and the model it might learn is something like this, right?

00:02:52.050 --> 00:02:58.079
It's figured out that these dots all lie along a path, and if its model was always to draw a line

00:02:58.180 --> 00:03:01.139
Then it could learn by adjusting the parameters of that line

00:03:01.140 --> 00:03:05.910
It would move the line around until it found a line that was a good fit, and generally gave you a good prediction.

00:03:06.910 --> 00:03:08.939
But then if you were to ask this model:

00:03:10.060 --> 00:03:11.780
"Okay, now make me a new one"

00:03:11.880 --> 00:03:18.060
unless you did something clever, what you get is probably this, because that is on average

00:03:18.060 --> 00:03:22.140
The closest to any of these, because any of these dots you don't know if they're going to be above or below

00:03:22.140 --> 00:03:25.619
or, you know, to the left or the right. There's no pattern there. It's kind of random.

00:03:26.410 --> 00:03:32.730
So the best place you can go that will minimize your error, is to go just right on the line every time.

00:03:33.310 --> 00:03:36.330
But anybody looking at this will say: "well, that's fake"

00:03:36.330 --> 00:03:41.160
That's not a plausible example of something from this distribution, even though for a lot of the

00:03:41.830 --> 00:03:47.700
like, error functions, that people use when training networks this would perform best, so it's this interesting situation where

00:03:48.700 --> 00:03:50.700
There's not just one right answer.

00:03:51.010 --> 00:03:53.844
you know, generally speaking the way that neuron networks work is:

00:03:53.844 --> 00:03:56.849
you're training them towards a specific you have a label or you have a

00:03:57.100 --> 00:03:59.879
you have an output a target output and

00:04:00.520 --> 00:04:06.029
You get penalty the further away you are from that output, whereas in in a in an application like this

00:04:06.880 --> 00:04:10.889
There's effect... there's basically an infinite number of perfectly valid

00:04:12.580 --> 00:04:13.480
Outputs here

00:04:13.480 --> 00:04:21.299
But, so, to generate this what you actually need is to take this model and then apply some randomness, you say: "they're all

00:04:21.640 --> 00:04:23.640
Within, you know,

00:04:25.070 --> 00:04:30.200
They occur randomly and they're normally distributed around this line with this standard deviation" or whatever.

00:04:30.270 --> 00:04:33.409
But a lot of models would have a hard time actually

00:04:33.930 --> 00:04:36.260
picking one of all of the possibilities

00:04:36.260 --> 00:04:41.089
And they would have this tendency to kind of smooth things out and go for the average, whereas we actually just want

00:04:41.670 --> 00:04:45.319
"Just pick me one doesn't matter". So that's part of the problem of generating.

00:04:46.020 --> 00:04:48.049
Adversarial training is is help is a way of

00:04:49.080 --> 00:04:51.080
training

00:04:52.440 --> 00:04:55.429
Not just networks, actually, a way of training machine learning systems.

00:04:57.210 --> 00:04:58.350
Which

00:04:58.350 --> 00:05:00.350
involves focusing on

00:05:00.750 --> 00:05:02.160
the system's weaknesses.

00:05:02.160 --> 00:05:07.369
So, if you are learning... let's say you're teaching your

00:05:08.010 --> 00:05:11.120
Network to recognize handwritten digits.

00:05:11.120 --> 00:05:14.960
The normal way you would do that you have your big training sample of labeled samples

00:05:15.180 --> 00:05:19.560
You've got an array of pixels that looks like a three and then it's labeled with three and so on.

00:05:19.560 --> 00:05:20.540
And the normal way

00:05:20.540 --> 00:05:23.480
that you would train a network with this is you would just

00:05:24.210 --> 00:05:30.349
Present all of them pretty much at random. You'd present as many ones as two as threes and just keep throwing examples at it

00:05:30.420 --> 00:05:34.129
"What's this?", you know, "Yes, you got that right", "no. You've got that wrong, It should really be this".

00:05:35.700 --> 00:05:38.929
And keep doing that and the system will eventually learn

00:05:39.930 --> 00:05:40.980
but

00:05:40.980 --> 00:05:45.259
If you were actually teaching a person to recognize the numbers, if you were teaching a child

00:05:45.840 --> 00:05:50.090
you wouldn't do that, like, if you'd been teaching them for a while, presenting them and

00:05:50.610 --> 00:05:55.460
You know, getting the response and correcting them and so on, and you noticed that they can do...

00:05:55.950 --> 00:06:02.029
you know... with 2 3 4 5 6 8 &amp; 9 they're getting like 70 80 percent

00:06:02.030 --> 00:06:04.030
You know, accuracy recognition rate.

00:06:04.230 --> 00:06:10.039
But 1 &amp; 7 it's like 50/50, because any time they get a 1 or a 7 they just guess because they can't

00:06:10.040 --> 00:06:11.560
Tell the difference between them.

00:06:11.560 --> 00:06:16.280
If you noticed that you wouldn't keep training those other numbers, right? You would stop and say:

00:06:16.280 --> 00:06:19.339
"Well, You know what? we're just gonna focus on 1 &amp; 7 because this is an issue for you".

00:06:19.890 --> 00:06:23.390
"I'm gonna keep showing you Ones and 7s and correcting you until

00:06:23.910 --> 00:06:29.809
The error rate on ones and 7s comes down to the error rate that you're getting on your other numbers".

00:06:29.810 --> 00:06:34.130
You're focusing the training on the area where the student is failing and

00:06:34.650 --> 00:06:36.949
there's kinda of a balance there when you're teaching humans

00:06:37.560 --> 00:06:42.980
because if you keep relentlessly focusing on their weaknesses and making them do stuff they can't do all the time

00:06:43.320 --> 00:06:49.480
They will just become super discouraged and give up. But neural networks don't have feelings yet, so that's really not an issue.

00:06:49.480 --> 00:06:50.500
You can just

00:06:51.150 --> 00:06:53.150
continually hammer on the weak points

00:06:53.880 --> 00:06:58.580
Find whatever they're having trouble with and focus on that. And so, that behavior,

00:06:58.580 --> 00:07:01.279
and I think some people have had teachers where it feels like this,

00:07:01.800 --> 00:07:05.359
It feels like an adversary, right? it feels like they want you to fail.

00:07:07.320 --> 00:07:08.910
So in fact

00:07:08.910 --> 00:07:14.270
you can make them an actual adversary. If you have some process which is genuinely

00:07:14.670 --> 00:07:19.159
Doing its best to make the network give as high an error as possible

00:07:19.440 --> 00:07:24.290
that will produce this effect where if it spots any weakness it will focus on that and

00:07:24.840 --> 00:07:27.200
Thereby force the learner

00:07:27.510 --> 00:07:33.170
To learn to not have that weakness anymore. Like one form of adversarial training people sometimes

00:07:33.170 --> 00:07:38.270
Do is if you have a game playing program you make it play itself a lot of times

00:07:38.490 --> 00:07:45.410
Because all the time. They are trying to look for weaknesses in their opponent and exploit those weaknesses and when they do that

00:07:45.750 --> 00:07:53.119
They're forced to then improve or fix those weaknesses in themselves because their opponent is exploiting those weaknesses, so

00:07:55.140 --> 00:07:56.730
Every time

00:07:56.730 --> 00:07:58.200
the

00:07:58.200 --> 00:08:02.749
Every time the system finds a strategy that is extremely good against this opponent

00:08:03.660 --> 00:08:11.089
The the opponent, who's also them, has to learn a way of dealing with that strategy. And so on and so on.

00:08:11.910 --> 00:08:16.100
So, as the system gets better it forces itself to get better

00:08:18.090 --> 00:08:21.950
Because it's continuously having to learn how to play a better and better opponent

00:08:22.980 --> 00:08:25.400
It's quite elegant, you know.

00:08:25.400 --> 00:08:28.180
This is where we get to generative adversarial. Networks. Let's say

00:08:28.890 --> 00:08:31.339
You've got a network you want to...

00:08:33.300 --> 00:08:34.919
Let's say you want cat pictures

00:08:34.919 --> 00:08:40.129
You know, you want to be able to give it a bunch of pictures of cats and have it

00:08:40.620 --> 00:08:44.729
Spit out a new picture of a cat that you've never seen before that looks exactly like a cat

00:08:44.729 --> 00:08:45.979
the way that the generative

00:08:46.320 --> 00:08:53.090
adversarial network works is it's this architecture where you actually have two networks one of the networks is the discriminator

00:08:53.580 --> 00:08:55.410
How's my spelling?

00:08:55.410 --> 00:08:56.850
Yeah, like that

00:08:56.850 --> 00:09:01.790
The discriminator Network is a classifier right it's a straightforward classifier

00:09:02.100 --> 00:09:03.290
You give it an image

00:09:03.290 --> 00:09:10.129
And it outputs a number between 0 &amp; 1 and your training that in standard supervised learning way

00:09:11.130 --> 00:09:13.939
Then you have a generator and the generator

00:09:15.540 --> 00:09:17.540
Is...

00:09:17.700 --> 00:09:21.950
Usually a convolutional neural network, although actually both of these can be other processes

00:09:21.950 --> 00:09:23.944
But people tend to use in your networks for this.

00:09:23.944 --> 00:09:24.799
And the generator, you

00:09:25.380 --> 00:09:29.390
give it some random noise, and that's the random,

00:09:29.400 --> 00:09:31.639
that's where it gets its source of randomness, so

00:09:31.860 --> 00:09:35.360
That it can give multiple answers to the same question effectively.

00:09:35.360 --> 00:09:38.360
You give it some random noise and it generates an image

00:09:38.970 --> 00:09:43.249
From that noise and the idea is it's supposed to look like a cat

00:09:43.250 --> 00:09:51.020
So the way that we do this with a generative adversarial Network is it's this architecture whereby you have two networks

00:09:51.570 --> 00:09:53.220
Playing a game

00:09:53.220 --> 00:09:57.160
Effectively it's a competitive game. It's adversarial between them and in fact

00:09:57.560 --> 00:10:03.880
It's a very similar to the games we talked about in the  Alpha go video.

00:10:03.881 --> 00:10:05.560
it's a min/max game

00:10:05.850 --> 00:10:09.140
Because these two networks are fighting over one number

00:10:09.960 --> 00:10:13.519
one of them wants the number to be high one of them wants the number to be low.

00:10:14.730 --> 00:10:19.040
And what that number actually is is the error rate of the discriminator?

00:10:19.890 --> 00:10:21.890
so

00:10:22.140 --> 00:10:23.970
The discriminator

00:10:23.970 --> 00:10:29.629
Wants a low error rate the generator wants a high error rate the discriminators job is to look at an image

00:10:30.210 --> 00:10:33.019
which could have come from the original data set or

00:10:34.050 --> 00:10:41.090
It could have come from the generator and its job is to say yes. This is a real image or no. This is a fake

00:10:42.240 --> 00:10:47.750
any outputs a number between 0 &amp; 1 like 1 for its real and 0 for its fake for example and

00:10:48.510 --> 00:10:50.280
the generator

00:10:50.280 --> 00:10:56.479
Gets fed as its input. Just some random noise and it then generates an image from that and

00:10:57.270 --> 00:10:58.530
it's

00:10:58.530 --> 00:11:00.530
Reward you know it's training is

00:11:01.200 --> 00:11:04.249
Pretty much the inverse of what the discriminator says

00:11:04.860 --> 00:11:07.610
for that image so if it produces an image

00:11:08.130 --> 00:11:13.459
Which the discriminator can immediately tell this fake? It gets a negative reward you know it's a

00:11:14.070 --> 00:11:18.349
That's it's trained not to do that if it manages to produce an image that the discriminator

00:11:18.899 --> 00:11:21.078
Can't tell is fake

00:11:21.630 --> 00:11:29.269
Then that's really good so you train them in a inner cycle effectively you you give the discriminator a real image

00:11:29.820 --> 00:11:35.299
get its output, then you generate a fake image and get the discriminator that and

00:11:36.180 --> 00:11:40.579
Then you give it a real so the discriminator gets alternating real image fake image real image fake image

00:11:41.250 --> 00:11:43.250
usually I mean there are things you can do where you

00:11:43.709 --> 00:11:49.789
Train them at different rates and whatever but by default they're generally to get any help with this at all, or is it purely

00:11:50.430 --> 00:11:55.669
Yes, so if you this is this is like part of what makes this especially clever actually

00:11:56.190 --> 00:11:58.549
the generator does get help because

00:11:59.100 --> 00:12:00.300
if

00:12:00.300 --> 00:12:05.120
You set up the networks right you can use the gradient of the discriminator

00:12:06.570 --> 00:12:08.599
to train the generator

00:12:09.870 --> 00:12:11.250
so when I

00:12:11.250 --> 00:12:17.510
Know you done back propagation before about how neural networks are trained its gradient descent right and in fact we talked about this in like

00:12:17.910 --> 00:12:19.910
2014 sure if you were a

00:12:21.209 --> 00:12:26.479
You're a blind person climbing a mountain or you're it's really foggy, and you're climbing a mountain you can only see directly

00:12:26.480 --> 00:12:28.480
What's underneath your own feet?

00:12:29.430 --> 00:12:35.239
You can still climb that mountain if you just follow the gradient you just look directly under me which way is the

00:12:35.819 --> 00:12:40.549
You know which way is the ground sloping? This is what we did the hill climb algorithm exactly

00:12:40.550 --> 00:12:44.630
Yeah, sometimes people call it hill climbing sometimes people call it gradient descent

00:12:45.149 --> 00:12:46.829
It's the same

00:12:46.829 --> 00:12:48.269
metaphor

00:12:48.269 --> 00:12:54.529
Upside down effectively if we're climbing up or we're climbing down you're training them by gradient descent, which means that

00:12:55.620 --> 00:12:58.279
You're not just you're not just able to say

00:12:59.699 --> 00:13:01.440
Yes, that's good. No. That's bad

00:13:01.440 --> 00:13:08.479
You're actually able to say and you should adjust yours you should adjust your weights in this direction so that you'll move down the gradient

00:13:08.850 --> 00:13:10.139
right

00:13:10.139 --> 00:13:15.379
So generally you're trying to move down the gradient of error for the network

00:13:16.589 --> 00:13:23.029
If you're like if you're training if your training the thing to just recognize cats and dogs you're just moving it

00:13:23.029 --> 00:13:28.308
You're moving it down the gradient towards the correct label whereas in this case

00:13:30.059 --> 00:13:32.418
The generator is being moved

00:13:33.869 --> 00:13:38.149
sort of up the gradient for the discriminators error

00:13:38.819 --> 00:13:42.319
So it can find out not just you did well you did badly

00:13:42.319 --> 00:13:47.538
But here's how to tweak your weights so that you will so that the discriminator would have been more wrong

00:13:48.629 --> 00:13:52.728
So so that you can confuse the discriminator more so you can think of this whole thing?

00:13:54.289 --> 00:13:59.299
An analogy people sometimes use is like a a forger and

00:14:00.959 --> 00:14:06.768
An expert investigator person right at the beginning, you know let's assume

00:14:06.769 --> 00:14:10.099
There's one forger in there's one investigator and all of the art

00:14:10.769 --> 00:14:15.469
buyers of the world are idiots at the beginning the the

00:14:18.089 --> 00:14:21.948
Level of the the quality of the forgeries is going to be quite low right the guy

00:14:21.949 --> 00:14:27.019
Just go get some paint, and he he then he just writes you know Picasso on it

00:14:27.019 --> 00:14:31.519
And he can sell it for a lot of money and the investigator comes along and says yeah

00:14:31.519 --> 00:14:35.388
I do I don't know that's right or maybe it is. I'm not sure I haven't really figured it out

00:14:36.079 --> 00:14:41.659
And then as time goes on the investigator who's the discriminator will?

00:14:42.029 --> 00:14:47.838
Start to spot certain things that are different between the things that the forger produces and real paintings

00:14:47.839 --> 00:14:51.229
And then they'll start to be able to reliably spot. Oh, this is a fake

00:14:51.439 --> 00:14:53.808
You know this uses the wrong type of paint or whatever

00:14:53.809 --> 00:15:01.038
So it's fake and once that happens the forger is forced to get better right you can't sell his fakes anymore

00:15:01.039 --> 00:15:03.049
He has to find that kind of paint

00:15:03.329 --> 00:15:04.739
So he goes and you know

00:15:04.739 --> 00:15:09.019
Digs up Egyptian mummies or whatever to get the legit paint and now he can forge again

00:15:09.019 --> 00:15:11.808
and now of the discriminator the investigator is fooled and

00:15:12.869 --> 00:15:14.988
They have to find a new thing

00:15:15.179 --> 00:15:19.969
That distinguishes the real from the fakes and so on and so on in a cycle they force each other to improve

00:15:19.979 --> 00:15:22.219
And it's the same thing here

00:15:23.129 --> 00:15:29.658
So at the beginning the generator is making just random noise basically because it's it's it's getting random noisy

00:15:29.659 --> 00:15:35.899
And it's doing something to it who knows what and it spits out an image and the discriminator goes that looks nothing like a cat

00:15:36.120 --> 00:15:37.650
you know

00:15:37.650 --> 00:15:39.150
and

00:15:39.150 --> 00:15:40.770
then

00:15:40.770 --> 00:15:44.660
eventually because the discriminator is also not very smart at the beginning right and

00:15:45.450 --> 00:15:47.629
And they just they both get better and better

00:15:48.000 --> 00:15:54.530
The generator gets better at producing cat looking things and the discriminator gets better and better at identifying them

00:15:55.770 --> 00:16:02.179
until eventually in principle if you run this for long enough theoretically you end up with a situation where the

00:16:02.820 --> 00:16:05.840
Generator is creating images that look exactly

00:16:07.470 --> 00:16:09.470
Indistinguishable from

00:16:10.260 --> 00:16:17.960
Images from the real data set and the discriminator if it's given a real image, or a fake image always outputs 0.5

00:16:19.230 --> 00:16:20.730
5050 I

00:16:20.730 --> 00:16:26.390
Don't know could be either these things are literally indistinguishable, then you pretty much can throw away the discriminator

00:16:26.390 --> 00:16:30.859
And you've got a generator, which you give random noise to and it outputs

00:16:31.680 --> 00:16:32.820
brand-new

00:16:32.820 --> 00:16:35.629
Indistinguishable images of cats there's another cool thing about this

00:16:36.090 --> 00:16:40.999
Which is every every time we ask the generator to generate new image

00:16:41.190 --> 00:16:46.010
We're giving it some random data, right we give it just this vector of random numbers

00:16:47.880 --> 00:16:55.309
Which you can think of as being a randomly selected point in a space because you know if you give it

00:16:56.400 --> 00:17:03.619
If you give it ten random numbers you know between zero and one or whatever that is effectively a point in a 10 dimensional space

00:17:04.500 --> 00:17:05.670
and

00:17:05.670 --> 00:17:07.759
the thing that's cool is that as

00:17:08.850 --> 00:17:10.850
the generator learns

00:17:10.890 --> 00:17:12.890
It's forced to

00:17:13.920 --> 00:17:19.550
You if the generator is effectively making a mapping from that space into cat pictures

00:17:19.550 --> 00:17:22.099
This is called the lateness base by the way generally

00:17:23.010 --> 00:17:29.030
Any two nearby points in that latent space will when you put them through the generator produce similar

00:17:29.580 --> 00:17:31.580
cabbages you know similar pictures in general

00:17:33.780 --> 00:17:35.780
Which means sort of as you move

00:17:36.360 --> 00:17:44.020
Around if you sort of take that point and smoothly move it around the latent space you get a smooth léa varying

00:17:44.450 --> 00:17:48.250
picture of a cat and so the directions you can move in the space

00:17:48.800 --> 00:17:50.540
Actually end up

00:17:50.540 --> 00:17:52.250
corresponding to

00:17:52.250 --> 00:17:55.780
Something that we as humans might consider meaningful about cats

00:17:56.420 --> 00:18:02.139
so there's one you know there's one direction, and it's not necessarily one dimension of the space or whatever but

00:18:02.990 --> 00:18:05.260
And it's not necessarily linear or a straight line or anything

00:18:05.260 --> 00:18:09.129
But there will be a direction in that space which corresponds to

00:18:09.530 --> 00:18:15.310
How big the cat is in the frame for example or another dimension will be the color of the cat or?

00:18:15.890 --> 00:18:17.420
whatever

00:18:17.420 --> 00:18:18.620
so

00:18:18.620 --> 00:18:20.680
That's really cool, because it means that

00:18:22.070 --> 00:18:24.070
by

00:18:24.140 --> 00:18:26.140
Intuitively you think

00:18:26.210 --> 00:18:30.610
The fact that the generator can reliably produce a very large number of images of cats

00:18:31.130 --> 00:18:34.599
means it must have some like understanding understanding of

00:18:36.860 --> 00:18:40.870
What cats are right or at least what images of cats are

00:18:41.600 --> 00:18:45.130
And it's nice to see that it has actually

00:18:45.920 --> 00:18:52.329
Structured its latent space in this way that it's by looking at a huge number of pictures of cats it has actually extracted

00:18:52.910 --> 00:18:54.910
some of the structure of

00:18:56.120 --> 00:18:58.120
cat pictures in general

00:18:58.340 --> 00:19:01.990
In a way, which is meaningful when you look at it?

00:19:02.240 --> 00:19:07.959
So and that means you can do some really cool things, so one example was they trained Annette one of these systems

00:19:08.900 --> 00:19:10.900
on a really large

00:19:12.320 --> 00:19:16.390
Database of just face photographs and so it could generate

00:19:17.660 --> 00:19:23.499
arbitrarily large number of well as largest the input space a number of different faces and

00:19:23.780 --> 00:19:26.259
So they found that actually by doing

00:19:27.470 --> 00:19:28.910
basic

00:19:28.910 --> 00:19:32.079
arithmetic like just adding and subtracting vectors on the

00:19:33.200 --> 00:19:40.270
Latent space would actually produce meaningful changes in the image if you took a bunch of latent

00:19:41.000 --> 00:19:45.189
vectors, which when you give them to the generator produce pictures of men and

00:19:45.980 --> 00:19:52.569
a bunch of them that produce pictures of women and average those you get a point in your

00:19:53.420 --> 00:19:54.980
latent space which

00:19:54.980 --> 00:19:56.060
corresponds to

00:19:56.060 --> 00:20:02.690
a picture of a man or a picture of a woman which is not one of your input points, but it's sort of representative and

00:20:03.210 --> 00:20:05.779
Then you could do the same thing and say oh, I only want

00:20:06.780 --> 00:20:12.259
Give me the average point of all of the things that correspond to pictures of men wearing sunglasses

00:20:13.230 --> 00:20:15.060
right and

00:20:15.060 --> 00:20:21.980
Then if you take your sunglass vector, you're you're men wearing sunglasses vector

00:20:22.800 --> 00:20:28.279
Subtract the man vector and add the woman vector you get a point in your space

00:20:28.280 --> 00:20:32.480
And if you run that through the generator you get a woman wearing sunglasses

00:20:33.330 --> 00:20:34.710
right

00:20:34.710 --> 00:20:39.799
So doing doing basic vector arithmetic in your input space actually is?

00:20:40.050 --> 00:20:44.509
Meaningful in terms of images in a way that humans would recognize, which means that?

00:20:45.330 --> 00:20:48.350
There's there's a sense in which the generator really does

00:20:49.050 --> 00:20:53.659
Have an understanding of wearing sunglasses or not or being a man or being a woman

00:20:55.170 --> 00:20:57.170
Which is kind of an impressive result

00:20:59.520 --> 00:21:00.600
All the way along

00:21:00.600 --> 00:21:05.389
But it's not a truly random thing because if I know the key and I can start want to generate the same

00:21:05.390 --> 00:21:05.810
Yeah

00:21:05.810 --> 00:21:07.110
I'm so I mean that's about

00:21:07.110 --> 00:21:11.689
Unfortunate is the problem with cryptography is that we couldn't ever use truly random because we wouldn't be able to decrypt it again

00:21:11.690 --> 00:21:15.980
We have our message bits, which are you know naught 1 1 naught something different?

00:21:15.980 --> 00:21:20.509
And we XOR these together one bit at a time, and that's how we encrypt

