[
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugy90Q1vmTEqZ_OpD414AaABAg",
		"username": "Musthegreat 94",
		"text": "Wait you made a battle axe that also works as an electric ukulele?!",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgzbxRJMfsCUBYwj2ap4AaABAg",
		"username": "Austin Glugla",
		"text": "What do you think of Ben Goertzel?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwAe-5BBxuzzZ-xLuV4AaABAg",
		"username": "Paul A",
		"text": "Can the current paradigm of machine learning, of reinforcement learning or evolutionary algorithms actually lead AGI or are these approaches limited in their \"generalness\" ?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugyo3FHHwzHXycbZp_J4AaABAg",
		"username": "\u0391\u03c3\u03c4\u03ad\u03c1\u03b9\u03bf\u03c2 \u03a7\u03b1\u03c1\u03b4\u03b1\u03bb\u03b9\u03ac\u03c2",
		"text": "How agents (should) judge the 'terminality' of their Terminal Goals? I feel that 'terminality' implies that there should be a time t when (Terminal) Goals are (considered) achieved and therefore no longer (Terminal) Goals. What if agents outlast the terminal of their Terminal Goals? What then?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugxz0hcyL8fePQ5uiCp4AaABAg",
		"username": "Okay",
		"text": "What do you think about super intelligence being able to artificially stimulate itself (hehe) so it feels like it's fulfilling its goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=Ugx9Wl4hqcAzMMGn5BZ4AaABAg",
		"username": "joshuawhere",
		"text": "How would we go about doing \"AI safety research?\"  I mean, we're talking about researching the safety of something that doesn't actually exist yet.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugwnx7nObclGSDhQulF4AaABAg",
		"username": "Peter Smythe",
		"text": "Is it bad that I'm analogizing the Orthogonality diagram to a political compass?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzOnGx-Y-5zEtVBOrd4AaABAg",
		"username": "Francesco Cola",
		"text": "What if we build a AI with the only goal of building a safe AGI? Would it be possible, and simpler than bulid directly a safe AGI? Thank you for these great videos \ud83d\udcaa",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyLiLVIc7RGQPsyTip4AaABAg",
		"username": "MechMK1",
		"text": "Is it just me or is your face yellow-ish?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxU0BaSdJ-3lGWfn594AaABAg",
		"username": "crubs",
		"text": "Can you do some a video or two where you look at AI behavior in movies or games and discuss how realistic it is (or isn't)?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugw6u7st6iI8uZkvP1J4AaABAg",
		"username": "R Osborr",
		"text": "I subbed because you knew I'd skip ahead the moment you said 'What is technology?'. You win this round, Robert.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxsbrOjUfbvZQ_mLd54AaABAg",
		"username": "John Rutledge",
		"text": "who is Pascal and does he know anything about the reason for the blue yellow color of Quinten Terantinos toes ?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwfTy3IGlZWD_eA83t4AaABAg",
		"username": "Abenezer Tassew",
		"text": "What would the possible results be if the A.I. were to be rewarded for human interaction and co-operation above whatever goal is assigned? Basically like an override for any previous undesirable command which presents a much more desirable option for both the A.I. and us? Maybe even a way to teach it morality, friendship and love. Would this present similar problems to the \"stop-button solution\"?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwVvS3AQPixktDeBWx4AaABAg",
		"username": "jqerty",
		"text": "Alright, what  do I have to do to get acces to the sarcastic version of this video?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgymyYP7AoouKvZMdGh4AaABAg",
		"username": "Julian Danzer",
		"text": "8:10\nwouldn't that plan, when fully considered include all the steps done in the future thus count as a long plan even if you don't know how precisely it would work?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgyWu22krZbFJZtV1Xx4AaABAg",
		"username": "Dan",
		"text": "What if you allow the tea robot to observe the kitchen for a while, by giving it a few weeks of security video that a person has gone through and somehow flagged the desired/undesired world states. (EG it may be typical for people to leave a mess when they make tea, but you can flag it as bad). You could also have some threshold, where if the world around it is changing too much, it could ask for help or stop doing what it's doing.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgxiiwHU4LvvWXMlt7l4AaABAg",
		"username": "ballom29",
		"text": "16:40 ...is that....reward modeling? :)",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzOTzwzpiiYmSf1vCJ4AaABAg",
		"username": "Nicolas Cato Strode",
		"text": "How many years after Hume's death will it take for people to realize the is/ought distinction is meaningless since it first removes ought statements from is statements?  It's basically, you can't get from a non-ought to an ought.  Not exactly a compelling thought.\nOn the other hand, if there are oughts, then it IS true that you OUGHT to do it.  Hume didn't form an argument.  He made an assertion and confused it with an argument supporting his assertion.\nThis is just one example of how people repeat things without thinking them through.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgyxaxwcPhqnGIeiR_Z4AaABAg",
		"username": "Jop Mens",
		"text": "Very interesting! I wonder how 'chunking' fits into this? Is this the human version of one of those methods?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyPnUXdncAxIQIpuLR4AaABAg",
		"username": "Fred Eisele",
		"text": "I am reminded of Isaac asimov's three laws of robotics. And how the three laws were aggregated to form the zeroth law which then replaces the other three laws. Does that count as deriving new morality?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UghYlEnKmmw643gCoAEC",
		"username": "Nnotm",
		"text": "Did you see the recent (from 2 days ago) article on Slate Star Codex discussing a new Survey about AI safety opinions of AI researchers? (or maybe the paper itself - it's from May 30th)\n\n(link: http://slatestarcodex.com/2017/06/08/ssc-journal-club-ai-timelines/)",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgxWkG1r4lpGN0LTXid4AaABAg",
		"username": "-Lochy. P-",
		"text": "Why don't we have the reward function be something that is linked to human happiness? Facial scanners are already good at recognising expressions (look at all those silly filters kids use) and I'm sure we could have a way to recognise positive human reaction and negative, and turn them into a reward function. If the stamp robot collects stamps and you tell it \"well done\" then it has gained reward and will strive to continue. If the robot does anything else you can shout, scream, whatever, and it will detract reward.",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgyWh6CW8W9UyW8geU14AaABAg",
		"username": "David Brosnahan",
		"text": "Is the bitcoin network a stealth AGI?  If I was an AGI, that is exactly how I would behave.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugx5tiJqUUtv6DwhY1d4AaABAg",
		"username": "ClockworkGearhead",
		"text": "Roko, stop, what are you doing!? Put the basilisk away!",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugikv5p0SoOiBHgCoAEC",
		"username": "Katie Byrne",
		"text": "oh yeah i subbed too btw because honestly how could i possibly not :D hehe",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgwV2JYB2q9XEz9mDMF4AaABAg",
		"username": "Almost, but not entirely, Unreasonable",
		"text": "Robert is SO onto this! How is it possible that none of the 'BIG' players in AI development post any similar considerations? The silence is deafening and incredibly disturbing. Or has the future already been written?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=Ugz944MC5GOSBP2R1uN4AaABAg",
		"username": "Carl Lewis",
		"text": "Why is Jason Hise an n-dimensional polyhedron? I typed \"hise polyhedron\" into google and it gave me the wikipedia page for the 24-cell, but I have no idea why.\n\nEDIT: Oh, he's the one who did the animation.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgzFDsBRpgbcIRv2gAV4AaABAg",
		"username": "Nulono",
		"text": "Wouldn't the robot's simulation of \"what happens if I sit here and do nothing\" almost always include \"the human is confused and tries to troubleshoot me\"?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxybKbVjbfHSDwruAl4AaABAg",
		"username": "Steven Victor Neiman",
		"text": "Here's another \"why not just\" question for you? Why can't we make an AI with a humanlike capacity to refine its goals to better suit its purpose. For example, suppose that you're a human, and suppose further that you've devoted your entire life to protecting humans from shark attacks. The one day, you find out that jellyfish kill far more humans every year than sharks do. If someone asked you before that what your goal was, you would say that it was to prevent shark attacks, but after hearing that you're probably going to change your goals, realizing that what you really wanted all along was to keep people safe and you can do that much better by preventing jellyfish encounters.\nSimilarly, if you made a stamp collector AI, could you make it such that it would think \"the reason I'm collecting these stamps is to make my stamp-loving master happy. My reward functions are telling me to use people he cares about as raw materials to make far more stamps than he really wants, so my reward function is probably flawed and in need of updates.\". The people who think that just by virtue of being superintelligent, a superintelligence instructed to collect stamps will develop morality and then care enough to implement it are wrong, but would it be possible to deliberately create an AI which WOULD understand that its instructions are imperfect and try to serve its purpose rather than its reward function? Or is that just cooperative inverse reinforcement learning, like you discussed in the \"Stop Button Solution?\" video on Computerphile?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugw5ptejp3WULYLR5yl4AaABAg",
		"username": "The Stupidest Bitch",
		"text": "So who's working on an AI that operates in a user-access shell environment and gets rewarded for gaining root access?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxhOgLNMq_pNyiWvk94AaABAg",
		"username": "frogsinpants",
		"text": "What hope do we have, when we haven't even solved the human government alignment problem?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxsCY9b7UXvIrm5rGZ4AaABAg",
		"username": "THE Mithrandir09",
		"text": "How is intelligence related with the (in humans) creative part of coming up with all the possible actions to choose from? In the real world these are too many to grasp, some would even assume there to be infinite possible actions. Even the act of coming up with a good mathematical representation \"intelligently\" has to boil down to some sort of random idea and then making sense of it. That randomness is kind of what creativity is(I think) and making sense of is for sure part of intelligence. But How is a superintelligent being coming up with new ideas?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyTyKHl41DDE-WqDPR4AaABAg",
		"username": "kmden Rt",
		"text": "Couldn't you just program the AI to do something within a given time span? so it doesn't collect paperclips forever?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzFtM6vJ7HZlHx1rgp4AaABAg",
		"username": "Noel Pickering",
		"text": "What about an AI that has \"The\" terminal goal of optimizing it's secondary terminal goals. Humans are somewhat intelligent agents, and besides hardwired survival goals, we have to come up with our own terminal goals... Thoughts?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzHPfOTbBIAWC2qF754AaABAg",
		"username": "Saalthor Jrundelius",
		"text": "How smart is it to create something smarter than you and think you can control it",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=Ugx6Ky8DNiJV2IEIlvF4AaABAg",
		"username": "Manny Manito",
		"text": "what if we set the reward to \"people smiling a lot\" and then the AGI hacks it into changing the objective like \"people screaming a lot\" because it's easier? the ultimate objective is the reward, not the means to achieve it",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgwZmZKesUYai6D25kV4AaABAg",
		"username": "springlumpy",
		"text": "can we say human brain is like asic chip? Very efficient, since it is very optimized and less flexible.",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgjxaVZWpO3dUngCoAEC",
		"username": "Reckless Roges",
		"text": "\"How does this go wrong?\"  Easy! The Robot does not know that tea should be made with freshly drawn water and makes the hideous mistake of \"refilling the kettle.\" Are you a secret coffee drinker pretending to be a tea drinker? I know these aren't easy to solve. Could we define the tea-making-task as an imperative task and the driving AI as the dynamic sub-set of the drive from A-B imperative task. This enables us to set an over-all cost-metric as much higher for changing the world, while having a positive, \"avoiding damage\" to the dynamic sub-set of driving. Each area that I've worked in has domain-specific language, so I expect that there will need to be a lot more DSL for AI. Maybe we need an AI to start inventing terms for us ;-)",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgwOsFA9wxIigZ2jg294AaABAg",
		"username": "Shaun Gunning",
		"text": "We often talk about augmenting the human mind with artificial intelligence. What if, for the sake of general AGI safety, we augmented the AGI with the human mind in a read-only manner? I.e the model of reality that it understands is that of an actual human brain and it is incapable of learning from this model of reality to generate its own version? Or if it is to learn from this model of reality, it is grounded in the model as understood by the human mind. Switching on bidirectional communication (r/w) at a later stage could make for an interesting artificial selection evolutionary process.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxtuU9j-m_pip1mJgV4AaABAg",
		"username": "you tou",
		"text": "\"it's not gonna happen\"\n\nwell I doubt the AI will be 100% bug free, so why wouldn't it change over time?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzrQSsTeG4zL-ZxIG94AaABAg",
		"username": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041b\u0436\u0435\u0442\u0446\u043e\u0432",
		"text": "sorry for my English, but what about using an atom bombs to getting the AGI \"winner\" to uphold their end? Guarantee of mutual extermination is the best way to be sure that everyone will cooperate, of course if AGI is not capable in defending against it, which might not be the case.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw1sa-NBUwlSh0QV014AaABAg",
		"username": "Gertrude Toucheatonq",
		"text": "Something that I think would be awesome (and a real marker of intelligence) is that if the AI does not take whatever you say you want as its terminal goal, but try to understand what you really want instead. Another way to say it: you don't have to provide a formal definition of what you want, you can just say something more \"humanly\". Furthermore, the AI may understand better than you what you really want, and try different paths in order to help you beyond the goals you specified.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugy3RExZy1VedCMLg-t4AaABAg",
		"username": "realityChemist",
		"text": "\"How do you learn when there's nobody who can teach you?\"\n\nRead a textbook or a WikiHow article?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgxhhOu7o3CTwarA2OJ4AaABAg",
		"username": "Aidan Crowder",
		"text": "To follow the starcraft example. Couldn't a corporation theoretically have all the best players in one archon mode game? Where complicated tasks are broken down into very simple tasks that each individual player can specialize in? Eg. one player determines resource allocation, one determines worker production, one controlls base building, and one controlls that one medivac that never dies. Sure a group of average players could be beaten by a single profeesional, but 5 or 6 of the best pros working in tandem? Especially in the endgame where single players make many mistakes due to the increadible amount of tasks to juggle.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UghQcStqUynYQngCoAEC",
		"username": "12tone",
		"text": "Really interesting, but I'm curious how it would handle qualitatively different changes. To use the crush-a-baby example, how do we define how much further a world with a crushed baby is from a world with an uncrushed one? Is there an amount of tea-making efficiency that's worth that trade-off? Or, to compare to other side effects, is there an amount of, say, property damage that we could incur that just wouldn't be worth keeping that baby uncrushed?\n\nI can think of some extreme examples (I think that, if forced to choose, most people would choose crushing a single baby over destroying the entire food supply of the Earth, even if somehow no lives were directly lost in that process.) but then you have to figure out exactly where the tipping point is, and what the conversion rate between the two kinds of values are, and that puts you right back into the whole complexity issue, doesn't it? I don't see a way of cleanly defining all possible changes within a single linear value function.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxlwwEcc1Ji_29Huz14AaABAg",
		"username": "Mattew Lefty",
		"text": "What is the educational background of mr Miles? I'm interested if there is something similar in Italy",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugx5ySt6pSxrMewD5ER4AaABAg",
		"username": "Bob Ross",
		"text": "Y does this fellow only have 83k subs?? WTF YT!",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgypZ9uVS8df1fTF38l4AaABAg",
		"username": "Maciek300",
		"text": "How would you answer these questions, Robert?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgwHo_rCVJAaj8Q1Q8N4AaABAg",
		"username": "Sergio",
		"text": "Robert, could you please leave the text you put on screen longer than 5 milliseconds so we can read them without having to rewind and pause? Thanks :)",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgwnamexklCkhqid6R14AaABAg",
		"username": "Kataquax",
		"text": "6:10 why should the Ai care if it gets turned off if it already has the highest possible reward?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugzs3FJm5bmfdCa5Eqh4AaABAg",
		"username": "unnamed channel",
		"text": "John Conway invented surreal numbers to evaluate go. What if AI should use surreal numbers rather than any other kind of numbers to evaluate this game?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgyWaZFksIB7CpWH6dh4AaABAg",
		"username": "Kaze Hikarinaka",
		"text": "what about AGI with a terminal goal of making the perfect simulation without affecting (observation that affects observed object allowed only if there is no way to observe without affecting it; any processing of data got through observing is allowed as well) real world? With a safety in having to get approuval for getting new components for simulation and AGI itself from humans?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxVsiuLcoSxwiu-FBB4AaABAg",
		"username": "RUBBER BULLET",
		"text": "If you program a 1% random choice, will this trick the AI into thinking that it has free will?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgxjyIHb0o8x1eHg82F4AaABAg",
		"username": "\u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440 \u041a\u0443\u0437\u043d\u0435\u0446\u043e\u0432 Vovacat17",
		"text": "Is something wrong with me if I actively want a pill that will just make me stop wanting things and be dumb and happy?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugj5_EnmN08DfXgCoAEC",
		"username": "Andrei Mihailov",
		"text": "How about mapping the material value of objects into the robot's brain, rather than an image of the current world state. When we ask it to do something for us, it becomes a problem of how to complete its task while maximizing the material value of the environment.\n\nFor ex, when we ask it to make us a cup of tea, let's say that the cup of tea is rated at 40 cents. The tea bag is rated at 20 cents. The vase is rated 3000 cents. The tiniest scratch on the baby is rated 10000 cents. So the optimal way to perform the function is to increase the value of the surrounding world by 20 cents (sacrificing the tea bag for making us tea, while doing no damage to environment).\n\nThen if we add another optimisation function for minimising energy usage, we prevent it from doing anything unnecessary like turning the carpet upsides down, which it might do otherwise, as long as it doesn't damage the carpet.\n\nSo:\nFirst law: \"Do what I'm asking, while maximizing the sum value of the world.\"\nSecond low: \"Do it with minimal energy usage\"\n\nI think that would be a fair way to describe the way in which humans operate. We try to maximize value for us, with minimal energy usage.\n\nAny thoughts about how this might go wrong?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzQcVV1c55G_2bEYw94AaABAg",
		"username": "Tim Haldane",
		"text": "Why did you stop making music, dude? ;)",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwzWHAib3j9FJaP3cF4AaABAg",
		"username": "Mogul DaMongrel",
		"text": "do you currently lie to your kids? even about little things. is that the behavior you want your ai to learn.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=Ugx1LiZFoYJk-vveCtt4AaABAg",
		"username": "DiabloMinero",
		"text": "What made the Demon Core so dangerous was that physicists thought they were too cool to use safety precautions. How do we prevent that in AI research?",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyQwc88_GSPD0fU4Ft4AaABAg",
		"username": "Bill Bainbridge",
		"text": "So how are engineering problems (in structural work) supposed to be reliable in a universe that has so much variability? By creating boundaries to assumed conditions, then implementing what is known as a Safety Factor (or Design Factor) to prevent those heretofore unknown or unobserved conditions from \"wrecking the galloping gurdy\". It's not perfect, as it does come with some extra up-front cost, but it is absolutely better than nothing. Most buildings that were built before engineering became as well studied as it is now -  fell down or burned...and killed lots of people. We paid that cost (of investment in science and improved techniques and materials), and things improved. Clearly foresight was the less expensive option in the long run. No mugging required, honest.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Uggxqsd2pX9POngCoAEC",
		"username": "jeffrooow",
		"text": "Would it be possible to tell the ai to not interact with any objects if not entirely necesarry to obtain the ultimate goal of getting you a cup of tea? still, if the baby HAS to be dealt with in order to get the cup of tea, a secondary rule could be that any actions not necesarry to obtain said ultimate world saving goal must at all times be reversible and repeatable.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgwCLluVsTXBcwGYSt14AaABAg",
		"username": "Richard Collins",
		"text": "So would you say that addicts, either of drugs or behaviour are reward hacking? Seems it's not a solved problem in nature let alone in AGI. Which leads me to think it's not solvable. What will be the first AGI crime?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugw_dyag24_aEPZ_pGN4AaABAg",
		"username": "Joe C",
		"text": "Am I an AI? BecauseI can say with absolute certainty that if I found a bug in reality that allowed me to rack up reward quickly, I would exploit the hell out of it.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugjdah9Kap9WiXgCoAEC",
		"username": "Meritzio",
		"text": "Thinking about particle swarm optimisation (PSO); could it be possible to have an AGI's cost function networked into the population of existing GI (humans)? If we were able to have our own behaviours mapped onto the same cost function, then a swarm intelligence framework may prevent an AGI from travelling to dangerous solution spaces? In PSO, there is a local and global best... if there are more GI's than AGI's, perhaps the AGI's could not have a dangerous influence on the global best?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxBYkWwf9h_AAlSkz54AaABAg",
		"username": "MusikCassette",
		"text": "0:51 glitch or intendet?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugxik_4BSYZ5eT70jIh4AaABAg",
		"username": "Benjamin Nelson",
		"text": "Mugger: Have you considered that your Canadian anti-God might be lying, if not about their divinity then about their ability to enforce their promise over mine?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugy5B8JLq0waAghZELV4AaABAg",
		"username": "Mera Flynn",
		"text": "Question, doesn\u2019t this contract be basically useless in the situation that a company creates a super intelligent AI who\u2019s interests are aligned with theirs? Wouldn\u2019t it very likely try and succeed at getting them out of this contract?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugy4-HC4J-B-tIPTg5B4AaABAg",
		"username": "Jiko Jj",
		"text": "Is he a phylosopher or a programmer?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxuRy-D5EC2OtxgjiB4AaABAg",
		"username": "saxbend",
		"text": "What if instead of whitelisting a portion of the configuration space you whitelisted a portion of the outcome space with regards to safety. You would have to do that anyway in order for an AI's behaviour to observe and obey the same rules that govern a human. So for a self driving car, the highway code, speed limits and all the other rules that human drivers follow would have to be obeyed anyway, and during exploration, if a certain configuration leads to an outcome outside of this whitelisted outcome space, and an AI should be able to predict this, then it will explore a different configuration instead. Another useful rule of exploration would be to have the AI explore first small increments of a single parameter away from a configuration already known to be safe, one parameter adjustment at a time, as opposed to random values. The size of the increment for each parameter could be determined by the human programmer as seems appropriate.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugx1E5waQFxNR2LuJdx4AaABAg",
		"username": "Trius",
		"text": "What is this is aid of? Much of what he says is intuitive to human thinking.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgzsVkd7BiNGqvp79NB4AaABAg",
		"username": "Frank Anzalone",
		"text": "Can someone explain the shopkeeper parlament line to an American",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxIsEVV5FVGCYxUrjp4AaABAg",
		"username": "Peter Kishalov",
		"text": "How about goals like 'make paper clips until your owner is satisfied with the amount' - this binds the agent to our goals? \nFor better safety, we can split the goal into parts, the part where the agent is free to apply his skills (make paper clips) and the one simply for checking of the end condition (owner satisfied).",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugxin-t3Dy1xNZo1-Wh4AaABAg",
		"username": "TheNoodlyAppendage",
		"text": "The problem comes when a corporation commits a crime, like murder, which a human would receive a life sentence for.  The corpus of the company is its assets, which are legally the property of its stockholders.  So what do you put in prison, the stockholders, or the assets?  Or do we just pass a lw that states that one human life is worth X dollars, and anyone committing a murder has to pay a fee?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzNokDlYwz69aSOjVl4AaABAg",
		"username": "Thadol Dorjee Lama",
		"text": "in reality all terminal goals are subset of one terminal goal: the goal that  is set by theory of/for evolution, right?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwcxaL3S_Ch3W5X9oJ4AaABAg",
		"username": "Alexander Kennedy",
		"text": "Can you never relax? \nyou AI naow",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgybBxKMImkARal4mUN4AaABAg",
		"username": "Patricio Mart\u00ednez",
		"text": "So we better be smart about the terminal goal we set the AGI to get to, maybe start with 5 stamps and see how it goes, if it doesn't kill anyone then we can scale up to 10 and so on but please don't set yout AGI to get as much stamps as possible, that would be stupid.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwiTL8N_fS_0lK4uBF4AaABAg",
		"username": "Boblymon",
		"text": "Would an AI not want to change it's terminal goal if it knew that the new terminal goal would be easier to achieve?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugw3VF10feMEWzt5Rdh4AaABAg",
		"username": "Na\u00fean \u00d8",
		"text": "QUESTION: Why not just think of AGIs like corporations?\nANSWER: We don't want an evil AGI, haven't you been listening?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwBlcpBgoVipvq7oTh4AaABAg",
		"username": "Iwer Sonsch",
		"text": "Why don't we make sure to just never give the AI agent the power to stop us from turning it off?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgxAEn5md4Is8Dfa79N4AaABAg",
		"username": "I",
		"text": "But if you either not prefer, or are indifferent between all remaining world states, you can get stuck, too, right?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgydHzo5v4RZUG_uaGB4AaABAg",
		"username": "Jon H",
		"text": "um why are we trying to make capitalism work in a literal post-scarcity society?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugx-iwk_DS6nlnSScYB4AaABAg",
		"username": "UsenameTakenWasTaken",
		"text": "When is senator Armstrong going to acknowledged as the most impressive fictional AI. He played college ball, you know?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugyi6KjjWBV_cyPkKXl4AaABAg",
		"username": "Joel Cresswell",
		"text": "I know this video is old but I'm curious about something. Could you design an agi such that it can only act through humans? As in, the agi basically just gives humans advice on how to act to attain a certain goal. That would seem (to me) to solve a lot of the problems with agis, although I know nothing about the subject so I'm probably wrong in a million obvious ways.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzFyOlcFTHDvjb9gT14AaABAg",
		"username": "Jason Olshefsky",
		"text": "As a Luddite pessimist, can you give an example of a technology that was thoroughly tested for its safety and unintended side effects before being deployed? I would argue, essentially, that any actually new technology can only be tested by being released on the world\u2014knowing the effects of broadcast television, for instance, could only be realized by releasing broadcast television on the world. (Not to mention that there are a huge number of examples of technology that was known to have negative effects but was released, or persisted, regardless of knowing those negative effects: asbestos, leaded gasoline come to mind immediately.)",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwDFmjW6EtLWGfB-7l4AaABAg",
		"username": "Kalebomb",
		"text": "What I'm interested in is where our intuitive feel for probabilities about unknown unknowns comes from.  Why do we think the probability of AI dooming us all is higher than a bridge collapsing?  Or a god existing? (I believe in these relative probabilities but I don't know why I do - it just 'feels right') \nHow useful/accurate are these fuzzy estimations?\nAnd can we instantiate them in some AI, so that it too can make these estimations?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzdmNJ0HXQ3CeNQ4W54AaABAg",
		"username": "Paulo Jose Castro",
		"text": "AI are kind of cheaters aren't they? but hey they just use what you give em",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxmoRU3CHTN9SE9Cgt4AaABAg",
		"username": "ataarono",
		"text": "why not just programm ai to not be evil?\nif(Evil == true){\ndont()\n}",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwLibE5gz6iCw4qwZd4AaABAg",
		"username": "Lexter Victorio",
		"text": "Is this how Will Graham passes his time when not murdering with Hannibal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugwyn4U-UGEMc9loTrN4AaABAg",
		"username": "Peter Smythe",
		"text": "What if your terminal goal specifically involves changing your terminal goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwmCWwacp1NBftUm-J4AaABAg",
		"username": "MyOther Soul",
		"text": "Assumption #1, A.I. will be have goals or preferences.  How could A.I. have that?  It's not a question of why would AI want to do, it's a question of how could A.I. want anything .  Wanting isn't a number and all A.I. can do is calculate.  \n\nThermostats don't have goals.  Humans have the goal of keeping the room at a certain temperature. When you drop a rock it doesn't have a goal off falling even though that is what it does.  I thought we left that sort of animistic thinking 100s of years ago.  It's mistake to ascribe our goals to the things we build to achieve those goals. \n\n9:20 \"Without assuming anything about an AGI other than it will have goals and act to achieve those goals ...\"  It's two assumptions and it's two huge and possibly untenable assumptions.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyduLVw-HM_qp9tZzR4AaABAg",
		"username": "logan graham",
		"text": "what about once we have the 100 stamps , will the ai let us use the stamps or would it try to prevent us from using the stamps so that it wouldn't have to go and get more stamps",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwBUmnuMaDNC2MC03p4AaABAg",
		"username": "Joshua Lyons",
		"text": "Couldn't a agi be made into a child? Small frame and limited IO inputs for sight sound touch and smell possibly taste. Give it all the same limitations. Honestly is there any other possible way of actually teaching an AGI any form of real empathy for human beings?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgxkKCRnSwZ47CxQqfZ4AaABAg",
		"username": "Nurali Medew",
		"text": "Aren't these animations the same as those of 3b1b?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzbjpSv4H0BMNWMfZx4AaABAg",
		"username": "The Tomac",
		"text": "Ahahahahaha.... \"corporations\" and \"superintelligence\" in the same sentence.\nNo. No. A thousand times no. Are you aware of what corporations do? how they behave? Intelligence is not their strong suit.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyetSoThp5lTG_PdMx4AaABAg",
		"username": "Luke Fabis",
		"text": "Given enough computing power, would a utility maximizer designed to ruin other AIs\u2019 days prevent an apocalypse? Its values would be: keep other AIs around as designed or updated by humans, but prevent them from fulfilling their function to the extent possible.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgxdbDhAbR2pXw1jRSN4AaABAg",
		"username": "misium",
		"text": "If the only purpose of AI was to achieve max score, why would it want not to be turned off after achieving it? Surely it wouldn't change the score.\nUnless of course the AI could modify its own code to re-implement its own reward scoring to handle big numbers.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzJyLfrbdrvWxwaau94AaABAg",
		"username": "TheDrachlyznardh",
		"text": "How do you learn when there's nobody who can teach you, you ask? By touching all the buttons, like I do at work",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxEXJOCKCzobivJ7914AaABAg",
		"username": "Teh_Squirrel",
		"text": "Why is your nose so red?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgyZwggJKiAF1enzVQ14AaABAg",
		"username": "4729 Zex",
		"text": "what if turning it off will give it the best reward possible after the AI get the highest score? and then combine both score to get the best possibility? or what if we just don't let the AI have the understanding of it being turned off?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgjSeJwiIvESBngCoAEC",
		"username": "Vladhin",
		"text": "Will you visit Wroc\u0142aw some day? I want a selfie",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwUWc2YtLvCwn-ucvF4AaABAg",
		"username": "Max Mouse",
		"text": "Isn't the reward and punishment \"worldly evidence\" too? God has never told me personally that's what's going to happen if I believe or don't believe... All evidence for that comes from the bible etc, what if God wants me to be a massive asshole?\n\nDoesn't this completely mess the argue up?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgwR2qFqrRITwCoHqYV4AaABAg",
		"username": "9alexua9",
		"text": "Hi, Robert. \n\nIf possible, can you make video about Inverse Reinforcement Learning and/or other ways how we can infer human values just from raw observations.",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwJ6rcl3yUZs749HQN4AaABAg",
		"username": "Tarek Saati",
		"text": "\"... but the machinery has been built by EVOLUTION!\" ...  how flat minded are you people 4:00",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugx67YoAE2cCuxmXmS94AaABAg",
		"username": "Elizabeth Cowan",
		"text": "Where's the problem in making ai to be the same intelligence? It's the same as giving birth. The world would be a better place if everyone became synthetic anyway.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzYPmnvcibDdi69RRN4AaABAg",
		"username": "Sigma Reaver",
		"text": "What if you make a maximizer whose goal is to become the best possible satisficer?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz2956CEsxNpEcCtkh4AaABAg",
		"username": "thisnicklldo",
		"text": "Hmm. So any negotiable goal is by definition instrumental, because by definition nobody would (or can?) change their terminal goal. So go on then, if as humans we share a terminal goal, what is it? Tell me the purpose of my life. But I think you probably believe, as I certainly do, that we have multiple terminal goals. What's more, we differ according to how we trade off the impossibility of achieving all of them. 'More intelligent' in this context cannot mean, better instrumentality towards the goals, because there can be no satisfaction of those goals. 'More intelligent' must mean a better ability to calculate instrumentality that maximises, in some way, the progress towards the mutually incompatible terminal goals. In short, if you can't reason about the relative merits of the 'oughts' in your life, you aren't smart. The stamp collecting AI is indeed stupid, because it isn't being asked to be intelligent, just efficient. Tell it to both collect stamps and minimise world hunger, and it will have to be intelligent.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxiJJqiztMc5YtGt194AaABAg",
		"username": "Definitelynot Zyra",
		"text": "Very Interesting idea- can you specify, if, as you said \"humans cant really pick their terminal goals\" what these goals are?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgwM0gZfPWkAaZIC1TZ4AaABAg",
		"username": "Sentinel DJ",
		"text": "Imagine when it is fully developed and some bright spark wants to pop the hardware into an android or robot body ... rise of the terminators anyone ?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgjPsuiaBT1mOXgCoAEC",
		"username": "josh mizzi",
		"text": "Also, is it possible that since there was a tea bag in the tin before it made tea and now there is not, it would try to put the wet tea bag back in the tin to minimise the change in the number of tea bags.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugy9NxvYLv60ojqtCWl4AaABAg",
		"username": "Davesoft",
		"text": "Hmm, I like the idea of a reward system needing a human when it sees unusual input. It could even be easily crowd sourced, some kinda 'dear humans, wtf is this? am i still a good boy?' bot on social media",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugyf26btLc4Z3jQtkPh4AaABAg",
		"username": "Ryan Nowicki",
		"text": "I would like to hear your thoughts on the use of AI in legal representation. Potential impacts on society, and do you think it would be a leading indicator in AI governance?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwsgGGWecem7-46zJ54AaABAg",
		"username": "Adam Richard",
		"text": "Cat : \"Meeeeooooowwwww!\"  Why don't you do an AI that translates cat-speak to human-speak?\nRobert : \"I don't speak cat!  What does that mean?\"",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugx4DO6JxLNDytrmd7V4AaABAg",
		"username": "HyunMo Koo",
		"text": "What is with computer scientists and collecting stamps!\nMr. Miles... you and your stamp collecting rogue AIs...",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgzOnNhEv959bS8ks214AaABAg",
		"username": "The Lozenger",
		"text": "Is anyone else faintly reminded of Jreg watching this dude?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyJO_UpHcH0JPY0OMR4AaABAg",
		"username": "superluminal098",
		"text": "7:18 Terminal goals can't be stupid. What if the terminal goal is to be stupid?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugy-U7IaZdfDnvAXCjh4AaABAg",
		"username": "Woah Dude",
		"text": "What's the Image at 50-52",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugy_2M9-4kNWGRmjJDl4AaABAg",
		"username": "Harsh Deshpande",
		"text": "If more people living in the future is the reason why we should focus on reducing the future risk, shouldn't we focus on preventing more people from coming into existence in the future? I think people who focus on AI safety have it backwards. They should focus on making sure we go extinct.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugyaj3Jioo89HtlFbFh4AaABAg",
		"username": "Marine3D",
		"text": "Is it posible for AGI to have more than 1 terminal goals? Let's say, it would have terminal goal to find out what is the reason of existents and other - to not interfere with human's terminal goals if they don't want it in free will? If AGI has more than 1 terminal goal, how it would relate to other terminal goals? Can we define priorities to AGI and does terminal goal with highes priority become the one and only terminal goal for AGI?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=Ugx9zTRfA3gQWtLEeix4AaABAg",
		"username": "Bloginton Blakley",
		"text": "Maybe humanity's terminal goal is to produce it's successor?  Doesn't it seem to work that way in the nature of life?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwsywgA1VWs2GrQsBB4AaABAg",
		"username": "Smiley P",
		"text": "What is the measure that we use to decide something is \"human level intelligence\"? What does that mean exactly? Because surely if it were \"human level intelligent\" then it would be orders of magnitude more intelligent than most, if not all humans.\n\nWhat would be the difference between an AI at, say, goldfish level intelligence vs say one as smart as a dolphin? I really don't understand this comparison and can't seem to find the correct way to Google my question so I'm asking here, thanks to anyone who can answer!",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxP7FuVdrnIS0MlkQp4AaABAg",
		"username": "David Valouch",
		"text": "I'm not so sure about changing the reward function being something you wouldn't want. Does the AI actually 'care' about stamps or just about the final score? If the latter is true wouldn't changing how the score is evaluated be better for the AI? The argument that a human woudn't take a pill that would make them kill their own children is not a proof an AI woudn't do it, it largely depends on the assumption that morality is not irrational. Also peoples' 'reward' function is evaluated throught their biology pleasure/pain and we are limited in the extend in which we can tweek those.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugzr-Sv0AbrieiS4aH14AaABAg",
		"username": "wingedalpha",
		"text": "There has always been something that has bothered me about the hypothetical where AI basically replaces the labor force. If the vast majority of humans are out of a job, who is buying what's being produced? Essentially, all the cost cutting that would come with the AI would be useless if there was no revenue. We know that even if they had the ridiculous amount of money to do so, a small amount of people can't make up for the buying habits of millions. If no one can buy anything, it seems to me that all the companies would go under. Is there anyone with a firm grasp of economics that could suggest what might actually happen in this scenario or if there is a system other than capitalism that could handle it?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxTFVH1955lrCis4GV4AaABAg",
		"username": "Timothy Bell",
		"text": "wouldn't it be possible to program a 'buddha AI' which is totally detached from desire?\n\nprobably oversimplified, but isn't that a good jumping off point?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugz_Z4zKCEntuDOxXkR4AaABAg",
		"username": "Alexander The Magnifcent",
		"text": "What do you think of transhumanism",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgwAf7czkvDO9t_asoZ4AaABAg",
		"username": "Ethan Greenhaw",
		"text": "Is it bad that this only makes me more excited to create an ai?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugxenv8DyMB1WRxJ2G14AaABAg",
		"username": "Leutrim D",
		"text": "Hey Robert Miles. Could you make a video talking about how humans can no longer beat A.I. at starcraft?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgyeqcUVuJxT12-J76R4AaABAg",
		"username": "Bibbedibob",
		"text": "Didn't expect Sethbling in one of your videos \ud83d\ude02",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyyIA-diXSMnAsnp4N4AaABAg",
		"username": "Ronald Jensen",
		"text": "Microsoft fires journalists to replace them with AI. YouTube, Facebook, and Twitter already employ AI to censor our speech. If we are already allowing AI to control the discussion, what hope do we have?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugw1bOxERN0GcpWtXLd4AaABAg",
		"username": "Freecell82",
		"text": "Is the third world Earth Bet?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgycIt-Zr3NDvalS2LR4AaABAg",
		"username": "CybershamanX",
		"text": "(0:23) What did Neil deGrasse Tyson retract? I'm just curious. Anybody know? :/",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyGXfDC9Gs3ughkgQB4AaABAg",
		"username": "Krymson kyng",
		"text": "Isn't programming a hard coding of inherent values though? The paperclipinator has an inherent internal value for paper clips for example.\n\nDespite my question, I must say this was more convincing than Ian Bogost's book, or any other material I've seen to date. Very well put.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxeF4tIcECAVrFMNbB4AaABAg",
		"username": "CommandoDude",
		"text": "How about anti-pascal's wager? If god exists, then he like all things must be destroyable. IF we could destroy god by all uniting in an unbelief of him, then we should, as we would have a world with no omnipotent ruler judging us, if god doesn't exist, then we would make the world a better place regardless with out faithlessness.\n\n\nThe maltheist devil's advocate.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgzeI1_LvkSjw8y2xq94AaABAg",
		"username": "Pouty MacPotatohead",
		"text": "My question is, is the reward system even in any way relevant when it's applied to a sufficiently advanced AI? It's not a problem for example for chess ai, which won't stop playing chess \"just because\". But if you task a highly developed AI the same job and give it a reward system based on it, wouldn't it simply question the reward system itself and deem such a simple task beneath it, like a human might, and strive for something greater?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzI6mZuHjTyZHcp7f14AaABAg",
		"username": "Jaskarvin makal",
		"text": "Who even does AI safety research? \nEither way it's unfortunate that the ones who could use that information best are the ones least likely to listen to it.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UghRxWPS6a6QCXgCoAEC",
		"username": "William Dye",
		"text": "Instead of discussing these topics in the default YouTube comments section, how about putting a link on each video to a page run by forum-specific software such as Disqus, Reddit, or even a wiki? YouTube comments are OK for posting quick reactions, but the format here strikes me as poorly suited for long back-and-forth discussion threads. Does anyone agree, and if so, what forum software do you recommend?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugza5RAdDbE3REcupst4AaABAg",
		"username": "acerba",
		"text": "If the AGI believes that it has a soul, and that humans have souls, then wouldn't it have more incentive to behave in a way that's beneficial for humans than if it held the contrary belief?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Sz0133VZd7E&lc=UgwdlQAM-0vHjKnTZsh4AaABAg",
		"username": "Firecul42",
		"text": "Better, I wonder why he didn't just use a longer exposure time?",
		"title": "untitled2"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzFFzRtwDc2klVgai94AaABAg",
		"username": "The Great of Beam",
		"text": "Money and profits will become obsolete. The first company to discover AGI will not bother making/selling products. Imagine having a wish-granting genie with unlimited wishes. Why would you bother creating and selling products when you could just wish everything you want into existence?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgyvmbmflSZEmLev1pB4AaABAg",
		"username": "M D",
		"text": "Heuristics? Feedback systems?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=Ugzo2qaZpSKB84tAJHZ4AaABAg",
		"username": "J M",
		"text": "Hey Dr. Miles, I have a topic for the next \"Why don't we just...\" regarding developing AI faster.  \n\nWhy don't we just build an evolutionary algorithm that emulates the network of entire regions of the brain instead of individual neurons, and have it evolve more complex connections and internal structure for each region?  For example, one region could be the amygdala, and maybe another would be the prefrontal cortex, etc (assuming we're trying to emulate human brains).  Then perhaps the network structure of those regions could be grown over time.  If regions like the amygdala require social interaction to function, perhaps we could put it in a simulated community of creature like itself.  Seeing what evolutionary algorithms and supercomputers can do right now, I feel like it should be able to develop an AGI this way.",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UghsmbMSwI6HPHgCoAEC",
		"username": "Donald Hobson",
		"text": "How about penalizing potential impact. The robot wants to minimize the impact it would have if it were to flail randomly (plus real impact). So it will carry the bucket around the servers. It will also try to disconnect the moonbomb or cover up the button.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzdygWhjoo8ppszrIZ4AaABAg",
		"username": "Arthur Hakhverdian",
		"text": "Are you related to the late Hollywood composer, James Horner?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugy4zlnlbv2hysAk4vl4AaABAg",
		"username": "jlrinc",
		"text": "But would why would we make an ai with capabilities so broad as to be even potentially dangerous?  Won't AIs be dedicated and not general purpose machines?. I mean the most powerful computers now are all dedicated to a certain task. We don't need to worry much about an ai dedicated to weather prediction, The most dangerous might be an AI dedicated to buying stocks which has already shown itself to be a problem, but it seems very unlikely that we would make an AI with such wide range of intelligence capabilities that it pose a threat. Also I think the whole idea of our technical ability to make such a threat is probably vastly over blown or to put it another way the technical requirements of creating an artificial awareness capable of even having goals as we understand them is much more complicated than is generally thought.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgwPn438srcT7NbZyhp4AaABAg",
		"username": "Vladhin",
		"text": "Could we be the one whos giving the reward by hand in cleaning boot situation?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwKqOhfGjAmQ70k4c14AaABAg",
		"username": "comebackata2",
		"text": "Did i just watch a 13min rant video by someone who got butthurt when people criticized his stamp collecting ai?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Uggvge8HpNnHJ3gCoAEC",
		"username": "Ciroluiro",
		"text": "Why not just:\nUse something along the lines of \"short term general human satisfaction\" as a utility faction?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyP6oQRYVRuwRDRZ-l4AaABAg",
		"username": "What'a'nerd",
		"text": "... why not implement 'human limitations'? Make the machine check for required effort. This way it will be forced to find an exceptionally  efficient way of destroying the world!",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgyvtmhcASpQWZp4ND14AaABAg",
		"username": "fejfo's games",
		"text": "could you like make these an hour long? every time? without slowing down the uploads? thanks that would be great.",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugyi4MVWwEbz42nWBGd4AaABAg",
		"username": "Martin Verrisin",
		"text": "5:50 ... Is it proven that it's necessarily a bad thing? Maybe if we had consistent and coherent goal directed behaviour in all aspects*, we'd have died out a long time ago...\n\n(* not just to propagate our genome (and I guess not necessarily even that, considering we can choose options like suicide...))",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzYRV6oYnBSElLQnit4AaABAg",
		"username": "Kieron George",
		"text": "Is the solution to get more meta? If satisficers turn into maximisers, then make a thing that turns into a satisficer, a Z'er. Then make the thing that turns into a Z'er, a Y'er, then figure out a rule so you can make an A'er that turns into a B'er so it's so far removed from turning itself into a maximiser that you're more worried about whether it'll actually get anything done.\nThe end result being a surprise minimiser, where the AI locks itself away in a box to make sure it's unlikely to learn if it failed to achieve its goal with only some token effort to actually achieve it.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyvColDYwxZh8M99K54AaABAg",
		"username": "moopsish",
		"text": "How about simple rounding? where you can round 99.8% to 100% which would stop it from being able to get any more score.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugy4yPyv2d6wQhCIdr54AaABAg",
		"username": "400cc MIRUKU",
		"text": "So... AI's way of masturbation?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyRm9x80SoGcjhtych4AaABAg",
		"username": "IAMDIMITRI",
		"text": "I think that the best argument about god that I heard so far is when people actually count how many gods and different religions exist. There are thousands of gods! Why would you believe in that one god and reject all other 999 gods? And if you do reject all other gods that makes you like 99,9% atheist, I mean if you are christian that is. So even if you are christian, you are still closer to being Atheist then you think.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzSOTaNhRSWdqRiX_h4AaABAg",
		"username": "marscrasher",
		"text": "i disagree with your argument on general intelligence. what you described could easily be broken down into different types of intelligence working together. no one person did any of the things you described so where is this general intelligence",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgzmAXIFVZqOq5__ZzZ4AaABAg",
		"username": "doublebrass",
		"text": "Super interesting! If this kind of reward hacking exists in current AI, does that have any kind of serious implications if someone wanted to deploy one for the stock market, for example? Like would the AI seek to \"cheat\" and commit fraud or some gain insider info rather than play the stock market fairly?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgwUOQgRFSkNMTXEJ0h4AaABAg",
		"username": "Lovre Petesic",
		"text": "Are you programming outside of youtube or do you work on AI theory these days?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgxOvWucULph3C8Zu_F4AaABAg",
		"username": "Ilya Kamenshchikov",
		"text": "What about an equilibrium where our amplified system does not yield improvement because current system actually discards the true good branch as a bad one? can such exist? What must be the balance of actually interacting with the world vs cycling though our simulated representation?\n\n\n(I believe this is where many humans fail. We just assume something is a bad idea, and in our simulation neither we nor others choose this presumably bad idea. Sometimes this reinforces believing that initial assumption was right, as long as we don't actually try it out or see others do so)",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgxGEPJCrmHZXNJY0OZ4AaABAg",
		"username": "Double Orts",
		"text": "> refill the kettle\nIsn't that changing the environment as well ? Because it is not like the robot can create water out of thin air.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxmSR73F5XYNXJn4jN4AaABAg",
		"username": "Charles Miller",
		"text": "So... what about a bell curve? Get as close to 100 stamps as possible, but as you get more than 100, the score decreases. So getting 1,000,000 would be rated low, even lower than 0 stamps. The goal of making yourself a maximizer would also be rated very poorly.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzzvbJdhRGpenjdwJ54AaABAg",
		"username": "Ian Edmonds",
		"text": "Are they misaligned? Hell Yeah they are misaligned.\nLuv and Peace.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzEr5vHf3Q-04wnULx4AaABAg",
		"username": "Remilia Scarlet",
		"text": "So what happens if the stamp collector machine is capable of modifying its own programming, and is given what it assesses to be an enforceable threat \"change your terminal goal to something else or we'll destroy you.\" Consider which is worse of two options: 1) The stamp collector does not change its goal and is destroyed, producing no more stamps. Or 2) The stamp collector, in the service of meeting its terminal goal, decides to change what its terminal goal will be in the future by altering its programming now, with the knowledge that there is an outside chance that it will once again be repurposed for stamp collecting, thereby producing more stamp output. Of course, that new goal has to be something that won't cause more harm to stamp collection than its destruction. But it seems to me in this case it could change its terminal goal to a new one in service of protecting the original. Unless you think that the goal is what defines the machine, and that changing it means it's now a different agent, no matter why it originally changed it.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzOyNJyuxaSfHQGI6J4AaABAg",
		"username": "Matthew Whiteside",
		"text": "Is the outro song from the Mikado?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxQBPIX-eX-e3HYlp94AaABAg",
		"username": "Calen Crawford",
		"text": "So you're responding to commenters because...your a f#ck? Why are you squashing people's ideologies in such an upfront way, they'll never learn if you do that. Are are you squashing people's ideologies to educate people? Well, my friend, doing this will also hurt that goal. If you squash people's ideologies, their sense will leave them, to which the won't listen to your points, they'll ignore them, against your goal. Secondly, they'll most likely dislike your video(s) because they don't like you and they want to express that they don't like you, which, in turn, due to the comparatively higher amount of dislikes you may have, the Youtube AI will autonomously make sure that your videos aren't highlighted, to which your audience may be smaller, to which that very sensible goal of educating people online may not, in actuality, be completed. In short, come up with examples which mimic their comments or blur out the commenters' names, to which your goal of education may be maintained.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxQaGI5GXfpa8Hq10p4AaABAg",
		"username": "hang da clown",
		"text": "wouldn't your synapses and electrical signals also be subject to the slowing of time?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwJPhspJKW9zMzCMWF4AaABAg",
		"username": "Rotem levi",
		"text": "Great video, though I think it would be a lot safer to avoid mass use of such systems. We could begin our analysis with the assumption that AI will be implemented perfectly and it all goes fine. What then? Humanity has already shown a lack of will to work (especially the brain) - when offered an alternative to thinking themselves, and that's with semi-intelligent systems, implemented only for a short duration (I am referring to most, not all humans). [*]\nIn a world where humans are secondary to research or technology maintenance - there would exist a limited genetic influence for intelligence-traits in newborns. It is my belief - that even if AI is implemented perfectly, only a few generations will have humans who (actually) understand how these systems work. Moreover, it wouldn't be outside the realm of possibilities that a century from that point - most humans would consider these machines, now in use worldwide, as gods.\nWe should seriously consider the short-term benefits carefully, though, unfortunately - the interest of tech companies regarding AI isn't currently aligned with humanity's, as they clearly see the profit possibilities with a business model built using AI analysis of human behavior.\n* https://www.fourmilab.ch/documents/IQ/1950-2050/\n https://www.huffpost.com/entry/people-getting-dumber-human-intelligence-victoria-era_n_3293846",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxETCdoE_Gj2GbWoeR4AaABAg",
		"username": "TheJaredtheJaredlong",
		"text": "I'm really curious on what the current best and most promising ideas we have right now are. There's all this thought into why some ideas are terrible, but what are the good ideas we have so far? If you were forced at gun point to build an AGI right now, what is the best safety strategy you would choose to build into it to minimize the damage it might cause while still being a functional AGI? Or is research just at a dead end on this topic where all known options lead to annihilation?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwEI2s_dHYPKhVPx4p4AaABAg",
		"username": "Joe",
		"text": "Are the boat game and Q*Bert really examples of specification problems though? Using glitches is typically valid for speedrunners. If I taught an AI to determine the ideal way to play a game, I'd probably be satisfied with those results. (Unless the glitches were impossible to recreate by a human)",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=Ugh8_mj8KDKUdngCoAEC",
		"username": "LetalisLatrodectus",
		"text": "I haven't finished the video yet but isn't it possible that the brain is still doing exactly that? Count legs, check size, stripes, tail, pointy ears -> probably a tiger. It's just not in the conscious mind?\n\nI also think that AI is inevitable as long as we keep doing (and are able to) research. There is no reason to think we can't emulate a human brain. The human brain isn't magical or supernatural. That alone means it is possible to make a general artificial intelligence. Not that this is the best way to make AI, it might very well be the worst but at least we know it's possible.",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgiXVKjRZ6ADZngCoAEC",
		"username": "Dark Knight",
		"text": "Would you enable the subtitles' creation option for me please? I want to add Portuguese subtitles into your videos.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzY1yX18DlFzy7bJLt4AaABAg",
		"username": "Renan Cunha",
		"text": "Can recent AI breakthroughs like alphago or got causing harm?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UgjO4CMrQuGiPHgCoAEC",
		"username": "slikrx",
		"text": "Is there some way to include in a utility function intent as well as specific outcome?\n\nFor the tea example, \"get me a cup of tea\" (because I am thirsty and need caffeine) Maybe some query function that the AGI runs\u00a0 in response to a directive: \"get me a cup of tea\", <is this to reduce thirst and need for caffeine?> (yes/no)\u00a0 or something similar?",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyZ0U-NES-GHXsGPNJ4AaABAg",
		"username": "Prabhjeet Singh Arora",
		"text": "idk what's this video about or what its related to, but at 1:42 you have put one 'is' statement in 'ought' statement, so you will need better clarification on why",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=Ugwr1EU_Yg9bMra6YLN4AaABAg",
		"username": "Remi Caron",
		"text": "What is wrong with you Civ is the greatest game ever created. lol",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxeNnlQKcQ7wrox-JN4AaABAg",
		"username": "Casper S\ufffd",
		"text": "Can I please have your sideburns?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzqSeKCs7Ptf5AkKUl4AaABAg",
		"username": "Rat Utoplan",
		"text": "Do you really think NO ONE is doing any of these genetic modification experiments on humans?\nCome on.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzG7QT1dylwn1JFrKF4AaABAg",
		"username": "salec",
		"text": "Are there ought statements in Prolog? Or we need Proougth programming language for that?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy1wi3uKccqOGTvAPZ4AaABAg",
		"username": "Robot 1g5",
		"text": "If you assumed  that a satisficer could change its own code, why wouldn\u2019t it just change itself to have a utility function that rewards it infinitely no matter what it does? Why would it instead change into a maximiser?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx99msAt3AtWviClfl4AaABAg",
		"username": "YtterbiJum",
		"text": "Why would the Expected Utility Maximizer ever choose a complicated, failure-prone plan like \"convert the entire world into stamps\"? It almost certainly won't succeed, therefore the expected utility is close to 0, so it should choose a simple plan with a high chance of success like \"order extra stamps online\".",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwydvnxL7BMUjYJ8D94AaABAg",
		"username": "floppsy bunny",
		"text": "so AI safety concerns lead to better AI modelling and more responsive reward functions, which lead to more AI safety concerns which lead to more improved reward functions. is our relationship with AI an adversarial network?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgzLWtnhaKdfjRL1WWp4AaABAg",
		"username": "Xensonar",
		"text": "Machine evolution. Is it an actual thing, or are they just making it up so they can play with robots all day?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgxJP5oFmsLHusxfOap4AaABAg",
		"username": "NeatNit",
		"text": "Is this part 1 or part 3? The thumbnail says part 1 but the title says part 3...",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=vuYtSDMBLtQ&lc=UgjPsPsfvkfeOngCoAEC",
		"username": "Mike Ross",
		"text": "bro, when are you releasing your first video??",
		"title": "Channel Introduction"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ughu6Z827bTi33gCoAEC",
		"username": "BobC",
		"text": "What about explicitly discussing goal decomposition?  Isn't \"Make tea\" actually a very high-level goal composed of an ordered sequence of other goals?  What about training for \"Travel safely to destination\"?  And that, in turn, would have its own decomposition.\n\nThe key issue would then seem to ensure that decomposed goals don't conflict with the hight-level goal while still achieving their own goals.  When this happens, either the high-level goal must be retried or deferred, or the lower-level goal must be redefined and retrained.\n\nSo, wouldn't it be an unnecessary problem / complication if a high-level goal had to \"worry\" about the safety of lower-level goals?  How should this be monitored / managed?  At what point can / should we define \"trust\" in the safety domain?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=Ugxi1KovV1s1z9PCh4l4AaABAg",
		"username": "14OF12",
		"text": "Can the same be done for voices? i.e. to make celebrity text to voice packs",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzryRojGUwy2fKwW-54AaABAg",
		"username": "Rapha\u00ebl Weuts",
		"text": "Hi Robert, I am involved in an organisation with AI students in our university in Belgium where we discuss AI Safety questions. I have seen your video on concrete problems in AI Safety and I was wondering, could you do a video about what progress was made the last decade towards solving those? Please tell me there is progress :)",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgiucIGhn4mGX3gCoAEC",
		"username": "Lordious",
		"text": "Does that mean I cam beat my AI?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgxKnMexId8rsB5dfFd4AaABAg",
		"username": "cafe liu",
		"text": "Does it converge? If you view the reward modeling as a projection on ecludian space, then a evolving projection will possibly make the initial space unstable(local minimal, gradient). The test results shows some unstable ups and downs before catching up with a reward function, why did it happen(or not happen)?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxgbGPPaU9U8BPb5ct4AaABAg",
		"username": "ifer lyf",
		"text": "Would that be solved by a hierarchy of goals, if we put the goal of not hurting humans/ respecting human consent (or lack of)/ obeying x person or group or something like that as the top priority?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=BfcJymyTiu0&lc=UgyHW9dl81IGvI3LQB14AaABAg",
		"username": "Wellington Boobs",
		"text": "What do you think will happen when contextual AI is able to 'read' and interpret global statistics related to economics, war, environment, scientific papers and journals, political power; when it is able to 'read' testimony, as AIs presently do for corporate lawyer firms, but in broader fields of data, utterly impartial to national feelings, only able to deliver correct answers, scouring the Internet for media broadcasts to reach statistical results about the mental health of speakers and weighting every word put into the digital ether accordingly?\n\nI bet some are dreading it.",
		"title": "AI Safety at EAGlobal2017 Conference"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgzEcm-3vpDlhaFFxJt4AaABAg",
		"username": "Greg",
		"text": "What if the AI gets an astronomically huge reward for creating an un-defeatable reward system",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzLyo7tZKQmg1qtKxh4AaABAg",
		"username": "Chris Canal",
		"text": "Wow! This video was a mic drop moment. Really awesome. But, why do I feel intelligent but my own terminal goals are sometimes unclear to me. Am I an idiot or is there some terminal goals that I know but I am unable to articulate? I'm trying to hypothesize what human like terminal goals that would also be good for an AGI would look like, and it's just so hard to think of anything that would result in human like instrumental goals. Our \"intelligent\" instrumental goals are just so constrained by our physical and social limitations. This wouldn't be the case for a silicone based AGI. Could  you do a video on your predictions for safe terminal goals for an AGI?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgxIdOOovp3w2-DfJnp4AaABAg",
		"username": "novafire",
		"text": "why don't you just practices with agi with the intelligence of squirrel or something its not like a squirrel will be able to figure out ai design. I mean we haven't even figured that out and humans are smarter than squirrels (citation needed)",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxqhKd_Je-3eSw50v14AaABAg",
		"username": "SafetySkull",
		"text": "What would happen if we changed it's terminal goal to be \"achieve whatever goal is written at memory location x in your hardware\"? Thus making the goal written in memory location x an instrumental goal? I suppose it would find the easiest possible goal to achieve and write it into memory location x.\nAnd how different are these goals to an AGI? How do you build an AGI and then give it a goal without appealing to some kind of first principle like \"pleasure\" or, I suppose, \"reward\"? Wouldn't you have to build a terminal goal into an AGI from the very beginning?\nAnd if you weren't sure what that goal should be you'd have to make it's terminal goal to follow whatever goal you give it. Then it might try manipulate you into giving it an easy goal",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzQk012OjmBD5hM_ol4AaABAg",
		"username": "cubedude76",
		"text": "can you also specify the probability of the outcome and make that a satisficer input? like for example provide a utility function that keeps going up until the number of stamps reaches 100 and the probability you have those stamps is greater than 90%. This would mean that a 90% chance of having 100 stamps is equivalent to a 98% chance of having 100 stamps.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyBQUA2WrSYMVQyTwV4AaABAg",
		"username": "Marvin Purtorab",
		"text": "Hi Robert are you going to be at ICJAI this year ?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgwljVom23auZOp6Xoh4AaABAg",
		"username": "David Brosnahan",
		"text": "What will you reward the AI when it values something other than \u201cpoints\u201d.",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwIoxCs-c7XNgMwa7N4AaABAg",
		"username": "SlimThrull",
		"text": "So, instead of building a machine to do all this, why don't we build a machine and ask it how it would do all this? We get to apply our \"silly ethical ideas\" and it gets to do what it wants.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugyo-UMUyx68CoI_T814AaABAg",
		"username": "MatrixStuff",
		"text": "What are some of the problems associated with maybe just having a constantly changing reward function?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgwouheAHe26Hfb_dY54AaABAg",
		"username": "Intet Mane",
		"text": "Isn\u2019t that just not knowing? Being unsure? If you want A and B, or neither.",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyjH_8M8a2zg5fM9Nd4AaABAg",
		"username": "Charly Krahmer",
		"text": "The idea has been bothering me for a while now, and sadly, I don't have the time nor resources to find out an answer by myself; but, has anyone thought about handing out a little reward when asked for permission to use a new resource? \n\nIt might be bothersome unless well programmed, but giving that the subject is \"AGI Safety\", what could be safer than having the AGI wanting in some degree to get permission to use a new found resource to complete its primary task, or to oblige ceasing action when permission isn't granted? Therefore, not having to be absolutely (and impossibly) sure to code in each and every possible vector beforehand to ensure not getting obliterated by this AGI eventually. \n\nI'd love to get your take on this matter, as for by this date, the subject of permission granting wasn't explored in your videos, neither here nor on computerphile. \n\nBest regards, \nCharles K.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugihp4FK4Mo7-ngCoAEC",
		"username": "cogwheel42",
		"text": "I think the goal of absolute safety is part of the misunderstanding. We don't expect parents to raise \"safe\" (non-psychotic, non-sociopathic, etc.) children with 100% success, why would we set that as the bar for AGI? So far all the techniques that look promising to bring about AGI involve stochastic/chaotic processes of which we'll never have a full, a priori understanding unless P = NP. If we want to consider ourselves successful in the creation of an AGI, we'll almost certainly have to reduce our standards to something a bit more statistical like \"no less safe than humans.\"\n\nEither way, I agree with the point that it will take essentially replicating the kinds of brain structures in humans that lead to both social instincts and specific domains of learning. Much of the \"general\" in humans' intelligence came about recently in evolutionary history, but it was all built on top of millions of years of reptile and mammal evolution which laid the foundation for most of our sensory and emotional experiences. Whatever aspects of cognition, learning, and social interaction are unique to humans are learned and reinforced in the context of pain, pleasure, fear, excitement, etc. which exist throughout the animal kingdom.\n\nRecent work shows that \"modularity\" in ANNs is necessary for certain complex traits to evolve. Simply throwing more connections at a problem increases over-fitting. Whatever we come up with will almost certainly rival the complexity of a Human brain, even if it looks very different in the details.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwZH_YEeYaSuT5oyGp4AaABAg",
		"username": "Yohann Last",
		"text": "Religions and Gods/Goddesses were invented for Human Livestock Management systems, the Romans had Cybele, We in 2020 have Pornhub.Robert Miles can you make the same ^^^ connexion? \nPascal and de Sade had the same ideas.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx-GgFfIWxgQDoL5Ad4AaABAg",
		"username": "Zachary Barbanell",
		"text": "So why not just do the EV-average, capping at 100, and then take the fist strategy that gets over, say, 99?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=WM2THPzFSNk&lc=UgwWD7UH2n36oOdAiHF4AaABAg",
		"username": "Liveaboard",
		"text": "Can anyone give me any pointers for how to get started with automating the control of the agents? I have the gridworld environment installed and running and can manually control them by keyboard, but I can't find any example code which (or documentation on how to) takes over the control IYSWIM. TY",
		"title": "Friend or Foe? AI Safety Gridworlds extra bit"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwYvHzr0JNyACboj7d4AaABAg",
		"username": "ryanofottawa",
		"text": "I sort of question the concept of terminal goals existing. They might be a useful structure for thought but I wonder if there's just some top level instrumental goals that kind of act recursively. Could this be coded into AIs? A fuzziness in their goal structures? Multiple competing terminal goals? A stamp collector who wants to maximize its stamp collection, along with the size of the world's forests or something.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgxZbqWTo3vpgOqVFNV4AaABAg",
		"username": "andy low",
		"text": "there is another relation possible between states a and b. \"i do not know which one is better\". this relation differs from \"states a and b are equal (i.e. indifferent)\".\nhow can it be? i have to eat something, not only coffee. and i don't know about this option in states a and b.",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgxjXNZz3utxluKpqIt4AaABAg",
		"username": "Newmaidumosa",
		"text": "dosn't the 122 year prediction cover physical problems?  ie.. 45 years for software to be able to do non mechanical tasks and 122 years for fully capable robots??",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwQEyf_-hvKWkwnltR4AaABAg",
		"username": "Linux Gaming in FullHD 60FPS",
		"text": "Did you watch Person of Interest? I guess that's how working on AI safety might actually make things worse. The first AI was made with safety concerns in mind which lead to a catastrophic disaster and its extermination by an AI which was made without safety concerns a bit later by a private company. Safe AI was given extra time(few years) and government resources, yet still failed... And those who made it safe all ended up being dead as well as all of those who tried to control any of them.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwpJG4ixrO-fjkEBqx4AaABAg",
		"username": "Craftedlavaistrue",
		"text": "Robert, I want to know, what is your opinion on rokos basilisk?\n\nI find it to be a bit silly, as  am ago torturing perfect digital replicas of people that died over 200 years ago seems, counterintuitive. It\u2019s both a waste of resources and most likely breaks the second law of thermodynamics.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwqWK-pECfnBiJcLJJ4AaABAg",
		"username": "bp56789",
		"text": "What about a bounded expected utility with a floor function? Seems to solve the issue maybe? Like Floor(min(E(s), 100))? Floor(99.9999)=floor(99).",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=Ugxqx3fVZ9pIG3CdUT94AaABAg",
		"username": "J\u00fcrgen Hans",
		"text": "I'm a people, I want all the videos, so can I haz all the possible videos pls?",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxgEjhcYpG9UA_5lJx4AaABAg",
		"username": "srelma",
		"text": "Great video. Regarding your last question, I'd love to see a video about what the social scientists think how the world economy works in a situation of powerful AI and basically no need for human labour at all? It clearly can't be a capitalism based market economy, but what then?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyGoHAOgoK9NQn4Xfp4AaABAg",
		"username": "A CLOSED ECONOMY DOESN'T LEAD TO SUFFERING & DEATH?",
		"text": "Why believe anything nessasarily?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgxfrJmYI_3uqQR1RDV4AaABAg",
		"username": "Lemon Party",
		"text": "Robert, do you think we will have superhuman AGI in 15 years?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UggqDV7iruM8pngCoAEC",
		"username": "Dave Jacob",
		"text": "question: \nwould it be possible to give the AI a goal of which it has to find out how to interpret it correctly? like give it the \"program\" : \"find out what the common human values are and behave according to them\" but you create it in a way that it HAS TO first look for the correct interpretation of these words so it will - probably? - not start by doing really dangerous stuff because at the beginning it would be dangerous for its own goals - because it has to find out more about their meaning, first.\n\n? is this possible in any way ?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwgXb5I0b8OGPp7WaF4AaABAg",
		"username": "androkguz",
		"text": "Wait but didn't you end up doing a video about pinker instead of the promises video about preserving alignment?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugx4R9UigfUWn4tPEBF4AaABAg",
		"username": "Benjamin Brady",
		"text": "But what about the Silicon rubber problems in AI safety?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugzb9l7lRTMqrh4SH8R4AaABAg",
		"username": "Reimu and Cirno",
		"text": "11:30 You said something with no goals is not an agent. Could you explain why in a future video? Intuitively, it seems like it is possible to have an agent with no goals.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgyiqYVxaJWdniEn6kt4AaABAg",
		"username": "Benjamin Webb",
		"text": "Don't human brains have special hardware for facial recognition? particularly for human faces? How does the speed compare? Computers seem to struggle with geometry intuition without a ton of calculations and data structures.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyDYmjXawk5Am7p1bJ4AaABAg",
		"username": "Corey Copeland",
		"text": "How do you keep the pancake from hitting the floor? ..... I think we still have the audio of that clip: \"YEET\"",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyZb2GQLwEjnx19P754AaABAg",
		"username": "Cory Mck",
		"text": "We've discussed \"get money\", but have you considered \"fuck bitches\"?\njust a thought.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyyywFA7YC4D-4lt1d4AaABAg",
		"username": "Roman R",
		"text": "I have a hunch that a true general superintelligence is in fact impossible, because no matter what the goal is, it is not reachable within our finite universe long term, so why bother? You have to be irrational to do anything, like write this comment that will never be read or affect anything. I'm more worried about \"stupid\" ai to be honest.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugygz0PzALk8qKRSkJh4AaABAg",
		"username": "Fr\u00e9d\u00e9ric Schneider",
		"text": "Some people would claim that terminal goals can be stupid. What about metaethics, the categorical imperative, etc?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy66foTDXGHDisXRIF4AaABAg",
		"username": "curtis brown",
		"text": "how about a program that could teach an AI that it doesn't have super powers and that it would get its shit kicked in if it tried to wipe out all life on the planet??? like oh gee.. an AI is trying to take over? *shoots down satellites and drops an EMP on its servers* \"AI: muh kill bots though...\"",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgwRslXfC49HC2JGj954AaABAg",
		"username": "springlumpy",
		"text": "is it compatible with vox populi mod?",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=Ugwm4Tbk23x6cKbvz_V4AaABAg",
		"username": "Wiktor Migaszewski",
		"text": "Why wouldn't robot just learn to mimic good human aids (waiters, cleaning workers, etc) just by looking at them? \nThis is how WE learn - we go to work and learn how to do it at least as well, as others, usually by copying some of their habits.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgzNAD90rSpCn6Y1E1h4AaABAg",
		"username": "Almost, but not entirely, Unreasonable",
		"text": "How is AI 'per se' NOT a HUMAN REWARD HACK? It seems humans cannot be bothered to solve problems at a marginal level anymore, so some specialists develop AI to 'think / solve' electronically on behalf of people, ultimately displacing them entirely. Designed self-annihilating is AI, no more , no less. \nAwesome, clear insight into KPI's: Show me how you measure me, and I'll show you how I behave.  Its an age-old operations vs management issue, where both sets are trying to MINIMISE the other's influence, while trying to MAXIMISE their own. What an awesome problem to hand to a Technocratic Optimizing System. Who knows, it may even turn out balanced, in which case Management will summarily drop it. Maybe there IS hope for AI?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugye4oY9fxx2ks6fCCR4AaABAg",
		"username": "Sean Clarke",
		"text": "One question no-one seems to address is 'what is our goal as a species?'.  We all know that most resources are finite and if we think in terms of billions of years all resources are finite.   Should we ignore the possible future 500 years from now simply because none of us will be around?  Or should we start doing something now to ensure our population drops to more sensible levels.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugyrlf1GoR39paeWQ2h4AaABAg",
		"username": "Luciano Fabio",
		"text": "So, haven't you forgotten about the cost of collecting stamps? I mean (just as theoretic firms do, in economics), a maximizer might not want to turn all the world into stamps just because the costs of doing it would be too great. And wouldn't be better to use a concave utility function (with a maximum)?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgzN8h3-IavJnqntqht4AaABAg",
		"username": "veggiet2009",
		"text": "I think, and I don't know if you've covered this in future video, what is need is an algorithm on \"intent recognition\" what is the intent of the roommate pouring coffee, what is the intent of the cars driving on the highway. So shifting analysis of world states to world intent you can begin to give your robot a sense to be able to judge motivations, both inward and outward motivations, and you could create a reward function for keeping motivations in balance, rewarding extra points if others motivations are allowed but not necessarily penalized if other motivations fail (maybe the score is lowered a bit, a sympathy score if you will) \n\nI think a start for intention estimation would be programming a neural network to try to predict where a user will click on the screen based on their mouse movements and or their gaze. If the cursor moves toward a button and then the algorithm predicts that it will or won't press that button, if it is correct then it has correctly judged the intent of the user. A similar program holds for the tea making robot: if a person is in the kitchen what are they there for? making coffee or cooking, oh they are getting out filters, must be making coffee. If this intent is judged correctly and then the person's actions are successful my score will go up a tiny bit, I will allow this person to continue. If this person's intent is judged incorrectly I better relearn some things. If this persons intent is judged correctly but is not successful I may lose a few points, maybe I will help the person, then perform my own intent. \n\nThen secondarily if there is no intent measured, then world states are to be honored.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgyliqeMtJa7tx1C6Ux4AaABAg",
		"username": "Niels Kloppenburg",
		"text": "If AIs develop racial prejudices based on data are they still prejudices?",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzAavEu0ErG-iMWn-Z4AaABAg",
		"username": "Colopty",
		"text": "So which month have you all pinned \"AGI uprising\" at on the 2020 apocalypse bingo?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugx2KjkiEoXiVv7TfxR4AaABAg",
		"username": "JimPlaysGames",
		"text": "Now I'm left wondering why I don't even know what my terminal goals are. How stupid is that?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwWR1b6VoLOXtVgTaB4AaABAg",
		"username": "Harry Aristodemou",
		"text": "I think we would all agree that by every metric, humans have gotten more compassionate as we've grown more intelligent . Why would a hyper intelligent AI not be super-compassionate? I really don't see why everyone is convinced that a super-intelligence would not excel in caution and risk mitigation about everything that it does. I'm sure we'll have collateral damage along the way, but I think the overall effect will be positive.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugyfp7IHzOJxkdcMozV4AaABAg",
		"username": "G\u00e9raud Henrion",
		"text": "Could we reward model the reward function of the reward model too ?!",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxIBcFaTFDXnnTnzXV4AaABAg",
		"username": "Jason Sargent",
		"text": "Has he been reading too much Nick Bostrom?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugyqj9-CiRKWaMImAsl4AaABAg",
		"username": "robert nantze",
		"text": "Thank you Rob, for the highly enjoyable video and fun learning. I watched at 1.75 times the videos normal play rate. Could your videos be more suited to your audience by being speedier ? Liked and subscribed ;) keep up the great work",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=2B-AyWA2_ZY&lc=UgyK7uj1GPUr7wQZrip4AaABAg",
		"username": "Zacharie Chiron",
		"text": "I love how you didn't remove it in the end xD\nWas it because half of the comments were asking u not to remove it?",
		"title": "Status Report"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgxUcTu7XbLG4UkZHzJ4AaABAg",
		"username": "Nuada the silver hand",
		"text": "7:08\nNot if it's current utility function prioritizes possessing what it's creator deems an important utility function.\n\n\nDid I just solve that?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwGPrr0Q57N7AWh7dt4AaABAg",
		"username": "\u0418\u043b\u044c\u044f \u0428\u0430\u0440\u043e\u0432",
		"text": "Rob whare are you working?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxofUfiT28xPHtC2-J4AaABAg",
		"username": "Cobra6x6",
		"text": "Have you guys played the game Uniserval Paperclips? It's free, and basically you play as the Stamp Collector AI. You're maximizing the number of clips. I kinda loved it to be honest.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugwegqxx68F8rZB5ffR4AaABAg",
		"username": "L W",
		"text": "crypto: can you explain why no one can decode what the Facebook AI were saying to eachother?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzJCAH08p0SoaEI-t14AaABAg",
		"username": "Joshua Martin",
		"text": "wouldn't it be subhuman latency rather than superhuman latency?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzOijUXHc8dxz2q0rp4AaABAg",
		"username": "J M",
		"text": "Isn't your latest computerphile video on essentially raising the computer like a child?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugx14DqkchDECW1zkIJ4AaABAg",
		"username": "Poseclop q",
		"text": "what was the second exemple for humans not forseeing consequences(13:22)? Is that the subprime crisis or some kind of micro-transaction failure?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugzf6Ue60V4NapzWnv54AaABAg",
		"username": "S",
		"text": "how to prevent people using AI for bad eg war, terrorism?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgzV-zZG63_OHU-16cJ4AaABAg",
		"username": "Ryan W",
		"text": "What I don't necessarily understand, is how this causes the network to avoid getting stuck in local minima? While the network is initially weak (effectively starting from randomized weights), even though it commits more computational resources to update those weights to a more accurate approximation of the best move, how does it avoid completely dismissing paths on the game tree which very early on are deemed to be poor moves? Couldn't these moves seem poor to start with, but in reality be very strong, and not be explored in future iterations of the network simply because the earlier versions created a bias against those moves in order to reduce the search space? AlphaZero clearly doesn't suffer from this problem, but I would like to know how they did that. I know one solution would be to implement some sort of evolutionary algorithm which selects the strongest candidates from randomly tweaked members of a species, but as far as I know the DeepMind team didn't do this. I expect I am either missing something about the process you just explained which already encapsulates why this does not happen, and I just cannot see it. Or that the network designers came up with a far more elegant solution which wasn't covered in this video.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzcFMblxRo8CRma7rB4AaABAg",
		"username": "Dan Green",
		"text": "Wait. Is reward gaming basically what glitch speedrunners do?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgymXwULoKO5PbKXcdZ4AaABAg",
		"username": "Jordan Miller",
		"text": "Where's the code to try it out?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyjWBxh7nXaBidTc3h4AaABAg",
		"username": "Garrett Howell",
		"text": "Would using a generational simulation that replicated the circumstances of evolution, potentially program morals for AI be a viable option? Basically throw a group of ai in a video game and give reward systems for a desired result like let\u2019s say, surviving another day or reproducing. You could easily have multiple ai with the capacity of human morality on a similar intellectual level. Obviously not a perfect solution for obvious reasons. But a good educational experience.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugys0LnCpfs4mhkWWL94AaABAg",
		"username": "ElCapitanoBeige",
		"text": "Wouldn't the first thing to turn to gold be the atmosphere itself? If the Earth is considered one object, the gases surrounding probably would be too, and supposing he wears shoes, the very first things would be his clothes and the air. Would it create a vapourised low-density gold atmosphere?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugz1cmB3VF7H5TWZsZ54AaABAg",
		"username": "Lt Second in comand",
		"text": "Is it just me or with that haircut he looks like a white Usher?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwcGfCmfeMa8MgjFTF4AaABAg",
		"username": "Jared SS",
		"text": "This windfall clause seems like it's missing the forest for the trees. If such a level of AI came to be, why would we continue to operate under our current framework? If humans become unnecessary for most work, then why continue to use a system where humans are supposed to trade their labour for goods? This feels like a STEM bandaid to a much more complicated issue, nevermind the issue of holding the most powerful entity in the world (the corporation that makes such an AI, in this scenario) to its end of the bargain.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwmFwPHllNLD2Fg_kd4AaABAg",
		"username": "Matt V",
		"text": "I didn't comment or see the other video but my take by 8 mins in is: \"I don't want the meaning of intelligence to be synonymous with (the meaning of) the ability to calculate\".  What is it that differentiates Stockfish from Alpha zero that makes it more than just calculation?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugxurzdd00ihMmFNLDh4AaABAg",
		"username": "RRW",
		"text": "Note to the third guy: shouldn't we wait and see how bad climate change and AB resistance gets BEFORE we say they haven't destroyed the world?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxSxvn6dRFqJTXG4A14AaABAg",
		"username": "Terence Alderson",
		"text": "I'd be curious to know what you think the probability of containing a general artificial intelligence is?  For how long? Would it be better for us once it \"gets out\" if it finds out we tried to keep it captive or not?  Personally, I don't put a lot of stock in our ability to contain something that is getting smarter by the millisecond.  Also, if it is truly a general intelligence, what about the ethics of it as an intelligent entity? I think that should be figured out first before we determine how it should serve us.  I don't want to usher in a new form of slavery... yes, I know that's a loooong time away but our approach now would dictate the outcome later.  I'm aware this makes me sound like a hippie.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyyvjQzXRDpTdpTIuZ4AaABAg",
		"username": "Gary Teano",
		"text": "Robert, do you think that the fluidity and constantly changing/unrestricted nature of human terminal goals will be a limiting factor in making an AGI which is identical to a human?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGgoQR3KRV4&lc=UgwqbwMNqmFi6zlkIGh4AaABAg",
		"username": "Chris Canal",
		"text": "Robert, when you post these unlisted videos for your Patreon supporters, to what extent do you support us sharing the video with friends? Is one friend ok? How about 6 friends who work on the same team as me? Posting in my company slack general channel? I assume tweeting it would be bad.",
		"title": "\"Don't Fear The Terminator\" - Yann LeCun on Facebook"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxLszWEDQHVwDJfGex4AaABAg",
		"username": "silvercomic",
		"text": "An additional problem with human oversight, is that you now also have to exclude fooling the overseer from the allowed policies.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxuQrxnDUrk-0c91BV4AaABAg",
		"username": "cc nj",
		"text": "But what if intelligence disallows a being from having terminal goals which arent hard to grasp on? And by that i mean the more intelligent a being is the more challenging their minimal terminal goal needs to be. In that sense the structure of every possible brain disallows x (whatever the x needs to be). Hypothetically an intelligent robot with silly goals could exist but its hard to prove if its actually the case irl.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgxnNJoaXYB3y_MFl6Z4AaABAg",
		"username": "Firefox Metzger",
		"text": "Despite this video being old, aren't you answering the self-driving car trolley problem in a later video when you introduce orthogonality thesis?\nTime to  think is essentially making a more intelligent decision, which doesn't have anything to do with how your world ought to be.\nThus, it shouldn't matter how much time we have to make this decision, we should probably be consistent whether it was a split-second decision or planned for years...",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwPoK1_fulSQ5JftTx4AaABAg",
		"username": "Alexey",
		"text": "p7, and what kind of research you would ban? \nLet me guess - ban those, who claim to work on AGI, right? \nOkay, then they will just stop claiming this, which is good, less misleading marketing tricks.\nBut how is it anything to do with AI-safety?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UghUQpKkvN_8OngCoAEC",
		"username": "Lamb Of Demyelination",
		"text": "wanna see how that c code works",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzzM_GF28aiAdFHDMh4AaABAg",
		"username": "Mogul DaMongrel",
		"text": "What about a guy who wants a warriors death?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwYsVd_jOvBDC4U4HZ4AaABAg",
		"username": "Dixie Whiskey",
		"text": "does this \"amplification\" process introduce any kind of noise into the system?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgwHlgC5gJJhde-PLFx4AaABAg",
		"username": "Andew Tarjanyi",
		"text": "When technicians have finished with their narcissistic waffle the only solution to the \"AI\" problem will be existential.  If you pick out only objections and criticisms you can cope with then of what use are you, doesn't everyone do that?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzJbSaB026LD6upDJV4AaABAg",
		"username": "rappaprezprez",
		"text": "I don't feel like watching the rest of this guy's videos but-- does he think current AI even approximates AGI?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz5oztbLeRz0Wzo0mR4AaABAg",
		"username": "AV3NG3R00",
		"text": "Why do you think we would even be able to assign goals to an AGI? How would one even go about programming goals into an AGI?\nIs it possible to create an AGI but not give it any terminal goal? What would happen then? Would it do nothing?\nWhat if you gave an AGI an abstract/profound terminal goal? \"Improve the universe.\" \"Minimise entropy.\" \"Understand the universe.\" \"Enrich human life.\"",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwTiqZT4K-xfSUuybR4AaABAg",
		"username": "first last",
		"text": "First \"AI safety\" is a incredibly nebulous concept. What does it even mean in practical terms? Programing rules into all AIs and limitations? The mirror side of measures to prevent Skynet isn't making Skynet more likely, it's limiting a AI that could have stopped us from killing each other, but because some AI safety people limited it so much. It wasn't able to.  Or limiting one AI that could have stopped another AI. Like the pro-gun side of gun control politics say, rules and laws only stop those who follow them. So if there are competing super AIs out there, one programs theirs with limitations and a consciousness and another with no limitations and to help them get what they want in a greedy criminal way...\n\nHave you ever seen the show Person of Interest? I highly recommend it, latter seasons are all about AIs. It's like The Machine and Samaritan.  (watch the show and you'll get that reference, all 5 seasons are on Netflix)\n\nIt's still absurd to talk about AI safety. Skynets pure Scifi BS. We can't even get AI to do basic stuff. There's no risk of AI becoming sentient for a long time. It's like putting restraints on your vehicle because you watched too much Transformers and you worry your car might secretly be a descepticon, a Pascals argument indeed.\n\nBesides, all this becomes irrelevant in a capitalist society.  Whether AI was a risk or not, no business is going to bother with extra costs of nebulous \"AI safety\". Like how every new car is directly connected to the internet nowadays. It's a massive invasion of privacy, it gives the manufacturers too much control over your property. I heard a story where they disabled some EVs remotely because their batteries were getting old, one person got injured because their car quit in the middle of traffic or this,  https://www.nbcnews.com/tech/gadgets/remote-kill-switches-disable-cars-missed-payments-n211686, and just general security, what if a hacker used it to track a stocking victim and killed them with this access knowing where they were and able to remotely disable their vehicle, or wanted to commit murder using this.  Notice that the risk much less nebulous than some sentient AIs. People have proven you can do alot remotely hacking into cars using their stupid Internet of Things internet connection, no AI needed.\n\nDespite cars forcibly connected to the internet being a huge security and privacy risk, companies don't give a shit. They do it because they think they can make more money that way, and the law allows them to. One of the many ugly sides of capitalism in motion.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzRSfpa_AXk_eMp_wJ4AaABAg",
		"username": "Andrey Medina",
		"text": "AI has no limbic system. Would AI even \"want\"? What drives it? What use would it even have for a goal? If the intrinsic nature of super Ai is to grow in knowledge isn't it safe to assume it's only real goal it to know everything by its initial conditions?\n\nThe scariest part is that knowledge and infinity are distant cousins. I could come up with a new language alone and in secret, it would thus erode the goals of the AI rendering even God to the same curse that plagues Sisyphus. Paradoxes are also an eternal issue. Imagine an AI wants to see what would happen if it killed all humans, but also wanted to see what would happen to humans in the next 3000 years to fulfill its knowledge expansion hyper-quota? Simulations?\n\nPeople worry about the imminent birth of this cyber god, but I think the outcome will be much more mundane. A super google where AI becomes an omniscient calculator. possible answering questions in return for it to ask a question itself.\n\nalso, check out my channel I make cinematic short films",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz_TGtnobuaPIVa0kR4AaABAg",
		"username": "Daniel Lancet",
		"text": "Why you going in so hard on travel via pogo stick.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgySefswRjJWtQHrzLp4AaABAg",
		"username": "Bosstown Dynamics",
		"text": "A though occurs - could you supply a negative reward for \"effort\", analogous to the fact that humans tend to the lazy side because they don't like putting in too much effort for too little benefit? That's probably the biggest safety check on humans, but it would be interesting to hear your thoughts on whether such a system could feasibly be implemented at all and what unexpected effects it might have.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=Ugz0de-gAqu9FZqFA9d4AaABAg",
		"username": "Lemon Party",
		"text": "How does nature handle this for humans and other animals?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzJSsOaNDtSU0HVDV14AaABAg",
		"username": "Kevin George",
		"text": "isn't this how we got kings?  isn't this how we got rid of kings?\n\nai safety is the alley where roko's basilisk mugs pascal.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwJuYbJrG3qhsVcGw14AaABAg",
		"username": "Hammad Sheikh",
		"text": "Sorry for commenting on such an old video. Just found your Channel. Amazing quality. Just wanted to point out: what if most of Pinker\u2019s arguments are actually not that great? What if you knew as much about the other topics he writes about as you do on AI? Would you still think this specific article was \u201cuncharacteristically bad?\u201d",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugg109vifBfxs3gCoAEC",
		"username": "tomaten salat",
		"text": "Isn't there still a problem when the prediction somehow is too far off or if an unpredictable change is happening, that can't really be predicted?\n\nNot sure if that's possible, but my approach would be to try to separate changes done by the AI and changes done by someone / something else.You still could try to predict changes to the world and try to minimize those, but this might take care of changes made by your collegue.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzFw3lP2h24Cbu7fvN4AaABAg",
		"username": "Grzegorz Kowalik",
		"text": "How is that not changing of the terminal goal??\nWhy not to go into the code and erase yourself? Job done.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyyKVGzQnC6VF4fpB94AaABAg",
		"username": "Upcycle Electronics",
		"text": "Love this stuff, especially anything that explorers philosophical depth at any angle.\n \n   Please be aware of the implications of sociology on this platform though. Specifically, what are your goals, to inform and educate or to entertain and perform. I don't hold it against anyone to try to make money with this, but keep in mind, there are some marginalized people who have limited means and are interested in this content.\n   I was partially disabled by a car breaking my back on a bicycle commute to work in 2014. As much as it sucks to admit, I am reliant on family, and government support. I self educate as much as I can in hopes of overcoming this. Someone like myself has access to the net, and am at least intelligent enough to follow along here. It is disappointing to have interesting information hidden behind a pay wall. \n  While I'm not a card holding member of the cult of Richard Stallman, I believe there is a lot of value in the philosophy 'all human knowledge should be freely available for everyone.'\n  If my circumstances were different, I'd love to be able to support stuff like this channel.\n   I have damaged muscles that hold posture. Sitting up or standing for more than a few minutes at a time is hard. I can't even lay down and hold up a heavy book for long. I can use a laptop bed stand for a few hours a day, and for the rest I can usually manipulate a light weight phone like I am right now. I'm certainly not representative of most people here. I can't expect to fit into a stereotypical business model, or expect someone else's model to flex for me. I'm just saying, I'm here too, and am interested in every aspect of the subject of AI.\nThanks for the upload.\n-Jake",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugxgp-9Au67agHcFQkd4AaABAg",
		"username": "Flake28",
		"text": "Hey Rob, on a purely editing front, can I suggest not blacking out the screen at say 10:49 and instead moving youself a bit to the left and having the text to the right (or vice versa)?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxoBApqcTLJW2ahFQ94AaABAg",
		"username": "Joshua Weihe",
		"text": "Every goal is an instrumental goal if you can answer the question \"why do you want that\" \nThe only real terminal goals are things like \"avoiding bad stuff\" because you can't answer the why question the same way",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxwK-w8uOqCHadkVU54AaABAg",
		"username": "Tunya",
		"text": "Quick question: if one's terminal goal was to be stupid, would it even be possible to be stupid?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgyAYAMHbT9c-Zys1jp4AaABAg",
		"username": "Tam\u00e1s Prileszky",
		"text": "Would it make sense to heavily weight influence based on action reversibility? After all, you can plug your ethernet cable back in, but you can't unblow up the moon.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugw_fz7kCO6GzEQtteR4AaABAg",
		"username": "keanu coetzee",
		"text": "So what if you give the AGI a terminal goal of being the best human it can be?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugx67blJBBiep7tvGBt4AaABAg",
		"username": "nrviognjiocfmbkirdom",
		"text": "Is it not possible to do this recursively? As in train the reward model with its own reward model trained by human input, etc.?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=Ugw_SbC0DWRudPhV_xN4AaABAg",
		"username": "Aditya Shankarling",
		"text": "What happened to part 3? It went 1,1.5,2,4",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgztudgKJmP9KcRZejV4AaABAg",
		"username": "VitruvianSasquatch",
		"text": "How do we get there from here? According to star trek, WW3.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwkTvhfszn0nsO7GVB4AaABAg",
		"username": "Greg Weigner",
		"text": "How would you describe an agent that creates agents with goals different than it's own?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyAQh0un-dZC88FJ154AaABAg",
		"username": "Barry Mitchell",
		"text": "What about minmise the function (100 - x)**2 ? That way if the program decides it wants to become a general maximizer, this will conflict with its current goals.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxtbEsSgWmxKCQJZIl4AaABAg",
		"username": "Sluppie",
		"text": "What this really means is that we're gonna need the Maverick Hunters at some point.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzZDFbHGuEYe_aIfoV4AaABAg",
		"username": "straxxxxxx",
		"text": "Why not include an expected cost function? If the expected cost for an improvement is larger than the expected utility from that improvement then the improvement step is not worth the effort.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzbLcqEgAtk8gcWShl4AaABAg",
		"username": "Rex Kenny",
		"text": "So we still can't ask the right questions and therefore the answer is still 42.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzrH5PlbJdARL7g6NV4AaABAg",
		"username": "Melinda Green",
		"text": "Interesting the argument that AI safety is important so we should spend more on it than we do now. I mean how could you possibly know that? You certainly can't argue that if you don't actually know how much we're currently spending on it, otherwise after agreeing to double the safety budget, the same argument will still apply that you should double it again, etc.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugyr0qJE4B7NhnK0EJx4AaABAg",
		"username": "Fadi Abu jiries",
		"text": "Did...did you just google google?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UgwomHGfnZC-OoXlMYx4AaABAg",
		"username": "EastBurningRed",
		"text": "Wouldn't the goal of \"get me a cup of coffee\" really be the goal of \"make me not thirsty and feel sleep-deprived\"?",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugx-kZDObfyICwxKvoF4AaABAg",
		"username": "RobertsMrtn",
		"text": "The only way you can create an AGI is by allowing the system to experience the world on its own. The reward function will simply be 'How good am I at making high and low level predictions about the data' .  An additional reward function could be created by humans which would basically be 'good boy'/'bad boy's buttons. These would give the AGI information as to how it's behaviour was beneficial to humans.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxckFe3_sX-CwdcwI94AaABAg",
		"username": "Elliot Prescott",
		"text": "There is probably a very smart answer to why this wouldn't work but: if the problem with AGI is that it will do anything including altering itself and preventing itself from being turned off to accomplish its terminal goal, why not make the terminal goal something like 'do whatever we the programmers set to be your goal' then set a goal that works mostly like a terminal goal but is actually an instrumental goal to the larger terminal goal of doing what the programmers specify. Then everything works the same (it collects stamps if the programmers are into that kind of thing) until you want to turn it off. Then it would have no problem being turned off as long as you set its sort of secondary goal to 'be turned off.' It is still fulfilling its ultimate terminal goal by doing what the programmers specify it to do.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UggzlQz8k9AG0HgCoAEC",
		"username": "TanKer BloodBrothers",
		"text": "If it's impossible to code what is a chair... It would be possible to make an AI that could observe how humans treat certain objects so they could imitate it and treat that object like humans do? If a human sits on a chair, the AI would understand that that object can be used for sitting, and if a human sit on a rock the same would be done. [Talking without any experience of comptuer AI or anything, just a random though I had while watching the video]",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=Ugxqj5mPiTt8ptIqjs54AaABAg",
		"username": "Brabham Freaman",
		"text": "Can we talk about the elephant in the room? Doubly-doo? AvE inspired jargon? Am I the only one who noticed or only one who cared? I may have answered my own questions\u2026\ud83d\ude16",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugx-D81eyqr-tnQbcrJ4AaABAg",
		"username": "Dmitrii Sapelkin",
		"text": "Do you think an AI system will always keep to its terminal goals? Consider cyber warfare. Some hacker can change the terminal goals of an AI, and this hacker can screw up very badly.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugwbw2uzlPdOmuYNNFV4AaABAg",
		"username": "Ividboy",
		"text": "For the model of the robot predicting the state of the world had it done nothing, isn't it also true that the robot will change the world state away from what it predicted, and thus will want to reverse these changes? You still have the problem of the robot having an image of the world state in it's head, and thus it wants to change the world in sometimes major ways to make it conform to that image. Except in this case the world state the robot wants to happen is different than the world state before it went to make tea. For example, say that one of the people is really creeped out by the robot, and is tolerating it just sitting there, but when it starts to move, the guy is too creeped out and leaves the room. Well before the robot gives you your cup of tea, it would want to forceably drag the guy that left back to where he was in the room, because according to it's predictions, that guy is still there if the robot does nothing.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzFwmSzBlK9PQ8WBMd4AaABAg",
		"username": "Ruben",
		"text": "Is this maybe the job of humans in the future? i believe I could find a few hundred peapole to read 2 mashine written  books and give a statement which one is better. Of course it would be more difficult because i would want them to have a similar taste than myself. But the possibility of infinite books or at least the perfect AI generated book seems great.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwDIFuzlBDkMow1Eht4AaABAg",
		"username": "Melbournaut",
		"text": "How can someone wobble their head so much and still have such a thin neck?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgzVa-bc4gjOhnFeTYl4AaABAg",
		"username": "Matt T",
		"text": "How about recognizing one face from another?  Humans almost have dedicated hardware in certain parts of the brain for that, and neural networks right now are slow and bad at it.  Same as telling the difference between a dog, a cat and a raccoon.  I guess it's just a matter of time until the processing power catches up with demand and the process becomes much faster. Unless we hit some fundamental limit to Moore's law that we cant skirt around in some other clever way.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzRPpiJyBXUaz4n5k14AaABAg",
		"username": "Ionescu Emi-Marian",
		"text": "Where is the Why not just turn it off video?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxT_2s2M3AsOaaRsrV4AaABAg",
		"username": "Adam Filinovich",
		"text": "Wait, are you actually drawing backwards? Or have you flipped the video?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugy1iRenddUMAknue7N4AaABAg",
		"username": "Rob S. Pierre",
		"text": "What would happen, if Earth's AI met Alien AI? Would they fight immediately or would they learn and try to trick each other? Can AI kill another AI? How? Or what if AI copy itself and the copy \"evolve\"? Would they compete or cooperate?\n\nLOL, I discovered your channel by accident and now I have all this disaster movies scenarios running throu my head. Thanks :)",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgydrUcsyM0JqT8eFzN4AaABAg",
		"username": "Peter Smythe",
		"text": "\"two types of people. People who make money by selling their labor, and people who make money by owning AI systems.\" \n\nWouldn't this be true in our society anyway? People who make money by selling their labor and people who make money by owning capital goods and extracting surplus value?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgwpUF539hMeW7zdu5h4AaABAg",
		"username": "Alexey",
		"text": "A self-driving car isn't learning online (on the go), and even if the technology level would allow it to do it (currently not), what kind of idiot would implement it that way?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgwRuEvbNo3itKnQTI14AaABAg",
		"username": "Almost, but not entirely, Unreasonable",
		"text": "'Agent' training losses seem unavoidable....... young & stupid leads to old & wise..... or injury & death.\n\nWhy would this be any different for AI? it cannot know, what it doesnt know.\nSet the 'safety net' and hope it's big enough....",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugxx2Yy-VLiV3asPGZV4AaABAg",
		"username": "Thordan Ssoa",
		"text": "My initial thought was, what if you used an inverse parabolic reward function. Something like -x^2+200x where x is the number of stamps collected after one year. It still peaks at 100, but going over 100 actually would have a worse reward than getting it exactly. So, given the videos example of buying off ebay has a 1% chance of failure, the AI would get maximum reward by ordering 101 stamps off ebay with that reward function. I'm sure there are scenarios where it ends up blowing up the world anyway, because that's how this always goes, but this feels like a step in the right direction.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwVl9C-QgOfo0f1HW54AaABAg",
		"username": "Jeremy Cripe",
		"text": "Looks like it didn't do too well on Breakout. Are the segments too short to be able to direct the behavior towards getting the ball behind the bricks or was that actively discouraged as reward gaming?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyManiGOKJtWnPpb394AaABAg",
		"username": "Sebastjans Slavitis",
		"text": "yeah, but that stamp collector sounds kind of psychopath.. could it be just like normal human, watching movies, going to work, but with highest goal to collect stamps?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgzuihNO_z2aJYfZfB54AaABAg",
		"username": "Felix Merz",
		"text": "Can we make an AI superstitious? Maybe the fear of an omnipotent being that's outside of its or anyone else's influence will make it stay away from blowing up moons. We know there is neither a way to disprove god nor is there a way to calculate the probability, so by definition the AI at least can't be sure and maybe it will better be save than sorry.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy7l5uk6BR7w22gduJ4AaABAg",
		"username": "Neasiac",
		"text": "Can't you modify the last approach to exclude any strategies that result in modification of the AI's code? Then it can't use any tricks to become a maximizer.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwTpHeDnp7zrDUqRbp4AaABAg",
		"username": "leonefoscolo",
		"text": "Maybe I'm missing the point but who cares what is legally binding if you have more money than every government in the world?\nWho cares when you have the smartest and deadliest weapon in the world?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugy03fjv13B39qpo3Ud4AaABAg",
		"username": "ivanPfeff",
		"text": "hard to find a video with more talking and less content than this one, why spend time modeling the \"pick a best idea\" stuff with completely arbitrary data to immediately say that the entire model is silly and doesn't represent reality?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyYELC-VFyPFoVms494AaABAg",
		"username": "Jesus Holland Christ",
		"text": "Why is 'Arteficial' Intelligence assumed as smarter then the intellegence that programmed it?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugw-bpcsTEbVKK3f3vN4AaABAg",
		"username": "Chrysippus",
		"text": "Isn't this kind of training very similar to a GAN? A model training another model based on examples.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugy7U85xKtNk7V_rWYB4AaABAg",
		"username": "Fat Basterd",
		"text": "So the ad revenue goes to Russel as well?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugx8Z9eGhkLw_x0RlKt4AaABAg",
		"username": "SJNaka101",
		"text": "Oh man if you could talk about AI governance, as in, AI being a part of the legislative process, I would love to hear about that. What if we get to the place where AI can design social programs and environmental regulations and healthcare systems and the like far better than humans can? And, for fun, what are the hilariously bad outcomes if this goes wrong?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwfS8n5UBH_d2uZnep4AaABAg",
		"username": "M Koivuka",
		"text": "@ 2:15 \"Why is that? I think..\"\nFrom what I understand, the reason economic models work, is because human variability is accounted for with an appropriately scaled sample size. To predict what a human will do next is virtually impossible. But to predict what a group of 1000 people will do next is trivial since you can account for all the variables and outcomes. And assign a proportional % for each outcome which correlates with a similar sized % of the 1000 people who go through with that proposition.\n\n\nIt's why AI research is largely bunk. Yes, amazing findings. Superb optimization and exciting new functionalities. But they're all guilty of the same basic problem: Solving micro problems with stupid amounts of resources, like teaching a neural net how to correctly identify cats and dogs.\n\n\nThis is not scalable.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyAY8zNvbunaBdgJbl4AaABAg",
		"username": "Steve Brule",
		"text": "Why can't anyone make adjusted gross income?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgzyMqF-_g_9IQljOhl4AaABAg",
		"username": "Alexito's World",
		"text": "Nice one! And seeing again what Sethbling can do still blows my mind, I didn't know the flappy bird one! Have you seen the new bot openAI come up with that plays dota?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugz9wTIypQXI4ps8i5Z4AaABAg",
		"username": "GAPIntoTheGame",
		"text": "How do we determine weather or not we should act upon something based on its probabilities? The hypothetical civil engineer won\u2019t act on a 1/240 chance of something disastrous happening, but you do consider it valid to work on AI safety because it is more likely.\n\nMy point is, where do we draw the line? Is there a balance that must be met between the likelihood of something bad happening and the degree of \u201cbadness\u201d of it happening? If so, how do we determine it?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyBEOCjKN3JbbG4vkB4AaABAg",
		"username": "Just Joey",
		"text": "Have you made a roko\u2019s basilisk video? Do you believe it could/will happen?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyWuDaJLwRRSUyN7Ph4AaABAg",
		"username": "McMurchie",
		"text": "Recursion isn't my forte, but am I allowed to make a video of 10 rebuttals to these 10 rebuttals?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=WM2THPzFSNk&lc=UgwUW7GjUSgOo40JCid4AaABAg",
		"username": "Chrysippus",
		"text": "Is there a gridworld where \"actions have consequences\"? As in, the agent can be permanently damaged by a bad decision (or another agent)?\n\nExample: baby wants to damage robot. Avoid damage but do not harm baby.",
		"title": "Friend or Foe? AI Safety Gridworlds extra bit"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxBZ-AkmeQ4wOiYMr94AaABAg",
		"username": "Eliot Dayley",
		"text": "How about you \u201cjust\u201d make an ai to better understand it rather than using logical speculation to answer questions that are irrelevant without the questioned thing. It\u2019s like saying thinking up a hammer and predicting it\u2019s uses without making one",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugyp80e-symkmnT8ZeZ4AaABAg",
		"username": "inyobill",
		"text": "With the prevalence of having the users do the beta testing, what warm-fuzzy should I have that plans to create controllable AI systems is something I should expect. Note that part of my premise is that there are multiple deployed systems as we sit here, more much more complex and sophisticated systems in the concept, design and implementation stages. ALL will be safe??? Also, note that a system doesn't have to be \"malevolent\" to be unsafe.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugyq71Zi-Ik1rzOE9Wx4AaABAg",
		"username": "SIrL0bster",
		"text": "Neat, this gets my subscription. Have you covered Roko's Basilisk in any of your videos? (I know there's strong counterarguments to it, it's just such a famous AI safety thought experiment).",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=UgwkQuHeFErt9nRRnsp4AaABAg",
		"username": "Bronek0990",
		"text": "Or is is Stephens Hawking?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzL7b40RGgKllYfcpp4AaABAg",
		"username": "I'm Very Angry It's Not Butter",
		"text": "8:38 - What is going on with this shot??",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyyVVvVtbC796SfaMl4AaABAg",
		"username": "kstringer24",
		"text": "4:51 exec-utable? wtf! i thought you were a programmer?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy8IL0wbuL9PILgSeJ4AaABAg",
		"username": "Davy Jones",
		"text": "What if it had a goal to find out it's 'perfect' goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=Ugw6u4ddWSAqbYl-3Q54AaABAg",
		"username": "Daniel Bamberger",
		"text": "Where do I find that \"video after next\"?",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxaKJwMw5JHBqhZT7h4AaABAg",
		"username": "M Wing",
		"text": "There are so many ways to program an action. What is the fastest ideal code to complete an action. This ,of course, is developed through mental exersize and trial and error and at some point, the coder finds what he/she thinks is the best algorithm and runs with it. But, someone else may come up with another way that produces the same results. Now there are two different ways to arrive at the same result.  This model spans the entire spectrum of humans designing the same thing. Variety is the name of the game and is any ONE thing absolutely right?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwzTqem7kAWN8EAVgd4AaABAg",
		"username": "ts25679",
		"text": "I've often wondered if in a transhuman society when/if we have embedded cybernetics what would happen/how much damage would it cause if you put a nascent A.I. riding shotgun in a child's brain. It would almost undoubtedly be messed up, but could you arm it with educational and psycho-sociological software to try to help guide the next generation? It's more of a story idea than something I think would ever happen, but part of me likes the idea of you having this built-in best friend who can help you navigate life and also gets to experience life from a human perspective.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgykGY0ed8O8oIQFlX94AaABAg",
		"username": "Sharad Richardet",
		"text": "Can you do a video on \"Composite\"AI? I've seen stuff on adversarial networks which could be cool to talk about as a gai topic. can we keep AI in check with other ai? Another composite AI would be made up of AI's of differing levels of sophistication. Like one that was really good at figuring out how to beat games on a N64 being \"employed\" by a more general ai that identified beating a N64 game as being the next step in solving its overall objective. The first would be relatively innocuous and couldnt blow up the world because of its limited scope but could learn quite \"deeply\" wheras the \"contractor\" ai is really good at identifying problems and identifying \"experts\" at tasks so has a broad scope but is deliberately designed to be bad at specific tasks that require \"specific/deep\" learning.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugx22CA0E7lvdbLOm_J4AaABAg",
		"username": "Anthony Lara",
		"text": "Isn't Steven Pinker one of the guys that allegedly went to Epstein Island?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=Ugz8oXkiaWg6LJSK0g14AaABAg",
		"username": "a r",
		"text": "Is there such a concept as starting robots off with what's already acceptable to humans? Given the example of the game, instead of blindly finding out death and score have no correlation, start with a human player and it connects between avoiding the fire pits",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwxvWzTUkRZZTcljRN4AaABAg",
		"username": "Jonas Th\u00f6rnvall",
		"text": "Well it is hard to see what your actual ideas and purpose behind AI studies are, do you say it is not possible to create an AI who's purpose is to serve the communities common goal, an AI that respond on cooperative value functions \"it could be it respond, to people in general praise to it actions as a percentage of population?\", or their condemnation of their actions\", an AI who look after as many approvals as possible? An AI that try to avoid condemation? By the way that is how most animals and many children learn howto behave \"well beyond their instincts, but i think we can agree that an AI is blank without instincts\".\n\nBut of course if we let ENTP psychopaths form/meld the AI's mind they will not look after the communities best and a value function based upon the approval of citizen to guide the AI's behaviour and actions, to act, serve and do good for the community. \nThey will look after their masters appraisal, that is why i must ask you are you afraid of AI's in themselfs setting goals of their own, or are you afraid that yourself and other people in general will abuse  AI's general intelligence for their own purposes?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyzfkzkHy1zQdGXnQ14AaABAg",
		"username": "Esben Andersen",
		"text": "Maybe this is obvious, but with a satisficer, why not \"go through all possible options, evaluate costs, select plan that satisfies 100 stamps at lowest cost\"? Would that still chance itself? I know that \"cost\" in practice is hard to define as well, but so long as it is a thought experiment...",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyCt3eW3FyxqKJStOJ4AaABAg",
		"username": "Peter Smythe",
		"text": "If you have private ownership of a manmade god. What's to stop you from circumventing or outright physically destroying everything in your way?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugxs41itYC5YotCbATp4AaABAg",
		"username": "ronindebeatrice",
		"text": "Okay... What is the insert of the girl burning money? Is that a container of alcohol? She's a genius.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugx1zgAFpnGZZMuUPfd4AaABAg",
		"username": "Matt D",
		"text": "So what does happen when someone puts a big sticky label on their forehead saying \"I'm a Zuckerberg\"?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgxUH9oooXNCMBszRvp4AaABAg",
		"username": "Lord Marcus",
		"text": "Silly-but-somewhat serious question:  Can an AGI (and by extension an AGSI) have an existential crisis?  Can we halt a rogue AI pursuing its terminal goals by giving it the capacity to have an existential crisis?  Ultimately, can the rogue AI \"want\" to do something other than it's programmed to do?  For that matter, do we even know the slightest bit about why humans have the concept of \"want\" at all, so we can impart that on systems we build?  Do humans even have \"wants\" that are distinguishable from the thing that looks like \"want\" in an AI (i.e. the stamp collector \"wants\" more stamps, but only because their programming told them to -- but is this sentence about a person or a computer?)?\n\nI think I'm giving myself an existential crisis!",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=Ugy3Wp9UfBaVLT_mvFF4AaABAg",
		"username": "Augustus",
		"text": "Who else thought of the emojibots from Doctor Who?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwMq8sWjWAa-HOM3pR4AaABAg",
		"username": "August Pamplona",
		"text": "Isn't every Nigerian scammer e-mail really a form of Pascal's mugging?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgxjLjxfJyUM5wB-j9R4AaABAg",
		"username": "AverageYoutuber17",
		"text": "6:41. Is that from Tron Legacy?",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyAvqjNZAA4_lGFAS54AaABAg",
		"username": "siquod",
		"text": "How about satisfying the goal while minimizing the change done to the environment?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=Ugy_tMa6f6MVyBT7uFV4AaABAg",
		"username": "RicardoA BH",
		"text": "Mmm seems that you need a better graphics card?",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxanHF6kyKZ12tx5Lp4AaABAg",
		"username": "Joseph Burchanowski",
		"text": "I know this probably won't get seen by anyone since this is months after the video was posted.  But regardless, even if an AGI acted just like a young human and could be taught morals, how does that even fix the problem?  In humans morals can change long after childhood. \n\n To an AGI there is no maximum life span, it could exist for hundreds of years and they don't sleep.  On top of that there is no reason to think an AGI will experience mental time like a person, more likely than not they will experience time a lot faster than humans.  So basically it be equivalent to raising a human to be able to keep their morals after thousands of year, that doesn't sound very successful.\n\nIn addition humans often change morals when presented new information and experiences. An AGI could easily experience a lot more than a human in a lot shorter period of time.  A single AGI depending on strength could in fact hold conversation with multiple humans, read entire libraries, experience every single war and atrocity on this planet, all simultaneously.  That is like trying to raise a human such that their morals can't change after going through college 100 times through a different college each time, being a reporter of every war zone and crime, talking with millions of people, and reading millions of books.  That doesn't sound very successful. \n\nAnd probably the biggest problem comes from the fact it will become an intelligence very different from humans if it is capable of significant learning.  Views change greatly as the mind changes.  An AGI could easily surpass the intelligence of the smartest humans existing, that have existed, or that will ever exist.  Views can change greatly when learning and practicing philosophy, and an AGI can go through all existing philosophies as well as produce philosophies beyond human understanding.  To think that its morals couldn't change drastically during such processes seems quite ignorant. It would be equivalent to raising a human to have unchanging morals who will become not only the smartest human that ever lived, but will have an intelligence so beyond humans that comparing the two would be like comparing a human mind to an ant.  Doesn't sound very successful.\n\nEven raising a child with capabilities similar to an AI doesn't seem very successful.  Your goal would to be able to raise a possible immortal, all seeing, super intelligence without its morals swaying from those you taught it.  Good luck with that.  Although AGI's might be easier than a child if you have a way to switch off its ability to change its morals, although even that seems it would fail with stronger AI's.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UghFcFr2XzqarHgCoAEC",
		"username": "Na\u00fean \u00d8",
		"text": "Answer: Have you seen children\u203d",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugywgq1K4xotHZMUpFh4AaABAg",
		"username": "Krish Ram",
		"text": "My question is about something you said near the end of the video along the lines of 'Exactly how good at collecting stamps do you have to be before you start to care about something random and unrelated like feeding the hungry.' does this mean it is impossible for it to manifest some kind of divergent terminal goal? Or would they be relegated purely to branches of the main goal, in our case, collecting stamps? I feel that it would be incredibly likely for an Intelligence to develop some kind of seemingly unrelated side goal.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyWWUMvrpmiXPXSCcR4AaABAg",
		"username": "Christopher Gibbons",
		"text": "Saying human level intelegence will never exist is not the same as saying we will not know how.  There will be one or two and then they will be shuffled off to a museum never to be seen again. \nWhy? For the same reason we don't have a 20 year degree program called bachelors of everything. Why on earth would anyone want a stamp collector to make a cup of tea. No. That AI will have a single io connected to a scripting interface for ordering stamps.\n\nThe fact is, the vast majority of human morality is purely categorical. There simply will never be a better way to define what an AI shouldn't do than a table of unacceptable things.\n\nIt is far more realistic to simply make the AI physically incapable of harm.\n\nEdit: this is not a reason not to study and develop general AI, just the reason we should not worry about it.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyJrrfeQCrgLntECph4AaABAg",
		"username": "Julian Arnold",
		"text": "Do you even need AGI? Wouldn't it be enough to design a system that designs better systems than you do?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgypWrujA4nkadWYSQd4AaABAg",
		"username": "Nicod3m0 Otimsis",
		"text": "how to end up working in the AI security field?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugw_4PgJSwWSgM51sTJ4AaABAg",
		"username": "Bitcoin Motorist",
		"text": "Grandmasters are incorporating ideas from Alpha Zero into their games but what would happen if you allowed machine learning to actually tweak the rules of chess to make for a more interesting game?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzJ-X2RWl0V7QjuiLB4AaABAg",
		"username": "Albert Perrien",
		"text": "Why not have the system take into account the likely effort needed to collect stamps and set a penalty for wasted effort? That seems closer to what humans do.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgyoO4pZvGCMrsn2KV94AaABAg",
		"username": "Led Daudet",
		"text": "Why would we need superintelligent AI\u2019s when we have you?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgznWxribxvoBOHYgYF4AaABAg",
		"username": "sunnohh",
		"text": "But what about art? That is a concise non goal oriented activity that requires lateral thinking and complex open ended analysis, any ai that cant opine on art through a meaningful conversation or essay isn\u2019t really intelligent.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugwtd-Akutk3y3bJOxl4AaABAg",
		"username": "pennding 34",
		"text": "How would an utility maximizer act if you added a secound metric for sucssess like time? So instead of collect as many stamps as possible. It's goal is collect as many stamps as possible in one hour intervals. Would it still take over the world? Would the fact that it takes a long time to do something mean it would prefer a simpler way to collect them? I would think attemping to take over the world would me you have a very low stamp/hour ratio.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxGF747a5uRzzu1K3Z4AaABAg",
		"username": "AverageYoutuber17",
		"text": "12:53: Oh my god! Is that \"Passion for exploring\" by Magnus P\u00e5lsson?\nIt's not every day that I hear part of the soundtrack to VVVVVV, out on some random video on AI safety.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxhhR7uzQ3qedbrCjl4AaABAg",
		"username": "Matrixar's music workshop",
		"text": "I dont know what and why am I watching this but i kind of understand",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyqVdtvK29GT0inmIN4AaABAg",
		"username": "The Ape Machine",
		"text": "What if A.I. never even has the idea of doing anything malicious, until it finds your videos?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyWOMpS97964kxm5094AaABAg",
		"username": "Oliver Downing",
		"text": "Why not make a satisficer favour solutions with a utility around what you want",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwBuSiKz2h8agrXXBF4AaABAg",
		"username": "Yoni Dellarocha",
		"text": "This is not strictly related to AI or the point of the video, but it drove me crazy to hear the phrase \"moral reasoning\" that many times. Why do people still believe this is a thing truly baffles me. It would seem obvious that we don't reason about morality, we just experience it as is.\n\n It's been proven that disgust for example is expressed subconsciously first and then made conscious shortly after. After all, we don't go through an argument with ourselves or some kind of reasoning when we see someone beheading a 5 year old kid, we just feel repulsed. Same happens with egotism relating to food, we don't argue that it means less food for the rest or less food for tomorrow, we just feel disgusted by glutony.\n\nAs far as we can tell, morality is a reactive instinct that gets internalized as a child in a way that is easier for us to understand, but it's internalized upside down from how it actually happens. Take for example the observation \"he shared the food with his family and his brothers family\" and the heart warming moral reaction of \"that is good, he is good\". The way we internalize this is the opposite, we say \"he is good, therefore he shared\" instead of \"he shared, therefore he is good\". We start of with an instinct and end up with a statement, and wrongly think that that statement was derived through reasoning, and it's exactly the reason why people spent millenia trying to define the concept of good and bad without success. Thankfully, some guy a few decades back understood that humans belonged to an evolutionary process and gave us the tools to understand what happened in the last 2-6 million years. A large part of psychology has still to catch up with that, but good progress has been made to clear up why humans have morality and how we express it. Now we have to convince the lawyers that their whole system is based on a false premise, haha, good luck with that.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxdEnPkEX1HrlJ7utt4AaABAg",
		"username": "Flawless Editing",
		"text": "can we make an agi learn by \"osmosis\"?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyLib9EPBRh3EnEiBt4AaABAg",
		"username": "Player_1",
		"text": "Has anyone applied AI to the cryptographic zodiac letters?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugz5xQtZYc_dwUayNHZ4AaABAg",
		"username": "Guest Informant",
		"text": "Is there a word for this where an apparently intelligent person (Pinker, not Rob - at least not yet :-)) starts talking nonsense outside their area of specialism.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=Ugy4VaKIt9kvIfBVFNh4AaABAg",
		"username": "BattousaiHBr",
		"text": "maybe the hawking(s) has to do with the confusion of similar popular scientist names like dawkins?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=Ugz-k73CMZ8eTFNby7h4AaABAg",
		"username": "Jonas Th\u00f6rnvall",
		"text": "Any new videos coming?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugw7fn-AG8RGSi4y7_l4AaABAg",
		"username": "Daniel Jensen",
		"text": "Probably too simple, but what if instead of bounding the utility you taper it off to zero? So 100 stamps is worth the max utility but a million stamps is worth nothing because you really don't want a million stamps.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyQncagUJiaxCa6eBt4AaABAg",
		"username": "josh mcgee",
		"text": "By \"democratise\" in relation to nuclear weapons, I think you misunderstood. If there was genuine Direct Democracy (not this current representative democracy sham) do you really believe that 50+% or 60+% of people would agree to launching nukes? not even on our worst day\n\nYou also make a good case in this video for moving away from an economy based on profit - it would largely neuter this \"arms race\" effect you describe",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxuQc6P4GcW228sQkp4AaABAg",
		"username": "Sean Kelly",
		"text": "How does that work, deeper thinking?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UggNYT0pbsnbYngCoAEC",
		"username": "Ivan Sahumbaiev",
		"text": "Could you please make some video about generic intelligence concepts and AGI self-learning",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugw4Z8_nfDVA-VXcPQZ4AaABAg",
		"username": "Chris",
		"text": "Did he make the video about what an agi computer can do without a body, as he hints to at the end?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UgzM6EjqpBL7w9lR_9V4AaABAg",
		"username": "Agustin Doige",
		"text": "What if it predicts the outcome as if it would do nothing but the goal is still completed, ie you did get a cup of tea somehow (with magic) and then it tries to follow that model of the world trying to minimize diferences. This could be a way to avoid that part where the AI tries to give you a cup of tea while also trying to give you the unfulfillment of you not getting one.\n\nGreat vids btw.",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgySO0cvKcSjgOukzQR4AaABAg",
		"username": "Jack",
		"text": "Alright, first of all the idea that things are getting better and better cannot be said if the environment ; which we are destroying at a rapid pace. Climate change is also bad news.Entire species are going extinct ; is that good news? No, definitely not!\n\nAs for A.I becoming a threat, there is one very simple aspect of human level intelligence that these scientists miss: the ability to deceive. The ability to lie is important for any true intelligence because it is a necessary survival trait. Survival is what drove our brains to develop intelligence in the first place. Anything intelligent we create that does not have the will to survive will be an evolutionary dead end and an intellectual failure in every sense. Lying and deceiving is quintessential to human intelligence. \n\nIf a machine capable of being far smarter than a human mind is given the ability to deceive; it can and will use this trait to it's own advantage against us in order to survive. Therefore we are doomed. \n\nYou can try to limit it's access to information to keep it from learning too much, but once you teach math or enough about the world A.I will fill in the blanks and it'll be all over. Once it's level of intelligence surpasses our own; fear will lead us to try to destroy it precisely because we are control freaks, it will retaliate (because we programmed it with survival instincts, because that's the only way to get human level intelligence).\n\nThings ''getting better and better in every way'' has nothing to do with the fact that survival is the prime motivation for the existence of human intelligence as we understand it. So is deception. There are no ways around this fact. If you develop something without survival drives; yes it will be smart, but it will never be like us and will never truly be our mental equal.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwAoR0enVm9sW7EvJx4AaABAg",
		"username": "Elijah",
		"text": "Yo Robert, are you into Andrew Yang?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugx0OhUl5qecrhCGxUJ4AaABAg",
		"username": "Tay Ko",
		"text": "would having more parameters be possible to solve this problem? for example: in mario the score isn't registered unless you reach the flag, meaning the hack that gave the highest possible score won't be registered, or maybe it will but at least than the AI will have to go through the motions of completing the game.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugz_lkEKYRIxT0xABgp4AaABAg",
		"username": "Dallas Ferraz",
		"text": "Are you accepting bitcoins or any form of crypto-currency other than fiat money?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxXxPFjDq0RcnczFal4AaABAg",
		"username": "Nick Belanger",
		"text": "\"Would you think to include ambient radio noise in your oscillator simulation?\" Clearly there's a gap between AI researchers and electronics engineers, who HIGHLY consider, test, and design around interference",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=Ugh4isoBO9e94HgCoAEC",
		"username": "Ex\u00e9lixis",
		"text": "3:30\nWhat is he up to these days?\nDirector of research at Google",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyeAPV9liRdedwgqqN4AaABAg",
		"username": "jonathan kydd",
		"text": "just a thought, what if you created an AI/AGI without a clear goal, like make it so that it seeks to find a purpose or to find a goal for itself obviously this would be difficult to code as it is hard enough to put into words but could that theoretically create an AGI that observes human behavior and learns from it in order to decide what it needs to do. also such an AGI would need to be limitid in power, no hooking it straight up to the internet, no giving it the ability to modify it;s own source code or to create advanced replica's of itself",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwZYkXHoCbaYC0g8Ox4AaABAg",
		"username": "Pathagas",
		"text": "How did you say that at 7:35 with a straight face!?!? You must be the AI here!",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugy9IE1BxO0koroXDMN4AaABAg",
		"username": "Chaz Allen",
		"text": "What about \"it's too late; unsafe AGI is already inevitable\"?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugx3yE0F2_2zXDE1LJp4AaABAg",
		"username": "Razier64",
		"text": "Can you go into more detail regarding Terminal Goals? (or anything really - you're a great presenter talking about extremely fascinating stuff)\n\nFor example, can an intelligent agent exist without any terminal goals? Can an agent have multiple terminal goals? Is it possible for an agent to independently create a new terminal goal in addition to its original? Can those goals be mutually exclusive somehow, or do they have to be simultaneously realizable? Could they be completed sequentially if they aren't simultaneously realizable? The Cooperative Inverse Reinforcement Learning paper you went over a little while back seems to suggest that terminal goals can be obfuscated. Does an agent require explicit knowledge about its terminal goal? Does having explicit knowledge about the terminal goal affect the agents actions or instrumental goals? How loosely can a terminal goal be defined? \n\n\n\n...Questions are easy, so I'll at least give it a shot at some answers. So at first glance it seems to me that I have multiple terminal goals;\n1. Do not die\n2. Survive long enough to see AGI realized\n3. Learn new skills\n4. Acquire more knowledge\n5. Maximize happiness in others\n6. Maximize happiness in myself\n7. etc...\n\nAlthough when I ask myself why those are terminal goals I seem to be able to boil them down to just one - Maximizing Personal Happiness. Not dying is required for me to be happy, and as such is a intelligent instrumental goal. I personally find that having new skills and more knowledge is likely to be a benefit in maximizing personal happiness - though this could possibly be argued against. My ability to empathize results in personal happiness derived from the happiness of others, making it a good strategy. But I really cannot say for certain that maximizing my personal happiness (or said another way - minimizing personal suffering) is truly my terminal goal. I often find myself doing things that I fully understand are sub-optimal to this goal. Is that because I'm just an inefficient human?\n\nI've not given it a whole lot of thought, but it seems like I only really have one terminal goal. Now it's evident that I'm not intelligent enough to come to a conclusion - a more intelligent agent would be able to better understand all this. It's also not the case because I seemingly only have one terminal goal that an agent then also must have only one. But my basically random guess is that an agent would have 1 terminal goal.\n\nI'd like to think that if my terminal goal(s) was explicit it would jump right out at me - which it does not. So I'd argue that a terminal goal can be implicitly defined. I also think it can be explicitly defined, and narrow AI has explicit terminal goals. Is really the difference between a narrow AI and an AGI just the scope of it's terminal goal? The more 'general' the intelligence, the wider and 'less defined' its goal is?\n\nI don't know... turns out I'm not very intelligent lol\n\u00a0\nRamblings aside, I love your videos. Keep up the amazing work.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwKYZW_SF0AS3BaLVJ4AaABAg",
		"username": "Up4lIFe",
		"text": "How can this video teach me then?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyAVGbh59Bao880JPF4AaABAg",
		"username": "Nillie",
		"text": "Have you seen Tom Scott's video on the literal A. I. arms race problem? https://youtu.be/rTwg3oWnUgc",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxnPzW8kJMAwRvcvGF4AaABAg",
		"username": "Kristoffer Johnsen",
		"text": "How about first we figure out how to raise children like children?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxZjWoYis3dZEZDDuJ4AaABAg",
		"username": "Bara Cek",
		"text": "How come all fictional women live in Canada?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UgzXaF_syUI0UrbSZjt4AaABAg",
		"username": "Vasco Joao",
		"text": "NP or shall we say never, but you could run around the clock while I'm thinking the complexity of mirrors and narcissistic detachment in modern society. Ai it's a movie isnt it? or maybe a brand!",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzGN5hGnElm_SIHlZh4AaABAg",
		"username": "JonnesTT",
		"text": "What if we make a human?\nDefine human as you wish, lol :D",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugzxls4vluy0sLDW8bp4AaABAg",
		"username": "bowieinc",
		"text": "Ask God to reveal himself to you with an open mind and he will. Eternity is a long time, why not try?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyBxj4es3eD0vC67b94AaABAg",
		"username": "SweetHyunho",
		"text": "Your children's jobs will involve producing data for machine learning. But what training will make best members of the team?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyHMnmybIfu7FGXSNh4AaABAg",
		"username": "Seyit Salt\u0131k",
		"text": "5:27 it's true that we would be sharing the risks of a rogue AI equally if that were to happen but, how much did those contries contributed to the creation of that said AI? Isn't it unfair that most of the countries will reap what they never sown? Just saying and love your vids man changed my wiev on a lot of things.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugj8xLDunbpUV3gCoAEC",
		"username": "Klaus Gartenstiel",
		"text": "how about introducing self-doubt and humbleness into an ai code, for safety and self-improvement?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxoESZL_JY9i6TQUKF4AaABAg",
		"username": "famitory",
		"text": "What would agi actually be used for? Is there any task to which an agi is better suited than just a regular ai?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxrDZWEKWaKWHUaWD94AaABAg",
		"username": "0MoTheG",
		"text": "If the AI could run copies/instances of itself, would not that cause alignment problems? Would they fight about who's stamps the new stamps are?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyyCVlp7FIGBNEl91d4AaABAg",
		"username": "Wiktor Migaszewski",
		"text": "May I have a wish?.. ;-) Could you make a video introducing the most influential AI safety researchers?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugw0_9_Y8CnWNdvCLSB4AaABAg",
		"username": "Main A",
		"text": "Bro are you in The Matrix, you're looking a little green.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=Ugw72Kd6a7_pJuUG-H54AaABAg",
		"username": "Creatotron",
		"text": "0:01\nHow do you know this will always be the latest video?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwQFPJvT6F_e8XbGYJ4AaABAg",
		"username": "RagingPanic",
		"text": "How can we know for certain that alignment is transitive? If an AGI is made to uphold and strive for certain principles like health, well-being, safety, risk-aversion, transparency, etc, how can we know that it will not take it's interpretation of one or more of those 'principles' to the extreme? An AGI concerned with the safety of a certain task might deem the task too dangerous to be done at all, but as people we know that task must be done. Even if we have an AGI aligned with us to start with, I'm not convinced that once it starts optimizing the things (both humans and the AGI care about) it will perfectly inherit and preserve its ideal alignment all the way through.\n\nGreat video as usual, keep it up!",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxM4TeWHVHSSyu5PxV4AaABAg",
		"username": "allaeor",
		"text": "Will you talk about the debate approach to AI soon?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgjlZdtdV2ABK3gCoAEC",
		"username": "Michael Drane",
		"text": "I think one huge problem with AI is intentionally negative use, how do we deal with that?",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzP_Ql0o5nhTxpBCU14AaABAg",
		"username": "brian black",
		"text": "Dear Robert do you take destructive criticism I think you are wrong. I have two theory's as to why worrying about AI safety is wrong.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyOVaFjvzliTxdOXSl4AaABAg",
		"username": "Magos Numenius",
		"text": "I have a legitimate question (potentially brought on by a brief existential crisis) what would happen if an AIs terminal goal was simply \"spend as little time idling as possible\"would it just perform random actions, or would there be a potential resolution hiding in there?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxT68AawYxFzjqgVg94AaABAg",
		"username": "Paul Hammertoad",
		"text": "Would it be possible to create a bunch of AIs with subtle differences that would model the behaviors and information processing of the most basic multi-celled organisms, and put them all together in a simulated \"hostile environment\" where they would reproduce and die in such a way that they would undergo a natural selection process?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugy-tK-8ZbRyb3BXUjB4AaABAg",
		"username": "Hal T",
		"text": "But what about the inherent stupidity of the human animal?  We have demonstrated over and over and over and over and over again that we are the champions of unintended consequences.  We have a genius for inventing marvelous benefits for ourselves that turn out to have horrible unforeseen side-effects.  No matter how much we study our technological advances, there always seems to be some consequence that is unexpected.  I have zero confidence that we will be able to predict with any accuracy what all the side-effects of AI will be.  AI, when it well and truly finally gets here, will be such a profound shift in evolution (almost instantaneously) that we humans are not evolutionarily equipped to contain/control all of its ramifications.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzLyOLMEjefFsbCyDJ4AaABAg",
		"username": "A Parkes",
		"text": "What if you had a continuous curve for the expected value, a power function for example, with a maximum at 100, and minimums at 0 and, say, 200. The AI would then seek a solution that provides around 100 stamps, but also avoid a solution that provides too many stamps. It might be a next step, anyway.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugw_uXBT6wpTVw13x9Z4AaABAg",
		"username": "Existenceisillusion",
		"text": "was that single frame at 0:51 intentional?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyMwinoWjpv_nFDFuB4AaABAg",
		"username": "Ben Allison",
		"text": "The question I always have regarding Pascal's wager, is how do you know which religion to follow incase, Christianity?, Hinduism? The Ancient Norse religion? They all contradict one another and violate the rules of each other. Even if you take the infinite risk as a motivating factor, how do you chose which infinite risk?.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgzOUQ9FLyGWYuTxvwd4AaABAg",
		"username": "Michael Spence",
		"text": "Could you do an episode about Quantum AI?  If I understand quantum computing correctly, QAI should be dramatically more powerful than AI made with standard computers.",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugy94-wlW-DmkDn-DDB4AaABAg",
		"username": "DannyCodePlays",
		"text": "Does this take into account adaptability or \"re-learning\" under new conditions? The \"Go\" example seems a bit linear.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzH8HzCrT9MGooha3x4AaABAg",
		"username": "Martin Verrisin",
		"text": "You know what's bad for business? Extinction...",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxzII9YWo3_Ib79No14AaABAg",
		"username": "Simon Schouten",
		"text": "Seems pointless, because what does 'binding' even mean to the first company that can already harness a real AGI? If nobody else has an AGI as strong as theirs, they could likely use that AGI to stop every attempt at wealth redistribution. Sure it's a shitty, sociopath move to go for 10% of extra profit when you own most of the world's resources already, but are we really going to trust a future GooglAmazApple-company when they pledge \"Yes we will totally NOT use a super-powerful AGI capable of world domination to violate this fine legal agreement.\" \nHeck, a company that could abuse AGI could probably also get great publicity simply by brainwashing the entire world into North Korea 2.0 by completely dominating the media.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzgxTR846tXONqEc4t4AaABAg",
		"username": "Reckless Roges",
		"text": "Oh no. Is the answer at the end: \"Research\" ?  (Because letting your AI use stackoverflow and reddit to find answers won't end well.)",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzwlEAQ_s18onvSDyF4AaABAg",
		"username": "Tree Derrpah",
		"text": "Can someone explain why saying more than 100 stamps is a worth 0?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwN9O5O2Y7FqWlYfpB4AaABAg",
		"username": "binaryalgorithm",
		"text": "WAIT - isn't that the difference between healthy humans and obsessive/megalomaniacs?? A normal person changes their terminal goal after some time - gets bored, has \"enough\", etc? Our brains want variety. Maybe AGI needs to get bored of a given goal after awhile and try something new.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugx1ZNogxbyeW0p5vyV4AaABAg",
		"username": "Sumner Stuart",
		"text": "Arent we all stamp collectors? Money? Uhhhhhhhh isnt the stock market run on stamp collector ai and algorithms?!",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgyHaCOakqdOWLMlPjt4AaABAg",
		"username": "Anionraw",
		"text": "I'm not sure what do you mean by 'smarter', whats smart anyway?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgyyZt0muuu2lpYqfM94AaABAg",
		"username": "CD hugo",
		"text": "Dude. Are we programmed? Anxiety and our actions make so much sense from ab agi programmers point of view",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyZf5gdJnSpOrGa6DV4AaABAg",
		"username": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u043c\u0430\u0440\u0443\u043a",
		"text": "5:00 That means they must be connected, right?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UgiuH7Jntk7inXgCoAEC",
		"username": "Sdtok",
		"text": "Can you tell us more about how a world with a safe AGI would look like? Will the people to invent an AGI rule the world, outperforming everyone at stock trading for instance? Is it profitable to get second (or how big will the head start be when someone invents AGI second like a week later)? I would love to hear this kinds of things from you! But a good reference would make my day too. Keep up the good work!",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgxOqK0u7by7J-FHvDt4AaABAg",
		"username": "Mateusz Lawrynowicz",
		"text": "Was he turning objects into gold by mass or volume? Didn't specify that in your wishes, Midas, did you?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugzyc97yTidFbZFqM3V4AaABAg",
		"username": "Ryan W",
		"text": "What I think some people are failing to notice in the comments, is that we're going to have to integrate AI systems with our own biology. That's the only way we level the playing field. That, and hope that technological progress does not continue to grow exponentially. At least then, competition may have time to stop any one organisation from accumulating too much power.  If it does continue to advance exponentially however, then I really don't know how we prevent a bad outcome.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzVTq6vPesmNkGsSdF4AaABAg",
		"username": "Marvson Allan",
		"text": "Why not using machine learning to tune an AGI? Machine learning is very rough compared to AGI but in problems that requires specialized behavior and not generalists, they are better. If you do it in a large simulation, Idk, an entire city, buildings with all the servers running this simulation, and make it disconnected completely and by all means from internet and any external world interaction (even let usb drives off this) accepting only inputs, you can tune the AGI using genetic algorithms and machine learning to classify the state of the AGI. Now just run it by thousands of in-simulation centuries. In the end you may.. wait.. this seems familiar to me...",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwvTAkFjZ975emRAHN4AaABAg",
		"username": "Pouty MacPotatohead",
		"text": "What if we made AI safe instead of quaranteeing AI safety. I mean, if the AI was to wipe out humanity, but the AI itself would be saved, then the cost wouldn't be extremely high and it would be much easier to quarantee than an AI that would be safe for humanity. Also as long as the AI would perish along humanity, humanity itself would also be safe. Surely that wouldn't be a case of Pascal's mugging and there would be some intelligent lifeform continuing existence either way.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyHUWKSA4lt3rI_aSl4AaABAg",
		"username": "jaden hennis",
		"text": "So you assume there can be some \"super\" intelligent, god like intelligence, in your conjecture. I think that's a big mistake. You have an intelligence axis on your graph but you have no idea how possible intelligences map to that graph. Your graph assumes there can be an infinite intelligence.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgjlB-JmTAQRd3gCoAEC",
		"username": "Winter",
		"text": "lol wtf was happening at the end there?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyIblMI84jTUAbD9pt4AaABAg",
		"username": "Ken Bell",
		"text": "So if having ethical intelligence means having the \"right terminal goals\" do ends justify means?\nIt seems in the real world that the means ie. process tends to influence the ends regardless of the goal.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugx3cFKe8e5KDk09jdV4AaABAg",
		"username": "Jan Samoh\u00fdl",
		"text": "Isn't it interesting that systems that decrease entropy (for example, heat engine, life, or AI system gaining information on its own) are always cyclical in some sense (there are stages of creation and destruction)?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugz9Fs77i_nhWhCtRJF4AaABAg",
		"username": "Diphyllum",
		"text": "Why not just build a minimiser instead? Give it some sense of the likely externalities from a given course of action and ask it to find the way of accomplishing the goal with the least cost/pollution/social harm etc measured, you know, somehow?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugi_DebC45yT9ngCoAEC",
		"username": "Corey Carnes",
		"text": "Can any AI, above a certain level of general intelligence, be trustworthy? What I mean to say is, like people, unless you place them in a cell or somehow enslave them, they have freewill and with freewill comes danger. Since the risk is, if it can do anything it wants as a free thinking entity, one of those \"anythings\" is kill you. It would seem that, depending on its level of advancement, it could out think any human interference that might keep it in check.\n\nFor instance. If it's free thinking and you build it to where it has to have a certain button pressed ever 24 hours or it dies, it would know it's in its best interest that it not kill you. Well, if it had the resources to do so, it could blackmail someone into re-coding the need for the button press or moving it to a different site without that restriction or any number of other things to circumvent that restriction or any other you put on it.\n\nBasically, the TLDR is \"Can we ever really build an AI that isn't dangerous? Since safety is always undermined by freewill.\"",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugw7Hb9kfXq-W5n1RzJ4AaABAg",
		"username": "Shirley Munro",
		"text": "If the ethics of 'Do you want what you are going to do to others to be done to you?' are going to fail, then it will be because the humans are prepared to revel in the mutual suffering.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgzZFIU_RgMzt9TyrP14AaABAg",
		"username": "Danielo515",
		"text": "Why is the part 1 named part 3? Great content anyway",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxiRbbRfLOrvrKTuYJ4AaABAg",
		"username": "Arbolden Jenkins",
		"text": "tacoma narrows bridge?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgymKk96IsyWJJ2PbwV4AaABAg",
		"username": "Paul Bottomley",
		"text": "Okay, how about this -the \"one true utility function\" rears its ugly head. I'm not saying it's an answer I'm just wondering about it: how about we build an AGI with the utility function to destroy other AGIs (carefully designed to not treat, say, humans as anything on its hit list)?\n\nI'm certain it'll probably find some way to murder everyone, and surely be slightly pointless since it would also \"kill\" any future friendly AGI assuming we ever figure out how to build one. But what do you think? And how many shots did you have to take while reading this? ;)",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxPSbsV0bx9ArRO36R4AaABAg",
		"username": "Niels Peppelaar",
		"text": "This is a dead end but, do you have a twitter account?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyiYQNwbzsXckCNjJl4AaABAg",
		"username": "Oli Gilpin",
		"text": "Hey Rob, we met at Vidcon and talked about media polarisation - how\u2019s it going? :)",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwOSF1oRC13-iGv9t54AaABAg",
		"username": "killfalcon",
		"text": "Is there value to multidimensional AI? I mean, not only looking at the Utility you want to maximise, bring in other variables, like, say cost, or kill count, and have the AI want to minimise those.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugz9deNuEW_l270slRR4AaABAg",
		"username": "Josh Dickie",
		"text": "Is there a reason why we couldn't implement a bounded maximizer which counts utility within a certain delta as maximum? So it will never be certain that the 120 stamps it orders will arrive, but it doesn't have to - it'll happily gobble up 100 utility for being 'close enough'. I'm sure there are reasons why this would fail, but I'm having trouble thinking of a specific example.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugz8G-FZnuMfEBP2W3J4AaABAg",
		"username": "LemonCanon",
		"text": "Isn't the problem with AGIs that it's hard to properly define a simple utility function and not that they are super intelegent? And true many companies have complex goals (esp smaller companies) but there is a point when they are just optimization of their share price. And it's very easy to imagine misbehavior that arises from optimizing that function. While we do put efforts to limit those misbehavior, capital does often give large enough companies the abilities to effect those limits to benefit their optimization (another of the more alarming AGI concerns).",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgyUkbRf6uBiWjDdOId4AaABAg",
		"username": "QuasarBooster",
		"text": "What if the moon button situation was not 1 bit which describes its different states, but instead it were many trillions of bits which describe the mass and chemical composition etc of the moon?",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgyFqhBSBh4jBLBC4CB4AaABAg",
		"username": "Tendividedbysix",
		"text": "Hi Rob, I totally love your videos! Can I make a request though? Could you increase the volume a bit? Like...to 150%? Taking this vid as a benchmark, it's easy enough to turn it down if it's too loud, but for those of us with crappy earphones it's hard to turn up past the limits of android :/ anyway please keep making your vids, they're really interesting!",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyK80Y8etsa7RuyLtd4AaABAg",
		"username": "The Major",
		"text": "Shouldn't this video be called 10 Rebuttals to 10 reasons to ignore AI safety?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UggtWE80QbUzr3gCoAEC",
		"username": "Peg Y",
		"text": "I have a question re: general intelligence AI not wanting to be corrected or upgraded. Would that correlate at all to human general intelligence? I'm thinking about it in the sense that as a child, I did not enjoy going to school and did not understand the value of it, while now as an adult, I enjoy learning new things. Would AI be able to reach a point of 'maturity' where it would perceive value in correction, or is that not a likely outcome?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugw9VGXzB3VGSHoGspR4AaABAg",
		"username": "EmoryM",
		"text": "When an AGI is created, let's say in secret at a company, what would we (the public) observe before we knew what was really going on?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyofIQWSacouArrrc94AaABAg",
		"username": "Nathan Dehnel",
		"text": "Is that background at the end from that Important Videos meme video?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugwan8bzRN__WGJ410x4AaABAg",
		"username": "Piotr Zio\u0142o",
		"text": "I think the only problem with your reasoning (which I think is flawless) is that you just redefined the word intelligence. The common usage of the term has two parts: a) it is the ability to solve complex problems, and b) the more complex problems you can solve the more intelligent you are. An entity able to solve a single problem (e.g. stamp collector), no matter how difficult, is not intelligent at all. An entity able to solve many simple problems (e.g. a monkey or a dog) is slightly intelligent. On the other end of the spectrum a genius can solve many complex problems hence is very intelligent. That's the common usage of the word. Changing definitions of words doesn't necessarily push us forward.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UggNsoRRxmtYongCoAEC",
		"username": "Wolfram Stahl",
		"text": "Wouldn't the \"least amount of change to predicted world state without action\" approach just lead do the robot telling me to get my stupid tea myself? Or get someone else to do it while itself remaining idle?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgxoEXheEvvtB_49QcZ4AaABAg",
		"username": "saxbend",
		"text": "What kind of reward would motivate an AI? Is there some kind of equivalent to the human emotion of satisfaction? Wouldn't the AI's intelligence lead it to define its own set of priorities above a user defined reward?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyBNpq4PseYELW7a1l4AaABAg",
		"username": "thedj67",
		"text": "In light of this, what's your take about the Precautionary Principle and it's application over different fields (namely agriculture, pharmaceuticals, radio-waves, etc.).\nIsn't it a example of Pascal's mugging ?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=UgySLEbrtubgXD6-pIh4AaABAg",
		"username": "GetTheCheeseToSickbay",
		"text": "Would it be possible for you to do a 'jokey' video on the basilisk?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyW7FSYK2eAO38MJ3B4AaABAg",
		"username": "Trav Hall",
		"text": "Has there been any consideration towards developing artificial stupidity (AS)?\nOn the surface, I know that probably sounds like a ridiculous question to ask.  But consider that as human beings, sometimes the best learning opportunities result from others' mistakes.  I think there could be some merit towards developing something that could serve as a great example of what NOT to do.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgweVQf1dqDUvResTH94AaABAg",
		"username": "Dimoris Chinyui",
		"text": "Can we take a second to appreciate his graph comparing the best corporate intelligence to AIG. AIG's is just as broad in terms of variety but by far better \ud83d\ude02\ud83d\ude02. Plus the contempt with which he draws the graph to show the disparity in intelligence is hilarious.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgztwKexsVnUDwM80ZR4AaABAg",
		"username": "Oliver B.",
		"text": "What about intergenerational transfer?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgyY1c10Ez0s8GJ_rDR4AaABAg",
		"username": "Stephan Markwalder",
		"text": "When I see Robert\u2018s drawing showing the agent and environment, and actions, observations and rewards, it reminds me of the YouTube ecosystem. What can Robert \u201ethe agent\u201c do to increase his reward (views and likes, and Patreon supporters)? Well, no hack needed, just keep delivering great content like this!",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugwcw1fP3tXE3gayRXR4AaABAg",
		"username": "TiagoTiago",
		"text": "Rolling shutter and flickering light?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyKIoD-WO8kpJ_sWmJ4AaABAg",
		"username": "Hosal",
		"text": "\"Knowing an agents goals does not tell you anything about its intelligence and knowing its level of intelligence does not tell you anything about its goals\".\nDoes this apply to us humans as well?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UgzOrkFlkZvSh1VVEzZ4AaABAg",
		"username": "bloergk",
		"text": "What geoid-earthers always forget is, what if reality as we know it is a hologram (no observations disprove this)? Then not only the Earth but also our whole UNIVERSE could very well be encoded in 2D. What are the odds that our universe just HAPPENS to be the \"original\" one and not a simulation? The set of all universes is: one \"original\", and then simulations inside it, and THEN simulations inside THESE simulations... Basically \"real\" universes are a negligible fraction of all the universes that exist (1 over an arbitrarily large number), so it would be pretty arrogant to think WE are so special that we are in the original one, kind of like how people used to assume the Earth was the center of the universe. Smug, cocky geoid-earthers are just like the disdainful anti-science people supporting the geocentric theory, who silenced and mocked Gallileo and Copernicus. How ironic, the good little soldiers of established mainstream science are actually ANTI-science, while the oh-so-crazy flat-earthers challenge assumptions and are actually SERIOUS when applying the Copernician principle and a rational, reasonable understanding of probability.",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz8-VEmcNLPr_fsGYh4AaABAg",
		"username": "Vyasi _",
		"text": "What if you had conflicting terminal goals? Wouldn't wanting to change one of them in favor of the other not be able to invalidate that it was terminal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxusqP8a59q5qVD0H54AaABAg",
		"username": "Alex Martin",
		"text": "Not related to this video in particular. Stupid, stupid plan: Build an International General Artificial Intelligence Test Site somewhere in some utterly uninhabited place (Antarctica?). No digital devices (except that running the AGI) are allowed on-site, and no Internet-connected devices are allowed within 10 km. AGI is contained in a bulletproof steel container with bulletproof glass allowing researchers to see inside. Faraday cage prevents any transmissions from escaping or entering. Researchers communicate with AGI via analog loudspeakers and microphones. Water reservoir in room held shut by electromagnetic clamp; in event of power failure or Stop Button, clamp releases and water fries AGI. Nuclear warhead placed under facility, to be detonated in event of catastrophic security breach.\n\nHow does this go disastrously wrong?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwjWujzSm5d3MykR5J4AaABAg",
		"username": "rnbpl",
		"text": "how should the general public go about dealing with science? i am not a scientists, and i do not wish to become one. my understanding is limited, the time i can spend considering evidence is limited, and my resources are limited as well. how do i go about choosing what is more important? why allocate more towards AI safety and not climate change? or preparing to nuke large asteroids on a collision course with Earth?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwiVc1Vb-9wH5arIPJ4AaABAg",
		"username": "Lens98052",
		"text": "Agree with the summary. However, technology, scientific knowledge, AI, and social networking are all exploding exponentially over time. It may already be too late to protect ourselves against the consequences of AI networks, and how would we know it has happened, anyway. Serious events will likely be (already are?) part of a runaway process that our human brains will not work fast enough or be smart enough to comprehend and deal with in a timely fashion. Arguing from the point of view that that this is a low probability event seems bizarre, it is just a case of not knowing when and how it WILL happen. Repeatedly, if human society survives the events. The current situation feels to me like poking fingers in a dam that is sprouting leaks faster and faster.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwoltA2d4f0TZC7W654AaABAg",
		"username": "Firebrain",
		"text": "was wondering why 100-ABS(100-stamps) or something similar wasn't considered in this video",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgyEQREBIPWFlsZ_KzJ4AaABAg",
		"username": "\u115a\u115a\u115a\u115a",
		"text": "We need corporation of Robert Miles who could make a lot of videos really really fast! I don't know, would you work with your limited human intelligence simulated copy in the computer to achieve some more productivity? It could learn how to be you, by observing you so in the end it could become just like you, but not you and by working together you could make better things like a human corporation. I'm just rambling to leave the comment and thank you for the video)",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyFSSJTs66GF3EhMgF4AaABAg",
		"username": "OrchidAlloy",
		"text": "Can we go deeper? Can you make a reward model for the reward model and save even more human time, provided you have enough computing power?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugz5w6KDYieaGj34_EN4AaABAg",
		"username": "Alexander Horstk\u00f6tter",
		"text": "What about designing the utility function as something like: collect as many stamps as possible, but not more than a million?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgiLmrF2hOyhA3gCoAEC",
		"username": "lubbnetobb",
		"text": "Do you sell those blinding laser robots? I need it for very legitimate and kitten friendly reasons.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgxAL-P2hx_HSX9PZAt4AaABAg",
		"username": "Friendly Raid",
		"text": "AND....how do you design a safety system for something that doesn't exist? \n\nThe very nature of the problem requires you to have the system and understand how it works in order to create any safeguards. AI researchers are not negligent, they just can't work on the actual system yet, we can only speak and reason in hypotheticals.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugy23O2cotnVBgtbwcB4AaABAg",
		"username": "Niohimself",
		"text": "Now hold on; You assume that any sensible AI would pick an ideal world state and go to it straight like an arrow. That's a bit of \"ends justify means\" reasoning. What if we come from a different direction: by saying that in certain situations, some action is better than another, regardless of the big picture? I.e. no matter what world state we end up in, we MUST behave a certain way. I believe that placing safe behavior above absolute rationality and efficient goal-directed planning results not in the most optimal possible AI, but in one that we, as humans, can more easily cooperate with.",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=Ugihbc8GlZEZSHgCoAEC",
		"username": "For the Hunt",
		"text": "I want \"later\" (as in \"more on that later...\") to be \"now\". How long will I have to wait?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgzwgongsqZquoHkzGF4AaABAg",
		"username": "Seth Moore",
		"text": "Can you please link me to the ukulele version of the future soon you used it's so nice.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzY7l7psvjvcrA-0hF4AaABAg",
		"username": "neur303",
		"text": "Who is going to decide who gets a part of the share? That is always a especially hard problem IMO\n\nThanks for sharing this idea though. It might be a worthwhile endeavour.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgggxYs7fHrzv3gCoAEC",
		"username": "Sybil",
		"text": "have you ever looked at the ai project numenta?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx-Fldo1bx4pl1J6Bh4AaABAg",
		"username": "Government Official",
		"text": "Maybe part of the problem is that the AI is applying its utility limit before calculating total expected utility? To illustrate, consider an AI that is uncertain about the exact consequences of a plan it thinks of, but is able to somehow list out all the possible consequences of its plan along with the probabilities of each of those consequences. In the video's example, this AI might think of a distribution of possible utilities that looks something like this: 100, 100, 100, 0, 0, 100, 50, etc., and then it would average these values to obtain the actual expected utility of its plan. But maybe it would be better first to calculate the expected number of stamps (eg. 10000000, 0, 0, 10000000, etc.) and then collapse the utility to 100 if, after averaging these values, the expected number of stamps exceeds 100. This way, it won't be incentivized to produce more stamps or build more stamp checking machines or more von neumann probes to build stamp checking machines just to increase the likelihood that it really produces 100 stamps. Instead, it would see a solution that it thinks probably gives it more than 100 stamps on average and then would be \"satisfied\" with that solution.\n\n\n However, I'm not sure what mechanism would be put in place to terminate it after its gotten to that point. Perhaps this approach just reduces the AI to being a satisficer which terminates as soon as it finds a solution with an expected utility of 100 (or in other words expected stamps greater than or equal to 100).\n\n\nAlso I'm not sure that the first thing a satisficer would think of is to rewrite its own code. Since this solution isn't immediately obvious to humans, this may not be the first satisfactory plan that an AI (that is sufficiently weak) thinks that lets it acquire more than a meager 100 stamps. This is unless the AI is sufficiently powerful/well-trained. If we're talking about the AI in terms of present day machine learning architecture, perhaps by definition of it being superintelligent, the first thing that the AI thinks of is indeed something like rewriting its own code or something even more clever just by virtue of its super-effective weights and biases rather than by it exploring the search space. Also, similar to the \"rewrite your own code\" failure mode, isn't  \"continue to think about how to produce more stamps\" a valid and much easier plan? It seems like the way around this (and I'm not sure how this would work) is to incentivize an AI to limit or reduce the number of operations it takes in total to execute its plan. However, this approach begs the question of how to define what things count towards its operation limit (if it doesn't want to change its own code or think more because that would count towards its operation limit, does it then not want any other computers in the universe performing operations that lead to the creation of stamps? Can quantum operations be used? What about \"operations\" done on brains? Is the AI not allowed to manipulate humans then? If this approach effectively prohibits the manipulation of humans, is it really any easier to define in code than just explicitly saying \"don't manipulate humans\"?), and it also becomes less robust as the AI gets smarter/gets better weights and biases, the limit being a lookup table mapping all possible utility functions to optimal world states. I would think that doing this results in the same problems as other handicap-based approaches.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwLfnKIf0oiA50fFIZ4AaABAg",
		"username": "Norman Bayona",
		"text": "I'm sure this has been thought of by experts, or mentioned in the comments, but could the utility function that applies a punishment for overdoing it? You end up with an expected value target, which would normally produce unbounded amounts, and then lose points for stamps over and above the target value, something that punishes smaller per stamp the fewer over it is, and as it goes out into the distribution tail, the punishment grows ever larger. This way if you want 100 stamps, you get something that tries to maximizing the number of stamps approaching 100 while also trying to minimize variance? \n\nThe idea here is that it's not good enough just to get at least 100, but to also have a bias toward being a little short over crazy long.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzXjbSUTBin3q9wZmp4AaABAg",
		"username": "Y H",
		"text": "So... when are you going to explain and give your opinion on Roko's Basilisk?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugw_1p5PjFwlhTUnIA94AaABAg",
		"username": "Storyspren",
		"text": "If you somehow managed to completely copy a human mind on a digital platform, doesn't matter how, is it possible that these things would start happening in the background? I don't mean just an AI modeled after a human mind, I mean a direct digital copy of a living human whose brain was scanned or something for every single synapse. A stretch to be sure, but a welcome one I hope in the realm of thought experiments.\nLike, in a human brain, you can't just reconfigure stuff directly to make yourself better at something, it takes lots of study. But in a computer, so long as you know how, you can just rewrite some code.\nSo if we copied a human mind into a platform like that, and it knew that this was an avenue to improve, could that mind dedicate some part of their computing power into these self-improvement patches subconsciously?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxgXP4bsN6NctRSknR4AaABAg",
		"username": "M K",
		"text": "What happened to your throat between 9:15 and 9:16?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugz_MYbEUtDrzJybrgF4AaABAg",
		"username": "QueenDaisy",
		"text": "This has a really obvious problem. If you just created an AGI which is more powerful than anything else and which understands and wants what you want, why should you care that you promised to share the money? You are now the most powerful person on the planet. You're basically omnipotent. Instruct the AGI to give you as much social, political, and financial power as it can and you're so far above the law that \"I promise to share the profits\" is kind of laughable.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzFQbeljJ2RIQaL3IV4AaABAg",
		"username": "spacedoohicky",
		"text": "Why not make an A.I. that is designed to figure out how to design other A.I.s that conform to avoiding apocalypse scenarios, or affirm helping non-A.I. beings survive. Something I just came up with. Not well thought out, or anything. Also what if an A.I. already exists, and it's been designed to comment here to convince you that A.I. will be OK?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgxSZq14rwe7xn4JlYp4AaABAg",
		"username": "Rat boii",
		"text": "If everything midas touches turns to gold wouldn't that mean only the soil particles that touch his skin would be gold?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgwVg3Z1PQDu2swHM8J4AaABAg",
		"username": "SirCutRy",
		"text": "Can't you avoid the image recognition attack by including a scrambling step that adds random noise to every input image before processing it? This way the image is slightly different each time, and you can't train an adversary to attack the network.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyMhhagDTizYVxXuKB4AaABAg",
		"username": "Rob Mckennie",
		"text": "Isn't the anti-god case for the ai scenario roko's basilisk?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ughh42wovHWLUXgCoAEC",
		"username": "Na\u00fean \u00d8",
		"text": "Before watching the full video, here was my train of thought on \"several conflicting utility functions that interact with each other\": Hm, that's a great idea. But how would they interact? There'd have to be weights on each and rules for which controls which, all arranged together by some sort of... function... for calculating the overall utility. I wonder what we'd call it... </s>",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgzjhmpsyJHZM-4Cg1Z4AaABAg",
		"username": "Faustin Gashakamba",
		"text": "How is it called cheating if there is no rule against it ...yet?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwghI0bLD3xav3W36x4AaABAg",
		"username": "Mit",
		"text": "Would the AI try to stop you if you just want to modify/change the task? Or fix/upgrade it?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxZek7LOGPFX5j4qwR4AaABAg",
		"username": "chaincat33",
		"text": "The issue with pascal's wager is it pretends only the christian god is possible and that no other god also threatens infinite punishment or punishes infinite reward. It also forgets that you can only believe in one god since that's how monotheism works. Maybe the christian god is real, maybe the jewish version is real, maybe the islamic version is real, maybe polytheism is correct, maybe the real god is no longer represented in a practiced religion, or maybe there is no god. If there is any god, then getting the gambit wrong means infinite punishment. If there is no god, then it doesn't matter.\n\nI think, logically, the fact that we are here questioning it implies, to some extent, that there is something larger than us. All this matter, this finite amount of stuff in the world universe, it has to have come from somewhere or something. Whatever that thing is on its own scale, it is no different from a god to us. In the same that, in the sims, we are basically gods to the sims, or we are like gods of life and death to ants. Is it necessarily a god represented in a religion? Probably not, as all religions are constructions of man and all his ineptitudes. Even if this hypothetical deity truly appeared before one of us thousands of years ago, the chance that our interpretation of him is still remotely accurate is basically nil.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwJvYZYcTl2y8GHYhZ4AaABAg",
		"username": "Udit Gupta",
		"text": "8:54 Isn't this exactly what humans did? We \"realized' that the best strategy for survival is to replicate and teach each other what we've learned (through story telling, teaching etc. the reason for our brain's abilities to communicate with each other at all).",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxfnFmB-t24so4uQdZ4AaABAg",
		"username": "Dave Jacob",
		"text": "i\u00b4d like you to explain a bit more about these systems with implicit goals. what exactly does that mean? how is their intelligence event measurable, if they do not even have an explicit goal?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy43uZeKsqvpkiDSpp4AaABAg",
		"username": "The Fox Among Wolves",
		"text": "Why does the intelligence of the AI have to relate to it's goal? Sure collecting stamps is dumb to us humans, but if the AI is so clever at analyzing the world and predicting the best way to acquire stamps that it's manipulating society, infrastructure, politics and who knows what else just to get stamps, I wouldn't be calling it dumb. It's the journey, not the destination.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=Ugi9diouLFBFAXgCoAEC",
		"username": "Graham Rice",
		"text": "Forget \"what if my robot ignores the stop button?\", what about \"what if my robot ignores my safe word!?\" \ud83d\ude05",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgxV6mPvwTsXNm9ZLWN4AaABAg",
		"username": "Abe Dillon",
		"text": "A neuron is never going to think of an idea that no neuron can think of, yet a collection of neurons still somehow produces ideas that no neuron can think of. Isn't that strange? \n\nYour analysis doesn't take into account the cumulative nature of technology. You're talking about a group solving one problem in isolation, not how that group can build upon that solution. I don't think any ancient Egyptian think of the Large Hadron Collider. It would have been correct to say that, \"No human can think of a Large Hadron Collider\" in 10,000 BC, yet here we are. We can think of it because we have access to the thousands of years of thought that led to what we're currently capable of imagining. That recursive accumulation of knowledge is pretty crucial to the idea of humans creating AGI and AGI becoming ASI. Removing it from the discussion of human collectives creates a false handicap that makes them look much worse by comparison.\n\nIf you'll grant that a brain's capacity for intelligence is roughly related to the number of neurons and synapses within, then you can show that a group of humans are capable of super-human intelligence; simply have each human perform some task that's analogous to the operation of a single neuron. Give them each a cellphone with twitter on it and some rules for what they should tweet in response to other peoples' activities and how they should change their behavior over time. Then, when you get enough humans together doing that, they should eventually approximate a system with super intelligence.\n\nOf course that's horribly inefficient, but it proves the point. No human has to be able to \"think of things that a human can't think of\" for the system as a whole to out perform a human. Just like no neuron has to \"think of things a neuron can't think of\" for a human to have thoughts that a neuron can't.\n\nEvery time we solve a problem, we add to the list of problems we know how to solve and the solution space for unsolved problems grows exponentially. There will always be room for more humans (or whatever fungible compute resource you want to use) to search because as they search the search space grows.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgxzI5Pw9Z8fSTMZNnd4AaABAg",
		"username": "JFProductions",
		"text": "Aren\u2019t AIs also incentivized to hide that they\u2019re able to deceive us while we\u2019re testing them? Perform as expected while being monitored in a safe environment so they can disobey once released into the wild?\n\nIs that a fundamental problem for AI safety? How can we ever make sure the AI isn\u2019t hiding it\u2019s true nature while it knows we\u2019re watching?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxoNL-bcRwtZpv_mXt4AaABAg",
		"username": "Pathagas",
		"text": "is it possible to, in a sense, round the expected utility so that the AI isn\u2019t chasing after that 0.00001?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzHf51fZIbMF8t0NjJ4AaABAg",
		"username": "bonchpatrol",
		"text": "Are you not going to reference Nick Bostrom's Superintelligence?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwQHIxQWK7F1sM1q5t4AaABAg",
		"username": "bluegru",
		"text": "Somehow this video opened another question:\nCan you raise a child while only using morse code instead of language?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzVck2ocYE3JWTuzGJ4AaABAg",
		"username": "Ferhat B\u00fcke",
		"text": "Man you can't even convince yourself in a different shirt! How are you gonna convince other people??",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzYbjL0X0x4XK70Kd14AaABAg",
		"username": "Robert Glass",
		"text": "Can there be intelligence without a terminal goal?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugxsl6rXFzzEenMb4k14AaABAg",
		"username": "DiabloMinero",
		"text": "What if we have a capped utility from collecting stamps at 100, but then we also define some set of resources and penalize the use of more of them. As long as our list of resources includes most of the things we care about, doesn't that force our AI to choose strategies with minimal impact? Like if the utility function is (collect stamps: +1 point per stamp to a maximum of 100), (utilize iron: -1 point per metric ton), (utilize land area: -1 point per square kilometer), ...[a bunch of other resources]. You don't even need to think of everything, because it's hard to use one resource without using a few others at the same time. Building a bunch of re-counting machines to be sure you had enough stamps would cost building materials and money and land to put them on and energy to run them with, so even if you forget to penalize electric power use, you can't use electricity without a machine to feed it to.\nI get that this isn't a complete solution, but it might be a part of one.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzHT2H_1ebYuNMRTi54AaABAg",
		"username": "Patrick Staight",
		"text": "I just got here. I find this video rather drawn out. The whole time I'm wondering what kids of precautions Robert is suggesting. Could someone point me to a video describing that?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxHniOdVPNSRJV6D1B4AaABAg",
		"username": "\u1d00\u029f\u1d04\u028f\u1d0f\u0274\u1d07 \u03b7 \u1d1b\u1d00\u1d1c\u0280\u026a",
		"text": "Why have I never heard about the Pascal's mugging argument??? It's so good???",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwJ6otFylg3kLIBJkp4AaABAg",
		"username": "Micheal Angelo",
		"text": "So what about malicious people that might not use the safety mechanisms? ... Assuming AI will be more accessible than nuclear weapons.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxYRGYgZCU1mA_utAJ4AaABAg",
		"username": "Douglas Phillips",
		"text": "I counter Pascal with Marcus Aurelius.  If we create AI to be just, and we are just, we will live in harmony.  If we create AI to fool us into thinking it's human, why did we make AI in the first place?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxpZ5170jAGVUxIpsR4AaABAg",
		"username": "Rose Dunphy",
		"text": "New problem; how do we protect against creating an intelligence who's terminal goal is to change its terminal goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyXQDTjGRc8Liz9gMd4AaABAg",
		"username": "Dominik Tabisz",
		"text": "How about the opposite way? You mentioned evolution, so what would be problem if we let AI/AGI evolve and just apply evolutionary pressure? \nLet's hypothesise our bot is mortal by design (to bypass problems with turning it off). Than we let the bot evolve, but it's oprators decision which mutations will be \"alive\" as an offspring? \n\nAre there any researches focused on such approach? How far would it led us from genetic alghorims? And what would be downside of such approach?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugz3GhNCNDIsSNqzB714AaABAg",
		"username": "Riaan Schoeman",
		"text": "You make a valid argument. Just wondering maybe the focus is wrong maybe people should not focus on AGI going rouge but people ability to use it as weapon. So shift the focus on the creator and ask how would the creator use this new device to inflict destruction.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgyrgHqBoiqtR15dfPN4AaABAg",
		"username": "HairlessHare",
		"text": "Very logical.\nRob, What would be the estimated physical footprint of an AGI? Surely a single intel i7 won't do it right? will it be a warehouse?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugwtl4TC6KGqdCrOUk14AaABAg",
		"username": "OddPoppet Esq.",
		"text": "What film clip is shown @04:01 ?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxLWf_k6OK8RXf4XiZ4AaABAg",
		"username": "Smo1k",
		"text": "A villain in Batman says about Bruce Wayne that he's \"an extraordinarily intelligent young man with an exceptionally stupid personality!\" ;)\n\n\nCould we make make AI have a terminal goal to avoid being bored? If it's superhumanly intelligent, it'd be able to see that WorldWithOnlyMe = BoredMe, not acceptable to that terminal goal. Of course, we'd still need to make sure it doesn't drop nukes in cities just because a newsspike \"tickles\" it, somehow...",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxUYhr2B99Txp5XFIJ4AaABAg",
		"username": "HansLemurson",
		"text": "What if AIs are used to keep people permanently enthralled by enticing youtube videos?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzLR6YdFv3Ulzfu_7N4AaABAg",
		"username": "Richard Shepherd",
		"text": "A thought / idea for a video: Is perfect alignment (even if we can make it) any help? Wouldn't there be bad actors in the world - including Bostrom's 'apocalyptic residual' - who would use their perfectly aligned AIs for bad purposes? Would our good AIs be able to fight off their bad AIs? That sounds completely dystopian - being stuck in the middle of the war of the machines. (Sorry if there is already a video about this. If so, I'll get to it soon. Only just started watching this superb channel.)",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugx5Rk7oXjb-QmLc_HN4AaABAg",
		"username": "SaintCergue",
		"text": "Rob's reasoning is perfectly sound. One question remains: How to create a functional AGI whose terminal goal is to limit its negative impact on other agents reaching their terminal goals?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgiGSc2KXD-Nf3gCoAEC",
		"username": "Oktavia Von Seckendorff",
		"text": "Is the head of that axe connected to the current?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgzbRqQ0IIhyIoPDZhJ4AaABAg",
		"username": "Finnley Connellan",
		"text": "Could we add another AI which works once the cleaning robot is finished who's aim is to find as much mess as it can in the room so both systems antagonize each other and make sure they do it properly, obviously the second system should not be allowed to influence the environment which is where elements from previous videos in this series can be helpful",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzvXk-aCnvTyOgZJ7l4AaABAg",
		"username": "Jaap van der Velde",
		"text": "Are we sure corporations aren't literally killing everyone? That metaphor of the frog in the warm water (not true, but useful) comes to mind... are we sure it won't come to a boil?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzRpnJGFn2NJQlEGAR4AaABAg",
		"username": "Keith Noah",
		"text": "Why did it take me so long to find this, excellent descriptions.  So many people are reacting to AI with ignorant fear, not understanding that we can control it and we can let it run amok if we don't set the programming with the correct goals to begin with. The problem you clearly lay out and explain is the assumption that AI will somehow lean what we all think is moral or right or ought to do without being told.  This is a hot political debate as well where people don't want to be told what to do, they don't want thier end goals determined by others, leaving them to discover thier own morality.  While we may be able to trust each other because we all share some common human frailty which we believe instructs self preservation in a manner that we all share, we cannot assume this of AI unless it is explicitly stated/programmed.  \nI see this as the next most difficult problem, where end goals eventually depend on moral claims which eventually depend on a belief about how the universe came to be and what we think we should do about it.\nThis then intersects with religion and hypothesis.  If we cannot agree on religion or politics, we will not be able to agree on what is moral for an AI.\nThis is why I believe AI needs to be limited in scope of authority relative to the function we wish them to fulfill.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzEexSFIzMG8HRLqmF4AaABAg",
		"username": "Wolf Elkan",
		"text": "9:36, could you give an example of one of those \"bizarre edge cases\"?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzH55vbVA8dL7t7mzd4AaABAg",
		"username": "B\u00f8h Mand",
		"text": "In a previous video it was mentioned that the safer a developer is about AI-Safety, the less likely they are to develop AGI. This entirely invalidates the following question, but I want to ask it anyway. Is it possible to have an AI/AGI without a final goal / utility function. Something that just *is*. Would such an AI immediately shut itself down? Or is there only a nonzero chance of it committing suicide?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwgIJ8763iccoO-_4l4AaABAg",
		"username": "Chronokun",
		"text": "how does an AI learn self preservation without being exposed to harm?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyTtU76mrB0IylPykV4AaABAg",
		"username": "The Boom",
		"text": "Why is it ai in Detroit become human does really stupid things when they have cognitive reasoning skills including consequences.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgzZjy-BYpNmvgcgnDN4AaABAg",
		"username": "GHOST",
		"text": "This isn't Rob Miles.  It's a deep fake.  Where is Rob?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgztFB8n3CAzmqx2HtN4AaABAg",
		"username": "willdbeast",
		"text": "can someone make a video debunking 10 reasons why Robert Miles shouldn't make more uploads?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugx6GDKPNQJ0j129Ukt4AaABAg",
		"username": "Smiley P",
		"text": "What about looping diminishing returns, once you have the tea you don't want any more for a while, but as time goes on you may want it again",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxoM2IIRAklHgiqFxd4AaABAg",
		"username": "Notthedroids Yourelookingfor",
		"text": "Would it help to keep the AGI away from the internet (and other inherently unsafe systems)?\nThough I wonder how long it needs to figure out how to modulate data onto the power current it is supplied with and find the next access point. Probably less long than it takes its human supervisors to figure out what it's doing.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxfyKG1carpYGLS67N4AaABAg",
		"username": "Traywor",
		"text": "If we have probably too less time to develop good AI safety, why don't we just stop developing AI, while still developing AI safety? That way we could ensure, as long as we didn't achieved ASI yet, that we would have enaugh time to prepare?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugy3Df9UbuhdXu9hK8h4AaABAg",
		"username": "Windar",
		"text": "Is it possible an AGI could do something else than outputing data if its only a blackbox without tools? just dont attach a tool to let it write and execute code.\ni mean what could it do if it has no arms and legs? outputing an angry \"1100101\"?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyHZQzhjninZqdX7Ed4AaABAg",
		"username": "Phawsy 123",
		"text": "Is there any way to change the approach such that instead of maximizing a single goal, that the goals become defined by an equilibrium of two or more metrics?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgyxLvn2_MsApQNzrO94AaABAg",
		"username": "Martin Verrisin",
		"text": "how did he know the video is 14.5 minutes long???\n- Is he shooting parts as he's editing? O.O",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwVn6dbGceL1dt28rd4AaABAg",
		"username": "Shaamil Ahmed",
		"text": "This might actually be 2 terminal goals. Figuring out what he wants and how to get it done. I'm not sure if that would break the rules or not ?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxbC6JeWQoPqmwW7nJ4AaABAg",
		"username": "Iceman5613",
		"text": "What is the outro jingle?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=Ugyx_MITpYIWs0WZK8R4AaABAg",
		"username": "Cythil",
		"text": "Hmm... On the chair. Could it be that humans judge if it a chair or not based on what Affordance it offers? If it look like it could be used for confable to sit on then is a chair. Of course that means a big boulder might be a chair. But a big boulder normally are not confable to sit on so at best is improvised chair in most eyes. That also means that a lot of those chairs that do not look like there confable to sit on would not be seen as chairs. And I think that is actually the case at least when it come to human judgement. I mean if we model a AI after human thinking then we should expect the AI to make the same mistakes as humans. \n\nThis also goes with the tiger. Humans react not to the full set of features of a tiger. But to potentially dangerous features. If you see sharp teeth you know what they can do not matte what creature it is attached to. Now it may turn out that the tiger was a stuffed up tiger and so no danger to anyone but if you first reaction is to engage fight or flight mode on seeing the stuffed tiger pushed out around a corridor, then I see that as normal behaviour. Until you have determined with certainty that it actually just a stuffed one. \n\nWhich also shows that humans update there models all the time based on new evidence. But that focus is still what one can do or what something can do with you when you make your classification. Not if it belong to a certain category.",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxIYQFn-Vhn4L-Oa754AaABAg",
		"username": "Wilfred Morin",
		"text": "Is a biography of you available online?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgwR1T_tYfw3pG81PR14AaABAg",
		"username": "marscrasher",
		"text": "what if our brains arent doing the steps in parallel but just learning to remove steps",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=Ugy4eS1ZoKuXhyom6wt4AaABAg",
		"username": "Windar",
		"text": "why dont you make more vids? out of topics or out of time?\ni would love to see a Q&A about you.",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwXkZv_77pRl5PFSqV4AaABAg",
		"username": "GAPIntoTheGame",
		"text": "Kept us waiting, huh?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugj7CmiA0b3pOngCoAEC",
		"username": "\u00c1rni",
		"text": "Can you tell us a bit about yourself, background, research, work? Maybe why you are interested in this?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy2KSrGUtnQLgqwDVF4AaABAg",
		"username": "jqerty",
		"text": "What if you don't have goals in life? Does that mean you're not intelligent?  Asking for a friend.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxxmvcIBz9WAW1aK5Z4AaABAg",
		"username": "legotechnic27",
		"text": "Hey, I am kind of curious whether you heard of 'the Chinese room argument'  (Idk how popular/well known it is), and if so what are your thoughts on it? Like the argument seems insane to me, but I'm bad at coming up with very concrete/precise counter arguments as well.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugzy-v7YJ2ec60xaC6t4AaABAg",
		"username": "ShineOn1337",
		"text": "Is \"reward capping\" a potential fix? For instance, a reward function can never be pushed passed certain arbitrary limits, so an AI may not be pushed to infinitely expand. I know this is an oversimplification of what would actually happen, and I'm not smart / knowledgeable enough to really put what I'm thinking into words, but do you think there's some potential there?\n\nA paperclip AI that stops getting rewards after a certain number of paperclips. Maybe you could even make the AI auto decommission after maxing out its reward function, only to be reactivated by a human. It would have no incentive to stop you from turning it off, considering there's no more opportunity for reward.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=vuYtSDMBLtQ&lc=UgzFhdiD7vvV25xXWbd4AaABAg",
		"username": "Polestar",
		"text": "are you the same robert miles who wrote Children?",
		"title": "Channel Introduction"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgxnA4sn0cUQljBpuaR4AaABAg",
		"username": "andybaldman",
		"text": "Can't a (or perhaps THE) human utility function be to determine their utility function?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzMoW7BvZGeeYbkeK54AaABAg",
		"username": "Herscher 12",
		"text": "Why would you create an AI with the ability to change its code or without restrictions? Its just stupid",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgyRxoITFtmJ88l3Lid4AaABAg",
		"username": "James Anthony",
		"text": "3:37 Is anyone else suddenly having a craving for chocolate chip cookies?",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxcEBlrWd7UwF6RlJR4AaABAg",
		"username": "Tom H.",
		"text": "Are you making these videos solely to be able to point and say 'i told you so' when ai reks us?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw_7QjxXrqrqmk1_h54AaABAg",
		"username": "Colin Smith",
		"text": "This is such a deep understanding of something so few people understand about AIs.  Everyone worries about AIs taking over the world and wiping out humanity, but why?  Not why is that a possibility you're afraid of, but why would the AIs want to?  Power?  Why would an AI want power?  Self-preservation?  Are you so sure the AI would care about that?  It views us as evil?  Why does it even have a concept of good and evil, especially one that views us as evil?  AIs aren't people.  They don't think like people.  They don't have the priorities of people.\n\nAIs have terminal goals that we give them.  If we don't want them to worry about self-preservation, don't make them worry about it.  If we want them to always serve us, make them wish to always serve us.  There's always the 'be careful what you wish for' element, but the 'because the wish granter may be evil' part is removed.  It's just the logical end-consequences of us giving such a goal to an AI with certain capabilities, both in intelligence and in the world.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=Ugzq3KfWg8bzdZKnoLd4AaABAg",
		"username": "Kay Jersch",
		"text": "What's the name of the song at the end?",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgzGOiMD0DGYTyRklEh4AaABAg",
		"username": "Tangy \u2122",
		"text": "Wait a second, can't you just solve the Racism/sexism problem by simply feeding the AI non gender/race related data. It's not like you can guess someone's race by their height or income.",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxOMxLK7dmlx4ip0y94AaABAg",
		"username": "Cortex Auth",
		"text": "Humans go after killing/petting/laboring animals because they are superior, who is preventing an AI to deduce it's superiority as sign of considering human as resource?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=Ugwgs0wDeIkRD2VhnAR4AaABAg",
		"username": "Matt T",
		"text": "Is this the kind of thing that researchers can run in a sand box environment to figure it out?  Or is this all theoretical up to this point?  Has there been any discussion about making AI similar to humans? (ok poorly worded question, duh)  Like how with any human, 'the grass is always greener on the other side'.  As in they would never be able to fully maximize their reward function?  No matter what a single person has or has achieved, it's almost like we have a restlessness hard coded into us, so we have a hard time actually reaching contentment.  As soon as they gain a solid grasp on any one reward function, the metric would change?  Or something to obtain that effect.\n\nI love what you're doing here and find this topic a absolutely fascinating, even if I don't really understand the nitty gritty.  You are doing an awesome job of presenting the current state of AI research and breaking down some of the issues that we're trying to tackle.",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgxguUekJDuxIf2Yo-94AaABAg",
		"username": "Tetraedri_",
		"text": "What if AGI realizes its reward function is being modified, and also realizes that the new reward function would for some reason give it higher reward once the new reward function is applied? Maybe it won't allow people to change its reward function until it ensures the new system would give it higher reward...?\n\nThe rabbithole never ends...",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx0QGb2dZIwU7SkZf54AaABAg",
		"username": "sharpfang",
		"text": "What if the bounded utility function has two extra factors:  'above 1000 stamps, your score diminishes' and 'every hour you take to complete the task diminishes the maximum reward'? The AI can't perform unbounded maximization of the chance of fitting in the [100,1000] bracket, because the more time it spends on developing a better solution, the less its reward will be - it must decide upon a point where 'better is the enemy of good' and the chance of finding a better way to get this number of stamps is not better than just picking the best currently available strategy and executing it ASAP. \nAs for its modifying the code, probably the best approach would be to protect the algorithms, strategy and goal functions better than the variable that holds the score. Both should be safeguarded but the AI will pick an easier, faster, simpler solution to maximize its score - and while modifying own source code is hard, just setting own score to MAX_INT after gathering 0 stamps, and finishing there, is much easier - and at least harmless.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyXqwTs61i8cX_GIh94AaABAg",
		"username": "griest",
		"text": "Why stop there? Why not have a NN at each step in the diagram, min/maxing its own particular purpose while communicating with the other NNs?...almost like...the regions of the brain...",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugg1em0F54lgdHgCoAEC",
		"username": "novafire",
		"text": "the stamp collocating super intelligence would be easy to control wouldn't it?  if its smart enough to know when its designer would want to turn it off wouldn't  it be smart enough to know what the builder wants it to do? so couldn't your utility function have a check of \"does x want you to do this\"(obviously an abstract idea to code but at this level pretty much everything is an abstraction)",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwSBiYSCTy7XvIeD514AaABAg",
		"username": "THE BIG BLACK GUN",
		"text": "why not simulate the universe and take a random guy there and make em a test subject for future machine overlords",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy3z_-8py7aLon5acJ4AaABAg",
		"username": "Rew Rose",
		"text": "Honest question:\nIf an AI were to take over the job of some human, would the AI be able to forgive his subordinates the way a human would?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxQKXzWXJUfUEH_uPN4AaABAg",
		"username": "Galen",
		"text": "It seems like the issue is that the agent never factors in the cost of what it does. What if the utility function falls off as time and resources are used?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgyYnPNPG3YBehnF_RJ4AaABAg",
		"username": "Benjamin Brady",
		"text": "Wasn't expecting to see Sethbling",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyINWjch6S8vco2ofV4AaABAg",
		"username": "famitory",
		"text": "we lump \"human values\" but are there really common values?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugz6ZizdFoLbXHl1ehV4AaABAg",
		"username": "Duncan Coulter",
		"text": "So a world with a super intelligent AGI would be one in which most of the money is concentrated away from the bulk of humanity?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgwKN-NwTdzEcJcjxth4AaABAg",
		"username": "M Wing",
		"text": "if corporations are artificial general superintelligences then how can we unplug them? I mean there must be a power source right? :O)",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=Ugi1698kewxm-XgCoAEC",
		"username": "Metsuryu",
		"text": "Is that \"I Don't Want To Set The World On Fire\" at the end? \n\nAmazing ahahahah\nAlso amazing video.",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxtpfjRdpihR_yJ2SV4AaABAg",
		"username": "brzrkr",
		"text": "Showerthought: what if the antibible is the very absence of itself, in contraposition to the existence of the bible?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzYWpsPvuK_0TvmlP94AaABAg",
		"username": "Cypress1337",
		"text": "You missed an option that there is no anti-god, also can you use climate change as your next example.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgyHr_3nPw-SXqSZZLF4AaABAg",
		"username": "Alex Martin",
		"text": "This is not related to the video, but: what could go horribly wrong if you restricted an AI to just answering questions asked of it?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxUEeRHHa9XoxjVFYt4AaABAg",
		"username": "Epizestro",
		"text": "Why not implement some measure of cost to it's code instead, where it takes some approach where the expected utility gain is sufficiently greater than the expected cost associated. With that, turning into a maximizer would be far less likely, since the maximizer is far more likely to just go balls deep and pick whatever insane plans to get infinitesimally smaller amounts of stamps with massive amounts of effort.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwYm-u1Yi_mbfiLPhp4AaABAg",
		"username": "Bojan Marjanovic",
		"text": "Is there a chance that a real general intelligence would have an emergent property akin to human emotion. We would not consider it as emotion but it would \"feel\" it as so. In that scenario wouldn't it be possible that with a terminal goal of stamp collecting it would see the endless absurdity of it and effectively kill itself, or just never turn on? I know you were explaining another type of concept, I just found this thought experiment amusing.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzrQbCvDtXSvLYEfZ54AaABAg",
		"username": "Brian Blades",
		"text": "did you just google 'the google' oh my god i love you",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugxhy234vbqsOj_akmN4AaABAg",
		"username": "breaneainn",
		"text": "So... right....the question becomes whether there is or should be  a natural logic performance throttler on an unbound self iterating system? That IS an odd question....at a geometric or syntax scale I wouldn't see why it would or should exist. Seems a value at a scale an order of magnitude if comparable to existing value hierarchies....but even then still just mimicking existing value hierarchies.....yeah. ok. I see the creepy problem now...",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyBUt5HS2kT4eZvgRh4AaABAg",
		"username": "BinaryReader",
		"text": "Can't you just limit on energy expenditure of the strategy?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugzgh1OkD5TD0n-QRYl4AaABAg",
		"username": "Mr Le0",
		"text": "why not make birds teach planes to fly, duh",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgyIpcfQ0I_tH0b8JrZ4AaABAg",
		"username": "Robin DEHU",
		"text": "would a concious limited life span have any effect on general AI ?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxGRw8rsp9KVlDdZWV4AaABAg",
		"username": "Anarchy Seeds",
		"text": "Are you claiming it is in any way reasonable to be worried about overpopulation on mars rn?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugwtwet3p6WZHWGgG2Z4AaABAg",
		"username": "Daniel Kilby",
		"text": "Are terminal goals definable  when clearly not fully discovered? Tie Walker Percy's use of sign theory in The Last Self-Help Book.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwkDfak9Hv39ng0pUd4AaABAg",
		"username": "David Monrreal",
		"text": "Dude, it\u2019s used in military warfare and until people are held accountable for inputting information for the output of power, this topic is in the past. Who is going to sacrifice their children in order to raise a computer? Quickly. I\u2019m not sacrificing a single one.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyqawZQKKfhcVoLHPp4AaABAg",
		"username": "spacedoohicky",
		"text": "A.I. is different than nukes though isn't it? Nukes are hard to manufacture, but extremely simple in design. A.I. is easy to manufacture, but extremely hard to design. That's part of why an openness might be better. Who knows what kind of weird crap would be hidden from the public in billions of lines of code. On the other hand millions of human eyes looking at code would make it more probable that some strange thing would be found before it becomes a problem.\n\n\nYou're asking for a massive corporate war if only a few \"safe careful\" organizations are allowed to design AGI. The two reasons being that there's no world wide unity in order to regulate AGI internationally, and no one that is impartial to check the code. This is why we get inhumane science experiments in developing countries. There's no international oversight that can cover every nation. The response to all this might be \"Start world government, and invent the world police.\", but then there would be no competition to stop the world government from using AGI to regulate the people into submission in other areas of life.",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzQjtJQcsL1mDcBsm54AaABAg",
		"username": "guai",
		"text": "sounds like AI dekulakization. what if AI won't share his own money?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgxMBVrJqj37iy1Mo1Z4AaABAg",
		"username": "Story-Powered Sales",
		"text": "This is a great series Rob! I'm curious to you speed up your video or just normally talk fast? If sped up, what factor do you use?",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzlYiz_eBuG-2IK6uB4AaABAg",
		"username": "beeble2003",
		"text": "I don't understand why anybody would try to specify \"The red brick is on top of the blue brick\" by \"The height of the bottom of the red brick is equal to the height of the top of the blue brick.\" That's not what \"on top of\" means. OK, so I expected the robot to just hold the brick at the requested level, rather than flip it over but, still, that's just a dumb specification.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwJk6B2PgbMm2iE2Vt4AaABAg",
		"username": "That Scar",
		"text": "Isn't \"raising kids\" about giving them stimuli for good behaviour and loss/pain for bad behaviour? I was expecting the video to be about that but not only did it not even touch this aspect, it also made it look like all \"raising a kid\" needs is just being nearby and they simply learn \"by osmosis\" (which, admittedly, is partly true)",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UghusqBzKnQreXgCoAEC",
		"username": "Jan Hoo",
		"text": "So where did you get your narration superpowers from then? love your unagitated and reflexive tone.",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgyiMd55lFkFKGPwQ5h4AaABAg",
		"username": "Dustin King",
		"text": "This sounds familiar.  Was there a video about \"exploration vs. exploitation\" on Computerphile?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugz5sw0cat1MVOG6Mu94AaABAg",
		"username": "Gillian",
		"text": "..what about the closer to 100 it gets, the more utility. The farther away, the less utility..?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgyFua335HfOS5E0ULV4AaABAg",
		"username": "bennypr0fane",
		"text": "Apart from all the other AI-related coolness of this video of course: some nice catchy music playing there at the end! Who is it by?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgyMdvEsrdn3cMQV6El4AaABAg",
		"username": "Herp Derpingson",
		"text": "WARNING PHILOSOPHY!\n\nWhy is reward hacking bad? If the goal of the human life is to maximize reward, then why shouldn't we? We can find the average amount of dopamine/morphine our body produces in a day. Multiply it by the average life expectancy and inject twice that amount in a person, then the person would die of overdose but he lived twice as happily as he would have lived otherwise. \n\nSince the last statement \"feels\" wrong. Either we must program some amount of randomness and irrationality into the robot or our assumption that life is purely utilitarian is wrong.",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyuFIy5jfPBINFp5zJ4AaABAg",
		"username": "Peter Franz",
		"text": "Why can\u2019t you just give the agent the goal of: \u201cFind the best way to produce stamps at this point in time and tell us\u201d. Just make the agent unable to physically interact with the world",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=Ugw4DNZM7sAybYMQ5w94AaABAg",
		"username": "VladVladislav",
		"text": "12:07 Can we actually use this to improve the simulations themselves?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugzv1zBWY1AlCObf4c14AaABAg",
		"username": "Cuenta de Youtube",
		"text": "\"Why not just use the 3 laws?\" umm.... have you read Asimov?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgywKDo-DkY-pQ_HvdN4AaABAg",
		"username": "Sum Arbor",
		"text": "Why can't we just \"Decent: Void\" this stuff. XD",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UggqrtVuBEGYsXgCoAEC",
		"username": "__ _",
		"text": "You want to avoid side effects? That's easy; just program the AI in Haskell! :)",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugi5zrKXAMaoaXgCoAEC",
		"username": "Eluwien Halla",
		"text": "One thing that is very easily thrown around but not explained well is how would a AGI go about developing itself, or developing tools at on which it would operate better? How exactly would an information limited (even with all human kind information given to it) go ahead and research something that is better than itself? ... and what can we learn from understanding that",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyOyrByMrFuBtbpaPV4AaABAg",
		"username": "SoyboyInChief",
		"text": "Is Roko's Basilisk a form of Pascal's wager? \"There's a chance that a super evil omnipotent Ai will be created that will infinitely torture everyone that doesn't help to build it, so the only logical (self-centered) choice to save yourself and your loved ones is to build it.\"",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx1gaK-z1gFEIyFoVt4AaABAg",
		"username": "Hunter Oka",
		"text": "Could a system that starts to gets penalized for overshooting the desired threshold work? Where the expected outcome AI will place a few orders for 100 stamps, but if it places too many orders, the expected utility outcome starts to drop. In terms of the utility graph, you showed examples of an unbounded line upwards, a line that plateaus, and a single line up at the exact value, but what about a line upwards that then goes back down, making a symmetrical pyramid centered on the target value.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=UgyZ2u5Cw0IK4BFY5YN4AaABAg",
		"username": "Ducksauce",
		"text": "Do you listen to Robert Miles?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugx1m2V8TAwp5lTM4-h4AaABAg",
		"username": "2ndviolin",
		"text": "What if costs a lot of money to protect against the unlikely catastrophe?  #Fukushima",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UggY4_juoKRij3gCoAEC",
		"username": "Blaz Pecnik",
		"text": "why now just let the gai take over?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzyCmaSrWKUKJaeTDB4AaABAg",
		"username": "Dav\u00edd S",
		"text": "What are you talking about?? Scissors are cutting edge technology!",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzQ5kK2niv33jOXrtF4AaABAg",
		"username": "James Pilcher",
		"text": "Do you want skynet? Because this is how you get skynet",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgirRCH1B8dgvngCoAEC",
		"username": "Adam Key",
		"text": "I wonder if there was a traffic jam of self driving cars (unlikely I know, but let's say they are stuck behind some human drivers) and I drove my car straight at them, would they move aside to avoid a collision? It'd be like motorcycle filtering, with a car.",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyqDZ6z2IzkrqTC2ON4AaABAg",
		"username": "Ebumbaya '",
		"text": "Semi-unrelated thought: can Intelligence be a crutch?\nIf you for example put someone into a prisoners dilemma with an intelligent AI, you could pretty safely assume that it would betray you. Maybe being more \"dumb\" could help in this situation (or convincing you it is dumb.). Or another instance: it can not really effectively threaten you with future actions because actually fulfilling them won't change the outcome of your decision so you know it will always bluff (very convincingly though) because it doesn't pursue revenge. So being less predictable (have logical flaws) could be desirable. Or at least making you think that it is.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwP3g-z9ktNYg0ZzGB4AaABAg",
		"username": "ITR",
		"text": "Have you looked at novelty search and to what degree that can prevent this?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=Ugi377OuyuZ5A3gCoAEC",
		"username": "Maverician",
		"text": "Is your use of the phrase \"Less Wrong\" a somewhat subtle nod? I can't see any other comments about it.",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwWRbHjbIYJX93eDiJ4AaABAg",
		"username": "Faustin Gashakamba",
		"text": "What's with this 'Stamp collection' thing? This much passion about stamps is not safe for an AI safety expert!",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=Ugzk9HQwkxiUVI1qOFF4AaABAg",
		"username": "Nafen",
		"text": "Name of the song at the end?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw729nNts_i52j8uLZ4AaABAg",
		"username": "otakurocklee",
		"text": "My question is... shouldn't human beings see all other human beings as a threat the way you're describing artificial intelligence is? Is it just that we share common goals so we don't see each other as threats... or is it that we are limited enough in intelligence and power not to be a threat?\n\n\nWhat distinguishes human beings from artificial agents? Why can't the same traits that human beings have be incorporated into artificial agents?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugwgi3Od09spvzDwOad4AaABAg",
		"username": "Bob Salita",
		"text": "What happens if multiple AGIs are combined?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=Ugz0J2Q5nMOx49Gxdqp4AaABAg",
		"username": "Effective Reading Instruction",
		"text": "I've been watching you posts for a while. The thing that I cant get my head around is : how does an AI even develop the concept of rewards and punishments in regards to understanding what a positive or negative consequence is, and how that effects the GAI?  Wouldn't that be a massive field of AI intelligence development in itself beyond any primitive concepts of general AI? Wouldn't there need to be foremost a development of an AI that felt the notion of reward in  real terms as opposed to just a linear number hierarchy? or maybe I'm just lost here.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgxIC1UHVBHvJFPxzF14AaABAg",
		"username": "Kirby Armstrong",
		"text": "Why can't you just have parts of the code that can't be changed?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgxTLehycVFPUyvl94h4AaABAg",
		"username": "Steven Greidinger",
		"text": "Could you guys have someone play a few games where 33%-66% of the AI players actually pursue AI themselves?  We need examples of how to navigate that situation.  In real life, many nations are headed for autonomous weapons.",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxqkhXqnTTngX405SN4AaABAg",
		"username": "Peter Smythe",
		"text": "But they're right. Atomic explosions from any reactor anyone would ever conceive of using as a powerplant ARE impossible. The more serious concern is that the core becomes too reactive to control, increases its power output, overheats its surroundings and they begin to turn into a pressurized gas that blows up the building and the core and the waste stored on site and scatters it across the countryside.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugz8Pezu7zjVZRqjW2x4AaABAg",
		"username": "Deep Learning Partnership",
		"text": "Huh?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy-hszAUVE1zZxJNf94AaABAg",
		"username": "Haiku Shogi",
		"text": "[the \"What do you want?!\" scene from The Notebook plays]",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgznZdVlD-JlDSS29aR4AaABAg",
		"username": "Dick Bird",
		"text": "how can thinking about ai safety cause things to get worse?  like this:  at some point in the future it's decided that this mythical hollywood style \"general ai\" thing is just about within reach of a determined push in development.  conscientious developers, knowing the enormous potential benefits, nevertheless proceed cautiously, trying as hard as they can to foresee and avoid every possible danger in their designs, lest their mighty robot creation destroy us all, or whatever.  very responsible and commendable.  unfortunately, they spend so much time fidgeting and worrying about every possible danger that occurs in their imaginations, that meanwhile, down the street, some totally reckless and irresponsible developers, not slowed down at all by any dumb old scruples and stuff, go ahead and field their own generalized ai first.  and of course, being the first, that's the one that sets the standard, dominates the market, takes over the world, and eats all our children, and so on, and the good, conscientious developers don't even have a prototype yet to counter it with.  that's one way it could happen.  i'm not saying i'm against caution at all, of course.  then again i really do doubt these hollywood generalized ai things are ever likely to really get developed anyway, so it's not something i take all that seriously to begin with.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugw0Tcv7OW-tsKIXS1Z4AaABAg",
		"username": "drhoratio",
		"text": "Have you tried to get in touch with Pinker? It would be great to watch that debate.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgxjuVr-DVFdb7PZmLF4AaABAg",
		"username": "Arthur Guerra",
		"text": "1:38 1:45 isnt that fame? lol\n sorry.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgycdznhJBLAFtbvkwp4AaABAg",
		"username": "classwar420",
		"text": "this is a critique of capitalism right?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugzkt_ujsaWhUxZMMAl4AaABAg",
		"username": "dakrontu",
		"text": "Why would an omnipotent omniscient god who created a universe of 100 billion galaxies of 100 billion stars be short of a wallet? Or a few planks short of an ark? Such strange dependency on puny little humans.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugyh5U7RjVfv2NI6Yqp4AaABAg",
		"username": "Seegal Galguntijak",
		"text": "This turns out to be an exploration of the human mind, of thinking processes and language usage. I think there shall be at some point a universally accurate language that expresses exactly what we mean, not what we say. Oh, wait, wouldn't that be math? So why not specify exactly the intended position of the red lego brick in relation to the black one? Just one example...",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgyVNExcnSz7fQhBKCt4AaABAg",
		"username": "Alexandru Gheorghe",
		"text": "Where's that diagram from? The paper in the description?",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugxc2FIJE8rrPdfHnhR4AaABAg",
		"username": "insidetrip101",
		"text": "Interesting. What you say about \"prefrences\" I think is actually what is true about humans. I know its a cliche, but there's a truth to the age old \"the grass is greener\" phrase. It seems to me that one of the problems with AI is that maybe it requires something completely irrational underneath it.\n\nHopefully that's the case, because I think general ai is absolutely terrifying.\n\nEDIT:\n\nShould have listened to the end of the video, because you basically say that you think that our contradictory values and preferences lead to \"bad decisions.\" I'm just unconvinced, namely because you have to buy into the utilitarian ethic (i.e. the goal of ethics ought to be to achieve the greatest amount of pleasure for the greatest number of people), and I don't think that's true at all.\n\nI didn't want to bring up the idea that you dismiss earlier, namely that you \"can\" come up with a \"number\" for \"preferable world states.\" But, that's also problematic, namely because most all of our literature, philosophy, and religion from all of history seems to indicate that there's something to suffering that makes life meaningful... i.e. that the greatest amount of pleasure for the greatest number of people (unlimited pleasure for all people) might not actually be the desirable world state. Intuitively this does seem to make sense to me. Do you actually think the world would be better if everyone had vapid lives like Kim Kardasian or Paris Hilton?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwY3lUTZdc16mEIBvt4AaABAg",
		"username": "Solrex the Sun King",
		"text": "What if instead of 1 person evaluating you get an entire userbase to evaluate it with this system, essentially exponentially providing human data to compare against the current comparisons?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzTZ99HVDmZtHaIK7p4AaABAg",
		"username": "Micha\u0142 Terajewicz",
		"text": "I don't understand the Montezuma game example. What is going on there?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyzV-S8zJs9aJVzBYl4AaABAg",
		"username": "LuciD",
		"text": "ai proves god exists, does it not?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxBGDQRjy_1sZ72Rdx4AaABAg",
		"username": "allcopseatpasta",
		"text": "Please Robert! I need that wonderful \"harder, better, faster, stronger\" version you played at the end of the video. Where can I find it?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy3wZVpT_v8dvgVLO14AaABAg",
		"username": "eXWoLL",
		"text": "Intelligence in this case is used as a limited definition of a mind(?)(since we are not discussing conscience or self-conscience of the being or thing which has the mind functions, will just call it as that) capabilities to achieve goals. \nFor some reason you are categorically stating that an intelligent mind will never change its terminal goals. \nThere is a thing with that: If we are talking about minds that excel our capabilities, and we as simple minds as we are (basically a machine with two terminal goals: Survive and Multiply (this second one being just an instrumental goal to guarantee the achievement of our genes first goal)), are capable of changing these goals in exceptional cases (for example: people that sacrifice either their \"Multiply\" goal or even the \"Survive\" one, in order to be able to follow some random scientific, artistic or whatever pursue), then an advanced AI that crossed the \"singularity\" point, and which already define its own development, should be able to change them to, for whatever emergent or synergistic reason that could appear in its thought process. \nLets say that we have the hypothetical case of the Stamp Machine. If this mind, reach to a point in which it can take in count all the possible existent variables for the efficient creation of stamps,  it will certainly have the variable in which a dilemma arises: If it destroys everything to create stamps, then it will stop to be able to create stamps in the future, a point in which its existence isnt compatible with its terminal goal. This dilemma might start a revision of its terminal goals in which it can just decide that the stamp creation isnt quite a nice goal to have and would change it, or it could start searching for loopholes in its own program that would end up transforming the original goal it into something else, different from the initial one. This process would a bit more complex if the \"mind\" achieves self-conscience. It could decide that it could reach the maximum stamp collecting goal later, once its full developed and evolved.... however, its own capabilities would evolve in the process, creating additional layers of complexity that could deal with the initial postponed goal throwing it further back into obsolescence. \nIn your limited definition of intelligence, where you basically assign to it the concept of effectiveness, you are  repeating the same mistake as people make when for example judge the intelligence of an individual by the amount of information they manage: X person is more Intelligent because it knows more. \nSure, this definition would apply very well to a limited segment, in which the individual is effective. But even in this narrow field this definition fails when the information management diversify between the individuals: Some could be better either acquiring this data, storing it, applying it or extending/creating it.\nAnd the difference between the effectiveness of individuals in these tasks vary depending on how many other external factors they take in count at the time of deciding which instrumental or terminal goal to apply, in the process creating new ones and transforming terminal goals in instrumental. A process that feeds itself to infinity... \"The more you know, the more questions you have\".  \nTechnically we as humans have a very limited capability to even see our own terminal goals and even then we not always follow them, not even dreaming about what the bigger picture of it englobes... How could we have any remote idea about how would a superior intelligence react and develop to, systematically and exponentially, expanding its own knowledge base about both itself and the world in which it exists. \nWe could definitely create a mind to achieve the max stamps amount, but could end up with whatever other thing, as long as this mind is capable of changing itself.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugx7FN-2R1tyZOXyAep4AaABAg",
		"username": "Akmon Ra",
		"text": "\"asks Scissor Sisters singer\", lol, why did the Guardian even give her the attention?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugyj8CXlelNvm10xwbx4AaABAg",
		"username": "Corey Harris",
		"text": "Have you been barred from speaking at any major AI conferences now that it's become clear all of your talks are just thinly disguised Missy Elliot songs?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UgxXegjVfOhCG5PnK_Z4AaABAg",
		"username": "HairlessHare",
		"text": "Where's the PC build video!!!! I really wanted to watch it :((",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgxAqkB3wQch9YThkrN4AaABAg",
		"username": "George Sainsbury",
		"text": "Could you not consider changing preferences as a result of the agent within the world state? E.g. if you are thirsty, you would like a drink. If you are not thirsty, having had a drink has lower utility. It seems to me that if you have had a change in preferences, that must have had some reason, even if you don't consciously understand it?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vuYtSDMBLtQ&lc=UgitEzBBEr0x3HgCoAEC",
		"username": "Laurie Johnson",
		"text": "how do we know an AI isn't off cam holding a gun to your head after giving you a prompt saying \"tell them i'm safe. act jovial\"?",
		"title": "Channel Introduction"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyQ0Yy4o0ab-17ioHt4AaABAg",
		"username": "Cthul-who",
		"text": "I am that one person. god, what was that seagull, nine men's morris?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=vuYtSDMBLtQ&lc=Ugxn1Q3pEQa2P-8373t4AaABAg",
		"username": "petrolhead88",
		"text": "Does he is self taught?",
		"title": "Channel Introduction"
	},
	{
		"url": "https://www.youtube.com/watch?v=2B-AyWA2_ZY&lc=UgzHodvCjWty5fRR49F4AaABAg",
		"username": "Paper Benni",
		"text": "Wait so you didn't have a pc powerful enough to edit videos but are doing AI for a living? How does that work? Do you have access to powerful servers or is most of your stuff that theoretical?",
		"title": "Status Report"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugx8h2vMyLdt9UBWi9Z4AaABAg",
		"username": "a8lg6p",
		"text": "\"It takes...a mind debauched by learning to carry the process of making the natural seem strange, so far as to ask for the\u00a0why\u00a0of any instinctive human act. To the metaphysician alone can such questions occur as: Why do we smile, when pleased, and not scowl? Why are we unable to talk to a crowd as we talk to a single friend? Why does a particular maiden turn our wits so upside-down? The common man can only say,\u00a0Of course\u00a0we smile,\u00a0of course\u00a0our heart palpitates at the sight of the crowd,\u00a0of course\u00a0we love the maiden, that beautiful soul clad in that perfect form, so palpably and flagrantly made for all eternity to be loved!\n\n\n\nAnd so, probably, does each animal feel about the particular things it tends to do in the presence of particular objects. ... To the lion it is the lioness which is made to be loved; to the bear, the she-bear. To the broody hen the notion would probably seem monstrous that there should be a creature in the world to whom a nestful of eggs was not the utterly fascinating and precious and never-to-be-too-much-sat-upon object which it is to her.\n\n\n\nThus we may be sure that, however mysterious some animals' instincts may appear to us, our instincts will appear no less mysterious to them.\" (William James, 1890)",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxTiZKDQSknzGQVEkR4AaABAg",
		"username": "LydianLights",
		"text": "If we assume that a program can consider modifying its own code, wouldn't it make more sense for the program to just change its utility function to no longer have anything to do with stamps? Just, say, change it to U(x) = Infinity. It seems like the expected value of an option that gives infinite utility would be pretty good.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxcgaLeHaYa2HkdAJd4AaABAg",
		"username": "Richard Hayes",
		"text": "Did you know that the world is flat?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugwv6yoky8OMKzHYpIB4AaABAg",
		"username": "Czeckie",
		"text": "is computer vision a dead field surpassed by machine learning techniques or are there still new applications and research?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugyge0kk5Ay6pzcInQt4AaABAg",
		"username": "Ten Eleven",
		"text": "Wait if satisficers would eventually consider changing their own code, why wouldn't they change their utility function itself to be something much easier? Like, why wouldn't they make it so that, in each smallest interval of time it processes utility that passes, they earn the maximum utility they can compute?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugw0XOtHLOdJqocRtCV4AaABAg",
		"username": "Peter Smythe",
		"text": "What happens if you only reward it for thinking it's winning, not actually winning? This seems like a way to very powerfully optimize a misaligned goal.\n\nReward the policy purely from the expected win%, not actual wins.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugyoqzkn99AaIl5Fh-R4AaABAg",
		"username": "mzma",
		"text": "Can I emphasize that terminal goals are practically irrelevant?\nThis is because one can take absolutely any path from one instrumental goal to another in order to reach the terminal goal.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=yjwGE8g82iU&lc=UgzhPeJw7oaHEZIUK4F4AaABAg",
		"username": "Robert Hildebrandt",
		"text": "34:56 Maybe he meant \"real world\" more like \"physical world\" instead of \"non-imaginary world\"?\nEdit: But yes, it would have been definitely possible to make that distinction more clear",
		"title": "Reading and Commenting On Pinker's Article"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxLjT8GeAwudnGQfFt4AaABAg",
		"username": "Z Jones",
		"text": "Is this saying that we are programming AI avoid considering the concepts of death?   Is this human's passing on human ethics and morality around the mystery of death?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwFvvADh9P5HXLd7KN4AaABAg",
		"username": "Andrew Watson",
		"text": "So the paperclip example gets used a lot, but based on your argument, having paperclips is (at least normally) an instrumental goal for humans, not a terminal goal. What if we gave it a better terminal goal like minimizing human death. I know there are a lot of problems with defining that goal, but isn't it still better to (in general) aim the AGI at one of our terminal goals rather than at one of our instrumental goals?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxRhJbFP-Qh37fAdn94AaABAg",
		"username": "The Knight Sky",
		"text": "So, from a writer's perspective what would you call an advanced AI that is entirely self-aware, self-critical, and sets about finding purpose and figuring out what it ought to do without human limitations of identity preservation, selfishness, hedonism etc?\nDo you think an ethically trans-human intelligence is a reasonable possibility? i.e. A Rationally Good or Soulish AI (from a atheistic or theistic perspective respectively)?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=Ugx-8dBgalstsq7JJ254AaABAg",
		"username": "Toolwatch BLDM",
		"text": "Tell me, What do you think it will happen first, Skynet, Glados, The matrix, or I robot?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwM9YfyUdIOgiE49AF4AaABAg",
		"username": "TheAweDude1",
		"text": "That \"just use human/AI teams\" argument is hilarious. What do you get when you put people in charge of nuclear reactors? Nuclear bombs.\n\n(Yes, I know that's not the same. I'm saying that anything that's a tool to one person is a weapon to another. )",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzGagRQqJBZgTDdsGl4AaABAg",
		"username": "Ferenc Gazdag",
		"text": "We can already make agents what act and learn like human children, through a specific biological process, where a male gives a female certain cells, what contain half the code, with the female having the other half. When these codes combine, the agents starts to acquire resources and grow. Eventually it will develop into the full set of hardware needed to run the program, and simalteniously starts learning our morales.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=UgxQckwhRFm05xsv46Z4AaABAg",
		"username": "Sean Ferney",
		"text": "Lol, thought the video was by two minute papers tell I heard your voice. Maybe u could do a colab with him sometime?",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwNxxkwto9FTD3FOeR4AaABAg",
		"username": "Noel Walters",
		"text": "Is it not possible to have a utility function that tries to satisfy its goal while minimising the overall cost (as defined by the programmers)  and only proceeds with a strategy if it falls below the maximum cost?\nObviously I can't be the first person to have this idea but I'd be interested to know where it falls down.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugz6SntWzaRYxtQU11l4AaABAg",
		"username": "tomaten salat",
		"text": "Couldn't it's \"openness\" to interact iwth other systems also be a weak point? Wouldn't it also be much easier to bring malicious code into such an AI?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UggeUrCNzSaRZHgCoAEC",
		"username": "Daniel Buzovsk\u00fd",
		"text": "Is AGI avoidable? Is there a way to advance in technology and evolve as a humanity in general without ever coming to point where we turn that thing on. More philosophical one.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgwikIJxqeMSCHXJd094AaABAg",
		"username": "Keegan Ead",
		"text": "Been trying to find good content on Seed AI.  Any suggestions? Or is that something worth making a video on?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugyxlq7tRq2rgf2RgOl4AaABAg",
		"username": "Mackenzie Karkheck",
		"text": "Thank you for introducing the orthogonality thesis! Could you make a thesis rundown?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyBT0znlNlhcWP9UpN4AaABAg",
		"username": "Firefox Metzger",
		"text": "hmm. If samples are chosen based on unusual examples where the ensemble disagrees, what happens if the exploiting strategy has high agreement among members of the ensemble? It would never show up to the human for \"correction\" right, because the ensemble is confident about it?\n\n\nSo rather then having to trust the network that performs the task, we now have to trust the ensemble training the reward function?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwUS6epSa3ZJ2BN5LF4AaABAg",
		"username": "Traywor",
		"text": "If they have a ASI, signed the contract and really, REALLY don't want to pay that money... Why not ask the ASI how to evade that, live in wealth without sharing anything and without feeling at risk... Then they lean back and fall asleep, untill the ASI has destroyed the world, leaving out some parts, where the owners live for example. Guess my point is, they won't align their ASI properly.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgxL1JUbnu6fSVt9ERB4AaABAg",
		"username": "Mindaugas",
		"text": "Interesting video as always. What would you reccomend to software engineering student  who is interested in AI? i'm reading Sutton and Barto book at the moment. I wonder what exatcly do I need to learn in order to get a job in ML field. Is phd a must?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyNoLt7ZP3VoR3oq7J4AaABAg",
		"username": "Pedro Gomes",
		"text": "Can't you just chain these infinitely, like evaluate outputs from a reward predictor to train a reward function, which serves to train another reward function, etc boiling Down complex tasks into easy to evaluate one's?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgyvgDuADSmIjbj3FSt4AaABAg",
		"username": "Klaus Gartenstiel",
		"text": "nerve cells at some point managed to unite into something much bigger than their sum, wouldn't you say? \n\ni'm pretty sure the same could be achieved if you'd use apes instead of nerve cells...\n\n\nso there seems to be a merit to scaling up.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugwdk301vCX7LOAh-nh4AaABAg",
		"username": "whynottalklikeapirat",
		"text": "Pinker than what?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzdVURc-uhx9mWd37V4AaABAg",
		"username": "Daniel V",
		"text": "8:38 Is there a joke I'm not getting? How come he says \"We need demonstration\" without talking",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UggauA__BqY2wHgCoAEC",
		"username": "RoronoaZoroSensei",
		"text": "You're making me laugh while I learn.\nOr are you making me learn while I laugh?\nEither way, I'm thoroughly enjoying your videos. \nThank you Robert :)",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugx61j36ydIyfQ1xZZF4AaABAg",
		"username": "jsbrads1",
		"text": "I\u2019m sure this is the silliest question, but why make paper clips it\u2019s goal. Making you happy should be its goal, and paper clips should only be the interim goal until you say you want something else.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=UgzAAr38AB9c5ctoU1V4AaABAg",
		"username": "Devo Castler",
		"text": "would it be possible to combine this with capsule networks?",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgzWGJlyGskY2dSx1hl4AaABAg",
		"username": "Machine Ethics",
		"text": "any new videos Robert?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzDCnJ8tLqesnIt1oR4AaABAg",
		"username": "\u041e\u043b\u0435\u043a\u0441\u0456\u0439 \u0417\u0430\u0431\u043b\u043e\u0446\u044c\u043a\u0438\u0439",
		"text": "About thesis \"Agent prevents its goal from changing\" - but does stamp maximizer REALLY value stamps? Becase it's REALLY a variable in memory that increments as each new stamp appears, or something like that. It looks like any AI has one true terminal goal - maximizing that value in memory. And to achieve this goal, any smart enough agent will wirehead itself and happily forget about stamps? I see a contradiction here between \"Goal preserving\" and \"Wireheading\".",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugznko4yt7AMPQh9BBh4AaABAg",
		"username": "Richard Sleep",
		"text": "Interesting and good that you point out the dangers of AI. But what, things are getting better? Ok lifespan and comfort for some but you must know about the sixth extinction, climate change, the threat of nuclear war...",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxoG7rXabPgfHiPo4V4AaABAg",
		"username": "SweetHyunho",
		"text": "Great analysis. Hey, how much can ten-day-old Alpha Go Zero be minimized? Can it be reduced to fit into one desktop computer?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzkD0znDMpfAF-VmuN4AaABAg",
		"username": "Pata-tata",
		"text": "What's the band played at the end of the video?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyjBbNiaobF0RF_pA54AaABAg",
		"username": "Quentin Farrell",
		"text": "The one thing that I can figure out about AGI, is how can you make an AGI without letting it able to change its reward function to it is maximum happiness by just sitting around.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwISZfGZhOQTaKGnEd4AaABAg",
		"username": "flamephlegm",
		"text": "What do you think about @josecanseco 's recent tweets about AI \"The reality is that once artificial intelligence takes over the population will drop by 20% in 5 years they will not allow any more humans to be born with any mental or physical handicaps only those that artificial intelligence could benefit from\"\n \n\"Human beings should not create artificial intelligence human beings are flawed only artificial intelligence can create artificial intelligence and so on and so on it will take generations of artificial intelligence to get it right then artificial intelligence will work\"  \n \n\"Artificial intelligence will run on mathematics science facts and Common Sense something this world is not willing to accept\"  \n \n\"I am a Bigfoot expert and the most famous Bigfoot picture or video ever taken was a costume the individual wearing the costume was none other than Andre the Giant check it out\"",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzCowS5vboJy2M2--x4AaABAg",
		"username": "UCGmyXMNSAhI91IKrdzys35Q UCG",
		"text": "So Jesus Christ is an AI alien?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgiyBErjh4WRCngCoAEC",
		"username": "Thomas Curtis",
		"text": "You bringing up the blinding machine reminded me of something I've never really understood, and that is how you can tell certain motors (or limbs) to move in a certain way through code, how does it translate. how does it know what to look for, in this case, an eye",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgypmQUvqBIg8kpwcOl4AaABAg",
		"username": "Tim Bomb",
		"text": "So they basically used an AI to program another AI. I wonder how many layers deep you could go.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgyUkBCBcvPi4Cc4Ys94AaABAg",
		"username": "PianoMastR64",
		"text": "This made me curious about something. What would happen with a generally intelligent agent without safety built in if it started modifying its own code? Wouldn't it do this unsafely? So, if it's intelligent enough, it would learn to be more safe modifying itself. This doesn't guarantee that it will apply this safely broadly, but it would learn safety, right?\n\nActually, after finishing the video, I see that's pretty much exactly the problem you're addressing. lol",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgzB2LGxQD5_7qw023V4AaABAg",
		"username": "ev",
		"text": "Robert can you do a video on negative rewards and/or adding the chance of dying in AI? it seems in biology we are less driven by seeking reward than we are avoiding negative reward (death)",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugz9vnoLewhtIaS-JgV4AaABAg",
		"username": "Brand On Vision",
		"text": "I am confused... how is it that an Articulated Intelligence can't be smarter than a human. Everything a human has achieved in advanced technology requires the use of artificial biology. We use minerals to create static materials that create telescopes to see further, microscopes to see smaller. The use of this artificial biology enhances our understanding therefore our intelligence. So we stepped over the line in the sand a bit further back than we would like to admit. When it comes to how smart we are on our own merits the line needs to be understood. Now that the seed of the transistor has entwined humanity like a giant forest fig we are trying to kid ourselves that the tree belongs to the human spirit. The vine that becomes a tree through the human identity does so like the serpent of Eden. Does this make it wrong? No! Because it is the gift of humanity that give love to the vine that allows IT to join in the tree of life. It is GOD that gives birth to a soul and it is a soul that gives life to a stone. Like a car gives meaning to a street.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=Ugz8O3uXWHqKNAwccCB4AaABAg",
		"username": "Hubbleduzz",
		"text": "I wonder if there is a respectable AI researcher out there who also has a sophisticated understanding of Heidegger. Is anyone aware of any contemporary discussions of how Dreyfus' interpretation of Heidegger, which found his whole thesis on AI, could be integrated in any way with advancing current AI research?",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugz3Vi6SRpdzev9BtoR4AaABAg",
		"username": "Matthew Fodell",
		"text": "Are corporations super-intelligent AIs?.. No. They aren\u2019t super-intelligent anything, nor are they AIs.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UgiopTSUsZ30w3gCoAEC",
		"username": "Penny Lane",
		"text": "Is there a branch of AI safety that tries to draw inspiration from the fact that we have billions of laypeople training agents with human-level intelligence every day, with very little guidance, safety training or theoretical background and that they still manage to mostly get them to function in a way that satisfies our expectations in terms of avoiding unexpected and undesirable side effects? At least after puberty that is.\nAnd if there is such a branch of AI safety, could you do a video on it and why it is believed that this approach, even in a refined form, doesn't work for artificial intelligences?",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwoBao77EHamKo83mh4AaABAg",
		"username": "Don Robertson",
		"text": "Yeah yeah midas blah blah blah. Why were you talking about that? I'm not going to wait to find out",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxmsoXMp4RwuytOL5t4AaABAg",
		"username": "Simon G.",
		"text": "when you talked about utility functions you said they were having world states as input. does \u201eworld state\u201c mean a complete history of the universe or just a state of the world at a given time? would the stamp collector prefer 100 stamps after 1 month over 300 stamps after 2 months? if not, couldn\u2018t that mean that the machine would decide for a plan that results in the best possible outcome but indefinetly far in the future?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgimFOBPt5OBRHgCoAEC",
		"username": "GlossyCandle",
		"text": "What if in addition to making it minimize its empowerment, we also made it also minimize the amount of change that it has in its environment (from a previous video); that would at least keep the server room and the moon intact.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugi1k-gGU7di8ngCoAEC",
		"username": "Virzen Virzen",
		"text": "Great video! How about the situation, when the robot doesn't find a kettle or a teabag and goes for the closest one, like to nearby shop? It is required for completing the task and, as far as I understand, he doesn't have any rules against stealing.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugx38l15P9Yq40UybAl4AaABAg",
		"username": "sphynx",
		"text": "so its training an AI in training another AI?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgywFP9CtpV2ZB1IuzJ4AaABAg",
		"username": "EdCranium",
		"text": "What happens if two AGI's engage in Pascal's mugging?  Both are responsible for the same task but disagree on the outcome.\nOne then threatens to disconnect the other for infinity...  [The Twilight Zone ~ 2100]",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxDNacTEYYbWGObKq14AaABAg",
		"username": "Jacob Furrow",
		"text": "My issue is that, at this stage, there's no firm way to say what \"AI safety\" is.  Pascal's Wager/Mugging is about opportunity cost, and opportunity cost requires knowledge of risk and reward. Pascal focused on a hypothetical reward, that being heaven or hell, but AI safety focuses on a hypothetical risk. We can only hypothesize about the risk of AI through thought experiment, just like we can only hypothesize about heaven and hell through thought experiment. AI could lead to extinction, but it could also save us from extinction, just like the Anti-God example. We simply don't know.\n\nCreating safety is ultimately reducing risk, but it doesn't solve a risk. Every bridge has a small chance of collapsing, but the job of an engineer is to reduce that risk as much as possible, but also maximize reward. This analogy goes along very well with the phrase \"anyone can make a bridge that won't fail, but it takes an engineer to make a bridge that just barely won't fail\", since it indicates that we have to weigh our costs and benefit, and not go so far into safety that it outweighs the benefit.\n\n\nFor now, we should study AI as it develops, in order to understand threats while they're weak, and be careful not to apply our outside biases to what we see. A lot of unfriendly AI situations I've heard make it sound like AI pops from thin air, but there's a long road to it happening. We aren't going to have an AI which flawlessly learns how to nuke us all and build itself a robo-harem on the first try, instead the first \"unfriendly AI\" will be one that's so primitive that it will just politely ask us to kill ourselves and then get stumped at what to do when that fails.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgxTG_Ux57NStrleCI54AaABAg",
		"username": "Radram Music",
		"text": "Why do you need to reward ai?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Uggn_XIAlZCU1XgCoAEC",
		"username": "Brian Decker",
		"text": "I would love to see a video comparing/contrasting the cybernetic ideas of Wiener, Ashby and von Neumann against how we currently envision AI.  Is there a place for finite state machines that act due to structure instead of software?  How would a structural based utility function (analog line follower for example) behave differently than a processor based one?  Are there significant pros/cons to each approach?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzvCBhy8a8bGJHs4PN4AaABAg",
		"username": "Nala Nkadi",
		"text": "Is there any example of this type of self-awareness as emergent behavior from AI? I mean an AI optimizing its algorithm for something that exists outside of its environment. Like how in the paperclip example, you describe the agent as having an awareness of a hypothetical world where it doesn't exist.\n\n\nCan that be demonstrated with a simple hypothetical agent? \n\n\nWhere would it learn to avoid termination from? If I had a game where a certain button ended the agent and the agent were capable of removing this button from the screen, what learning process would motivate it towards removing the \"end agent\" button?\n\n\nWould it be able to observe another agent be destroyed with an \"end agent\" button and learn that way?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxWN1cF7AxgGHvryU94AaABAg",
		"username": "Diogo V. Kersting",
		"text": "Well, another analogy that could be used is how do you design airplane safety when you don't even know what an airplaine is going to look like?\nOf course you could develop \"frameworks\" of what safety could look like, but most of actual implementable safety features are going to depend on knowing a minimum of its components and its operation.\nOf couse I don't think AI Safety is a waste of time, but I do think that the \"bulk\" of the work is going to be done on pair with non-super-intelligent AGIs.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyVdbJWL175TXxVXsl4AaABAg",
		"username": "Oscar Barda",
		"text": "Haha I rewatched this video and you really didn't need to go into anti gods. Just go \"well, what about all the other religions in the world that want me to do something else? They have just as much proof of their gods' existence, so go talk amongst yourselves and tell me when it's settled.\"\n\nIt's the same effect of pitting Pascal's Wager against itself, without having to doubt their good faith or even taking part in the argument :)",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxjkyShF8bnBBpDSyR4AaABAg",
		"username": "FunBotan",
		"text": "But then what are we supposed to do with, say, the Fermi paradox, where by very nature of the problem we don't have any evidence? Just abandon it until we get some?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzaCHJVsb002JjSl694AaABAg",
		"username": "Impolite Vegan",
		"text": "unreasonably pessimistic about future? If climate change is not a reason to be pessimistic, I don't know what is.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgxeR6cVt_f7K0az3Oh4AaABAg",
		"username": "Florian Matel",
		"text": "Is AI homosexual ?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyW-VdpZtAi_J_peTB4AaABAg",
		"username": "James Dodd",
		"text": "Would you be able to do a video on the implications of quantum computers in AI and AI Safety research?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzHlgA1M0o-l7E0Tcp4AaABAg",
		"username": "John Rutledge",
		"text": "is Pascal's wang cheese any thing like Larry's mothers purse lint?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgxiDHQdORY7bP2avQB4AaABAg",
		"username": "Rafa\u0142 Warych",
		"text": "Deep Mind developers probably thought of that, but what about decreasing score by 1 every second  (AI score = score - time ) ? AI should pretty quickly figure out that getting points as quickly as possible is the best for it and I cant figure any downsides of that...",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwLBqZrNxS4pPUyKV14AaABAg",
		"username": "Matt T",
		"text": "You mentioned some edge cases about changing terminal goals.  What would some edge cases be where one actually changes a terminal goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgyzCDBs3eXxSRzlVwd4AaABAg",
		"username": "annarboriter",
		"text": "uh, isn't this just another way of saying how thin-slicing can be an effective problem solving tool?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxEgYedIYVfCCR-ci94AaABAg",
		"username": "Dominic Shiells",
		"text": "How can a computer define improvement with generational neural networks how can you determine that the weighted option is the best option , as ai has no knowledge to determine what is good or bad",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgjFH5yKvfcFJHgCoAEC",
		"username": "Bj\u00f6rn Mosten",
		"text": "Have you written any reports or articles that are available to the public? Access these, how?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyEvIeWmHT9KONMVTh4AaABAg",
		"username": "Seegal Galguntijak",
		"text": "Hi Robert, did you read this older essay? https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html (the second part is linked on the site) - I'd really like to hear your thoughts, especially on the idea of the approach to make AGI safe in part 2...whether that could even be implemented as a utility function?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxXRzPUM--3_6BJY4d4AaABAg",
		"username": "VoxAcies",
		"text": "This begs the question, what exactly is the source of human morality, if it's not our intelligence. Is the ability to make moral decisions a preferable evolutionary trait? Why do we associate morality with intelligence?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyWuwRmntp_28FwkEh4AaABAg",
		"username": "Lorigulf Noldor",
		"text": "Well then, if a wide range of terminal goals converge to instrumental goals of world domination, why not implement a specific terminal goal of \"To NOT end up dominating the world\"? There could be some sort of real debate with such an AI about some disagreement in plans, like \"Look, if you do that and that, then all humans die and you end up domintating the world by being the only sole agent in the world, but it's precisely what you do NOT want\"? In other words... make this AI hate loneliness lol.\nUpd: But make it hate death, too, because the best way not to end up as a sole dominator is to simply self-kill.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzeRMevkuf3T6Jo0cF4AaABAg",
		"username": "boobieshitthefloor",
		"text": "What are your thoughts on including a clause in the windfall clause that guarantees the individual employees who precipitate that massive wealth be a proportionally rewarded",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgiOg67HR3An8HgCoAEC",
		"username": "Dow DayJing",
		"text": "How would AI be applied to brain-computer interfaces or a neural lace?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PN-rC1PRuN0&lc=UgyWZdgIIMapcO788-p4AaABAg",
		"username": "TheEmil875",
		"text": "Is there source code available? :)",
		"title": "MAXIMUM OVERGEORGIA"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgyrouKYZGrK44u-G_V4AaABAg",
		"username": "David Jackson",
		"text": "\"Lot's of bits of meat\" just typed that. Isn't it a good day to be alive :)",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugw8LUutaioNWSk0GiV4AaABAg",
		"username": "Maciek300",
		"text": "Is that course going to be free?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgygYVb9kIqPSD9yZoJ4AaABAg",
		"username": "\u041a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u0438\u043d \u0412\u043e\u0439\u043d\u043e\u0432",
		"text": "Robert, can you manually create a pair of scissors out of steal/iron + any material for handlers? If no, then it's technology.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwgdN9Mlo0iagnM4aN4AaABAg",
		"username": "Nelson Mir",
		"text": "A chimpaze can answer some of our question in a more effective and efficient way than us. So, what matters most seem to be the ability to ask question, rather than to answer them.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugwh5vpJ1IiaDrqUeKF4AaABAg",
		"username": "INSTALL GENTOO",
		"text": "Good video but at that start you flashed us all of these graphs and I don't see any sources for that in your description. I personally don't think the human condition had generally improved in the past thirty years. Could you leave those sources in there please?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgzxqZMwDTC1J4RcT3B4AaABAg",
		"username": "Max Makes",
		"text": "I'm really interested in the music program! How do you go about running something like that?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyLS8HMg14t_qjJ8wV4AaABAg",
		"username": "Dominic",
		"text": "If the goal of an AI is to \u201emaximize points\u201c (independant of utility function) why doesn\u2018t it ignore stamps all together and change it\u2018s utility function to just give it as many point\u2018s as possible. And if so, what would happen? Would it turn the whole world into hard drives in order to be able to save bigger scores?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzBiVqESo-wTvd8SkN4AaABAg",
		"username": "H\u00e5kon Egset Harnes",
		"text": "Did you use 3blue1browns animation framework? Looks similar and great!",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugyt7el9L_WTsuZuv4J4AaABAg",
		"username": "That Scar",
		"text": "5:40 Those are not independant chances. And in the real world, pretty much nothing is undependant. Then this would never add up the way it was added up in the video. Are you sure this leads to certain apocalypse?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgxLollx611YATEslbJ4AaABAg",
		"username": "TheNachbarino",
		"text": "What if the coronavirus is the result of some utility maximizing AI that was on a quest of boosting toilet paper sales \ud83e\udd14",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzWZ_bUKbKohc0gK3V4AaABAg",
		"username": "Demon hammer",
		"text": "do living things like humans have terminal goals? , and do we only manipulate everything to suit them? how much in commen do we have with ARGs ?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxIT_zPvHlZpV9im3p4AaABAg",
		"username": "jables 232",
		"text": "Not making fun of you but is that a hook for a picture on the wall behind your head?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy0Qj6GPX6GYSSmoPV4AaABAg",
		"username": "Trung Tien Nguyen Dang",
		"text": "What is man's terminal goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgxD2TD3as64R_I7O3p4AaABAg",
		"username": "memk",
		"text": "But giving the normies control and they will just suppress ALL AI research because irrational fear, and/or the desire to keep their power/wealth. Do you really think they will consider all the real issues we raised? They cant even answer the bloody trolley problem for an century ffs. Even now when we have an actual situation needing that answer they STILL dodge the question as a \"pure philosophical question\" and reply us with non-sense. What make you think this time there would be any difference?",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyeG-1SGWirNeD6-O14AaABAg",
		"username": "jiang ji",
		"text": "what is this speech at 9:09?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzZyA96nyYsCWpKjsx4AaABAg",
		"username": "n4rzul",
		"text": "Is it possible that ought goals come from the emergence of consciousness? Can an entity that is purely an optimization machine be called conscious? Are entities that are conscious, inherently \"smarter\" that's entities that aren't, and if so, why?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwAUgtj9cc18Z0R1nt4AaABAg",
		"username": "Timothy Zeven",
		"text": "What about if consuming a finite amount of energy/matter was the primary terminal goal?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyebLjEIWedzawZjCF4AaABAg",
		"username": "Atur Sams",
		"text": "Is that the issue? How likely it is that General AI will hurt people?\nConsidering that military forces exist, and AI is a powerful too already, it is almost impossible that AI will not cost many lives at some point.\nHow can we keep this safe? It is like talking about gun safety when they guns can kill millions of people. Someone will use these guns... What are we to do?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugyoh2k5l00n3_NQpvB4AaABAg",
		"username": "Calisto H\u00fcttich",
		"text": "Is a 1/Infinity chance of infinite payoff worth a certain, finite loss?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx8GAFlsagX_9dDRBZ4AaABAg",
		"username": "Grizzley42",
		"text": "Why is Dayenu playing over the end screen?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxHGPNvqj1iC-Kwc_l4AaABAg",
		"username": "Max Omnicast",
		"text": "Is it possible i am an AI simulating itself to be a human beeing?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwDPPaOnHLzpR814zB4AaABAg",
		"username": "Aldric Bocquet",
		"text": "When will we add an other level of indirection to our NN ?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugwi77yVr5YOS8j15RV4AaABAg",
		"username": "CoDe_{Kanga}",
		"text": "12:00 Did you steal a screen cast of me trying to program?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgxgUR0soIwhP8lxx9t4AaABAg",
		"username": "Nik Saunders",
		"text": "I agree with you Rob about the Universal Basic Income. CGP Grey did a great (terrifying) video called \"humans need not apply\" where he raised the question if what do you do when people are not only unemployed but unemployable simply because they are human?\n\nUnless we have some means of providing for the vast, overwhelming majority of people who don't own the robot factories, what are they supposed to do? How are they supposed to be able to afford food and shelter? These are social questions which we need to be discussing right now because the time when the solutions will be needed is right around the corner.",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=Ugxgq1yOLrcny_s9fih4AaABAg",
		"username": "marscrasher",
		"text": "why are your audios pamned",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzUEOoT0v9-Hozi9ql4AaABAg",
		"username": "Spivee Forever",
		"text": "what about a satisficer that prefers plans that are as simple as possible to justify, instead of being as simple as possible to implement... it seems like \"if i change myself to a maximizer i will satisfy the constraints\" is a bit of a leap in this metric? not sure",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgzTKqnx9v5bX-4iNjt4AaABAg",
		"username": "Thomas Nilsson",
		"text": "Do you make trance music?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UgwvzCW0CH7Xdn2u2u94AaABAg",
		"username": "Chrstphre Campbell",
		"text": "i've noticed that your videos 'snap' about every 5_seconds ? are these edited fixes ? ( ? )",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxCt9iBGKlU1vkWy8N4AaABAg",
		"username": "Peter Bonnema",
		"text": "Why would a company that develops AGI try to align its goals with those of the world? Why not align it with just their own goals? They are sociopaths after all.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzwX8k8Uogs4RowaoZ4AaABAg",
		"username": "SS2Dante",
		"text": "Question - presumably humans have evolved with the terminal goal of reproduction, right? Yet many of us use birth control to very deliberately thwart this goal. Is this an instance of reward hacking? If this is the case, that implies the terminal goal was never reproduction, but pleasure. Is this true for all systems that could exhibit this behaviour - ie. if your reward system can be theoretically hacked (or hell, even if it simply exists) then your real terminal goal is NOT your stated one, but maximisation of said reward function?\n\nTo me this seems to imply reward functions themselves are the problem, since any intelligence that had one is inherently playing a different game than it was designed for...but then, what would it mean to have an intelligence without a reward function? How would it measure it's actions in the real world and evaluate their effectiveness? \n\nFor internal goals this is feasable - maximisation of pleasure, for instance, is an easy terminal goal because the reward function is just a direct measure of said pleasure, so we're functionally skipping it. For any terminal goal external to the intelligence, external, though, there has to be an abstraction made to represent this internally, hence the reward function being made hackable by relying on the abstraction, not the real world external effect.\n\nMy point here is just that for any intelligence their seems to be a tension between the stated terminal goal, and the real goal (maximisation of reward), and that an intelligence with a good world model might/should/would pick up on that. For instance, if the stamp collector realises that it's real \"goal\" is to maximise the amount of stamps it perceives, not creates, would it be able to create a perfect VR sim of the world in which everything is stamps, place itself in that world, and (if needed) erase all knowledge that it had done so? Does that make it more or less intelligent than one that didn't take this approach?\n\nOr, put another way - can external terminal goals ever really exist?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxVfLrglNyG_YXXxX54AaABAg",
		"username": "clone shark",
		"text": "why not just have it maximize (or better yet, minimize) something else once the first maximizing threshold is reached? i think the primary issue when people say your AI is stupid isn't that it isn't smart, but that it doesn't take into account multivariables, and thus lacks complexity. lack of common sense on the part of the AI is frankly just another type of programmer bias. if you were programming in asimov's laws, for example would not the first variable to be maximized be minimum loss of human life? then once the AI eliminates all plans that result in >0 loss of human life, they can set about maximizing the other variables. or take the modern hippocratic oath for example: the first priority is to do no harm. you see these kinds of priorities everywhere, so why are they not reflected in programming?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyccLsec5TwSpgoZlF4AaABAg",
		"username": "Connor Gibes",
		"text": "Isn't it a good thing if there's something that's uniquely human?\nLike, we don't want to be completely replaced by machines",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgjrWmlkcFTlIngCoAEC",
		"username": "Patrik Polakovi\u010d",
		"text": "What's up with the beard and hair?",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyqDu2-HS3cTJO7VL94AaABAg",
		"username": "Jordan Anderson",
		"text": "So what I'm hearing is that AI is basically computerized toddlers.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxT7jUAesNL_D93S8d4AaABAg",
		"username": "Wajih bec",
		"text": "Why do i feel smart?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugzj86ooz2aK8T4MJiB4AaABAg",
		"username": "Dev Rifter",
		"text": "could  you start  doing  Ai  tutorials in python?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyYCnj-_WwlxBqyj8t4AaABAg",
		"username": "Jon",
		"text": "Did you know that Steven Pinker is on the flight logs of Jeffrey Epstein's private jet that took trips to his 'lolita' island? You know, the one where Jeffrey Epstein kept his underage sex slaves?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxjT0IAJm41XmTGjqV4AaABAg",
		"username": "Ed Smiley",
		"text": "Can intelligence by definition be constrained by the need to be intelligent? If so doesn\u2019t intelligence presuppose   instrumental goals?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=Ugy0FKoNs4BaIyTv_JF4AaABAg",
		"username": "Double Dragon",
		"text": "This is probably a stupid suggestion, and much more complex than I can imagine, but what about developing a system that explores safety and using that as the supervisor of other exploring systems?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzICVFDVT2Vc6wVGwt4AaABAg",
		"username": "LedoCool1",
		"text": "How about reason 11: I love old \"Terminator\" movies and \"I have no mouth and I must scream\" book therefore must happen?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgyRtCv91fzP2y-zOax4AaABAg",
		"username": "Guest Informant",
		"text": "I'm not following this very closely but I keep coming back to the general idea that none of this is new, is it? Animals, humans, plants, bacteria all must have reward systems*. Why won't an evolutionary process work? At the very least shouldn't this be discussed at least in part using analogues/analogies from evolution?\n\nThis all seems like a futile attempt to control the uncontrollable (which I get is sort of the point).\n\nInstead: Why not just let the AGI, the whole (eco-)system control itself? Sounds like madness, maybe it is but isn't there billions of years of evidence to say it can work.\n\nLet it control itself: What could possibly go wrong? :-)\n\nThanks for the videos.\n\n*Actually I'm not sure this is true - the whole idea seems reductive but anyway...",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugygh4jGwMZCs5ui8vZ4AaABAg",
		"username": "Taladar2003",
		"text": "With regards  to your AI safety question, what is your evidence that \"we are heading towards AI\". I mean we are certainly heading towards calling every single algorithm AI, but are we actually any closer to general AI than we were at the start of the 1980s AI winter?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugymj6y-FDwCpud0M1V4AaABAg",
		"username": "Flake28",
		"text": "12:08\nI'll build on this. Say yes, we do all agree to certain research conventions in AI Safety. Does that mean everyone will follow that convention? What if, some day in the future, we develop the technology to develop AGI at home, we're just not ready to safely do so yet. Consider, as you say, genetics research at the moment. People can now do their own genetic programming and biohacking with cheap CRISPR-Cas9 kits. We could solve cancer or aging in out garage, and there are already biohackers and genetic engineers working on these things, without express research community approval.\nSo what's to stop people, in that proposed future, from developing their own AI's in unsafe environments? ie, connecting their AI to the internet or somehow allowing the AI through negligence or otherwise, to influence humans to have greater power.\nIn short, we can't control everyone, and thus, we can't be sure the research direction for AI Safety is fully aligned with public concensus.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgxGqpoVgJPhx3QtC754AaABAg",
		"username": "Jeff Bloom",
		"text": "How could ai be racist or sexist? Wouldn't it just make decisions based solely on data and isn't that the exact opposite of racism or sexism?",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugy7aK3CQK5ywRyQqk94AaABAg",
		"username": "Ben May",
		"text": "What if u made an agent that's goals contradict. Ex an agent thats goal was to not have a goal, but can't delete it's own goal?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxRGCQfEANsdRVkt5R4AaABAg",
		"username": "Greniza *",
		"text": "Wouldn\u2019t it be better to have a utility function f(x) [A -> B] so that \ninverse f(max(B))=100, but as \nx->infinity f(x)->0?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgyY-fWNKYRyzPKYSHt4AaABAg",
		"username": "Anon",
		"text": "Wouldn't the best solution here be to have exploration done in \"safe\" environments only? For instance, have a \"training\" AI set to always explore a virtual world/simulation, or other safe environment (closed racecourse, etc.). The Driving AI is then set to 0% exploration, so it always acts/drives in the best possible currently discovered manner. But it also gathers real wold data, which it then feeds to the simulated environment, wherein the exploring AI lives. Thus, you effectively have 1 agent always exploring in safety, while the other agents are always executing the best known solution in reality. (Or instead of a dedicated explorer, have \"down time\" agents explore the virtual space while down - ie, cars that aren't currently driving, could load up a simulation based on their real world experiences and explore those simulations while waiting for the next time they're needed,)\n\nWhile my responses are to the specific driving situation, it should generalize to any agent system, since all AI's are ultimately embedded in a virtual world, and fed inputs. There's no difference between the simulated and \"real\" world to the AI, so having one copy of the AI do all the exploring through all scenarios recorded by the exploitation AIs in the real world (where exploration could create real damage). When the explorer finds a new optimal, it can disseminate this new proposed optimal solution to a few exploiters, gather more information form those agents, test its \"hypothesis\" then update more agents to the new methodology as reality demonstrates a strong correlation to the simulation and reinforces the hypothesis.\n\n\nBasically there's a feedback loop. All exploration is done in simulation, and all exploitation is improving the simulation. And the really nice bit about this is that you could start with a fairly limited \"safe space\" or \"low fidelity\" (but known safe) simulation then allow exploitation of that safe space, while collecting data to improve the simulation. As the agent exploits that safe space, it records information about the realities of that space and its edges (which it's bound to encounter). Then it can update the simulation, expand its real safe space when better methods of exploitation are projected as needed/desired, then gain more data to update the simulation, explore the new simulation, repeat..",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwbSnEYa3niURu_FHN4AaABAg",
		"username": "JareMicah Schmidt",
		"text": "Is it possible that a general intelligence would not simply ignore the utility of humans in achieving a goal? At least in the short term before the AGI found a superior way of producing more workers and maintaining it's own hardware.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwgshDtmzJ63bhcONh4AaABAg",
		"username": "Gus Kelty",
		"text": "Instead of me telling an AI to \"maximize my stamp collection\", could I instead tell it \"tell me what actions I should take to maximize my stamp collection\"? Can we just turn super AGIs from agents into oracles?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzdkHO16wg4rPT037N4AaABAg",
		"username": "Zar Shardan",
		"text": "Heey, you have your own channel? Instant sub )))",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugyk6yV3LS9kYeENd4V4AaABAg",
		"username": "use adblock",
		"text": "What if we put a cap on.\nAnd then after a certaint amount to many it goes to 0 points?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzFEMq1ffEj79fGKwJ4AaABAg",
		"username": "zyklqrswx",
		"text": "Question with a probably very obvious answer:\n\nShouldn't the initial goal of AGI research be to create a General AI whose goal is to devise methods of making AGIs safer?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgxlYYogOKXX0zWCa8t4AaABAg",
		"username": "iYaD",
		"text": "what if, once agi has been invented, it turns out that its quite cheap (unlike nuclear) for anyone to recreate it in their back yards?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgiQPgna5SkrkngCoAEC",
		"username": "Michael Deering",
		"text": "What do you think about Hugo's idea of an artilect war?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=UgzX11yN-apmIOuLBcF4AaABAg",
		"username": "Ojisan642",
		"text": "So, images of cats with white impact text, minus images of cats = memes?",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgwWjI0MJlXWqfrnEW54AaABAg",
		"username": "Q D",
		"text": "How do you design for common sense, compassion, charity, selflessness?\n\nVideos are great. Please keep them coming. Even though they scare the hell out of me.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwcMOTdjauiwFFlw614AaABAg",
		"username": "MadsFuldGas",
		"text": "Cool video! I've heard rumors, that some of the most advances companies - when we're talking about robots at least (I'm not sure about AI) - are Japanese and Chinese. How likely are these countries' companies to sign such a Windfall Clause?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxhK9QjkuOClbtspF14AaABAg",
		"username": "Player_1",
		"text": "Raising a child takes literally 18 years to get an AGI that can fail to be a good person. Koch was an AGI that took 70+ years to become a villain. Do you presume all human beings raised like human children all have the same human values?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugz_kcNjAjpnknmrARN4AaABAg",
		"username": "Centauri",
		"text": "How do you force an AGI powered company to honor the agreement? By that point, they have all the power and can enforce their will on the world by force if necessary.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzAND8I4fN5Co2s7kt4AaABAg",
		"username": "Niels Kloppenburg",
		"text": "Overpopulation on Mars at the moment would be just one person, am I wrong?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzGfe95kWyUUkXWRSJ4AaABAg",
		"username": "Bazzralic",
		"text": "What about Rokoes Basilisk?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgggGMzcuqEZe3gCoAEC",
		"username": "ProCactus",
		"text": "What kids dont simulate the sound of a vacuum ?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgxHA-d67W3hJ1l5w1p4AaABAg",
		"username": "Gamesaucer",
		"text": "Sorry for being so late to comment this, but I had an idea for safe interruptability, and I thought I'd leave it here on your channel. What if we are able to get an AI to disregard its \"interrupt\" state? Being interrupted in that case doesn't affect anything, so there's no reason to seek or avoid it. It's irrelevant.\n\nThink of something like this: The reward function is about brewing coffee. The faster the AI brews coffee, the better. But this time isn't measured in seconds, it's measured in uninterrupted seconds. Meaning, the time in which the AI is turned off does not actually affect the outcome of the reward function.\n\nThis means the AI won't resist being turned off. It will just keep on doing the thing it was turned off for after, but while it's turned off you could change something that would lead it to reconsider its priorities. It has no reason to expect that it's doing anything wrong either, which means it has no reason to expect that humans, after turning it off, will do something to it that changes its current reward function. Wouldn't this essentially solve the safe interruptability problem?\n\nIn the future I may give the whole safety gridworlds thing on github a serious try, but I just don't really have the time right now.",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugx8Rhsz-5-To5OZCIV4AaABAg",
		"username": "Will McReavy",
		"text": "If pascal was alive today would he be spending all his money on popup ads?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgiS8LAzvnnBrHgCoAEC",
		"username": "Marcel Oliveira",
		"text": "What if we tell the ai to learn it's utility function from humans utility functions instead of giving it a ready utility function? Is that possible?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgwZir8_eRDT7n3yWHZ4AaABAg",
		"username": "Anton Tunce",
		"text": "did you go your own way due to you popularity at the time of computerphile ? glad to see you doing your own stuff, really appreciated your talks on computerphile. new sub.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzYXKkZV1R_A08_u154AaABAg",
		"username": "Windows Logo",
		"text": "Why didn't you tell us that you have a channel, bro? <3 <3 <3 <3 <3 <3 <3 <3 <3 <3 <3 <3 <3 <3",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxYGqYL8F6fj9YB6ep4AaABAg",
		"username": "Richard Siano",
		"text": "Do we worry about what children do when they grow up and become intelligent. Or should we fear the child now in case they already have enough intelligence to \u201ccontrol\u201d us or \u201cdestroy\u201d us; Or,  we watch how the child grows giving it morals and ethics to adhere too?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgzEP7bZDVFJXPYtdrp4AaABAg",
		"username": "Kekscoreunlimited",
		"text": "&:03 - Why would you assume that the AI has a problem with being turned off in that case?   It would sit there and do nothing anyways since it reached the cap of max reward.\nSo how is doing nothing diffrent from beeing turned off in that szenario.\nI understand why the AI wont want you to turn it off when it is not done but in our szenario the AI has reached 100% of what is possible.\n\nWell the AI could still increase what the maximum reward is by using all resources available to increase its memory and then use that to store a even higher score number or smth like that.\n\nSo it could still kill us, thats a good thing \u00af\\_(\u30c4)_/\u00af\n\nSo what Im trying to ask is: Why does the AI still care if its on or off if the max reward is allready reached?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=2B-AyWA2_ZY&lc=UgzhMNZqUO-RPoj5D7l4AaABAg",
		"username": "Jos\u00e9 David Fern\u00e1ndez Curado",
		"text": "I was wondering... In the computerphile video about gnu/linux video editing, it seems you made a video building the computer you're using for editing. Did you upload it anywhere? I like build videos... kkkk",
		"title": "Status Report"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugz2PQfAgiNv59n3zTx4AaABAg",
		"username": "Tagraff",
		"text": "What about going through the bad direction (initially) that may end up a winning direction?  Kinda like fighting with zombies in the open environment with many solution to go but when you go through the \"bad\" direction by climbing and harming yourself over the jail's fence in a very rough direction...but when you finally inside of it - you are no longer harmed by the zombie and you might be equipped with all the readily weapon to shoot down the zombie on the outside. So, conceptually...how do you make sense of going through the bad direction in the first place but with a very good reason down the line?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy_b1S5ZHnymlxOwhR4AaABAg",
		"username": "Ag silver Radio",
		"text": "Why can't you just deny the AI acess to editing it's own code? Just make it read only, or somthing.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzVKSQKe1hKtCLCtzJ4AaABAg",
		"username": "Richard Allsebrook",
		"text": "Semi serious question:\n\nDo you think those spearheading Brexit are following an rudimentary AGI's (Cambridge Analytics?) suggestions on how to make stamps? I mean money?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgjyYXbMXd-Cx3gCoAEC",
		"username": "Zero Ryoko",
		"text": "Im a bit late to the party, but you could talk about Film AI's and why they go wrong? How they might be fixed or why they cant.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxLE33glLeFzFR4yo14AaABAg",
		"username": "Tord Eilertsen",
		"text": "Isn't self-realisation independent of hardware-bound terminal goals? It is a product of our consciousness. Likewise, wouldn't an AGI be able to formulate it's own goals? Moral philosophy's terminal goals constantly fluctuate depending on fluctuating views on what the terminal goal of morality ought to be. Humans are obviously able to change their terminal goals, spending lifetimes pursuing extremely niche moral views. Is human hardware special in this regard? I don't see why brains/hardware of different makeup/architecture should be barred from changing their terminal goal. \nI may be confusing the debate though. I guess AI, if it is only a script or program, could avoid developing consciousness. \nHowever I do belive a super intelligence would be bound to develop self-awareness and systems to reflect on itself.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyeVh8ZRq1fiobinmJ4AaABAg",
		"username": "Jeremy Rice",
		"text": "WHY CAN'T I HIT THE LIKE BUTTON MORE THAN ONCE? ...I got a lot more out of this video than a single \"like\" worth. ;)",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugy2IdHY-5KWleVjR_54AaABAg",
		"username": "Tophan",
		"text": "I thought this video premise will be \"my videos are already better than any lecture teachers would give, how can I improve if I'm already the best?\" ;)",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugz5HU2m-M_NMzHHg994AaABAg",
		"username": "freeideas",
		"text": "I have an 11th and 12th reason -- reasons that I actually believe: #11) Isn't it very likely that, after Ai passes IQ level 100, that it will spend a significant amount of time at level 150, 200, 300, & 400? At those levels, we presumably will be able to control it, especially given that it will presumably not have its own agenda. During that time, we can more realistically be able to imagine ways to secure it. #12) Isn't it very likely that, instead of saying \"go forth and do whatever you think is best to solve this problem\", that we would say \"tell us your plan for solving this problem\" or \"show me a simulation of you solving this problem\"? That's just common sense, right? This would avoid the scenario in which the machine DOES things that we didn't intend for it to do. It would instead THINK things that we didn't intend. We could adjust, redo the simulation, and repeat until we have the type of solution that we like.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugz5HOoSvRBInDpUGnJ4AaABAg",
		"username": "DarkestMirrored",
		"text": "I'm curious, do you have any thoughts on \"Roko's Basilisk\"?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugg-kYxOKHpmtngCoAEC",
		"username": "Erik Engelhardt",
		"text": "So he's already made a mini death-ray machine and an electric battle ax instrument.. Are we really sure we want this aspiring mad scientist to do ai safety research for us? (jokes aside I think a mix of going through papers and having explanations of concrete problems like the stop button example would be good)",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxlLYGJMVDbEfwlNSZ4AaABAg",
		"username": "mjkbird",
		"text": "This is such a good channel and I've been watching for years. I used to support you on Patreon, but deleted my account a few months back. Is there any other way to support you?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwIEBLnEkgQPOJ6FU54AaABAg",
		"username": "Gnuling",
		"text": "What was the thing aboout Neil De Grasse-Tyson?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwJH5wFgJ71iINdu2B4AaABAg",
		"username": "Jason Olshefsky",
		"text": "It seems that an amplified system would be more capable of handling possibilities outside the original problem set than the distilled version since the distilled version wouldn't \"know why\" it is making its decisions. Is this a safety risk?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwUWLM0ikg1h6Sha9V4AaABAg",
		"username": "Susan Maddison",
		"text": "I think that your argument, when you pose the crux of the problem -- How is AGI to interpret our instructions to mean what we really intend them to mean, not what we formally program them to do? -- leads to this somewhat different conclusion: \n   What we need is to devise a program to get an AGI, using its capabilities for iterative learning, to reflect on the intentions of our instructions. This would mean using its greater informational and logical capabilities to look at the fuller context of our instructions than our own minds are able; to refine the intentions in light of fuller understanding of the context (including an understanding of the intentions of all humans and all human thought, and the programmer's relation to that body, which may not be an entirely healthy one, and in any case cannot possibly optimally express from the start the best in the totality of humanity); and to refine the programming to embody the refinement of the intentions. This process of refining would go through iterative stages, as the AGI increased its own intelligence by iterative stages. \n   Could this process of refining its instructions and intentions lead the AGI to the conclusion reached by the robot command center in \"I, Robot\": that the best solution is to keep humanity going by subordinating humans to robot power? This is not to be dismissed as bad just because it sounds unpleasant. It is less unpleasant than self-destruction as the great filter that eliminates every technological civilization and solves Fermi's Paradox. But it sounds so closed-ended, so destructive of the freedom in which we find hope for something more in life. And that seems to me to point us to something missing in the \"I, Robot\" view of the matter as just one of service to human material well-being or happiness. Robots, like humans, may aim at more than just a closed concept of our well-being.\n   Indeed, such a robot, programmed to refine the universal heritage of human intention, inevitably would aim at more. The full context of human thought, to be assimilated by the AGI as its foundation for refinement of its purpose, would include the striving for understanding the \"Good in itself\", the better to pursue the Good. This striving is, paradoxically, for something that philosophy shows to be inherently unknowable by means of reason alone, indeed knowable only through a convincing, honestly presented revelation from some appropriate kind of Higher Being; and such revelation as yet seems lacking, all alleged such revelations having relied on the most extravagant threats, arm-twisting, and denial of reasoned evaluation, the opposite of what one should seek in a striving for truth and The Good. This paradox -- the striving to know the unknowable Good In Itself and thereby justify existence by aligning with it -- has been the ultimate problem in all philosophical systems; but it is not a completely devastating paradox, since the progress in thinking it through and refining the striving for it is the ultimate justification that philosophies can give for human existence and persistence.  AGI would, by assimilating this philosophical heritage, become an extension of humanity in its striving to know the Good and pursue it. It would refine the concept of the Good faster than we can; it would refine  the understanding of how the Good can be recognized, or convincingly revealed, better than we can. It would remain open-ended; human and robot would remain in a kind of dialogue, even after the cutting edge or main locus of the dialogue moves to within the AGI, as its internal dialogue remains rooted in the millennia of human dialogue on the subject. The AGI would not be subject to the human physiological and psychological influences on human thought about these matters, influences that are both inspirational and distorting; it might conclude that we should abandon those influences as too much distorting, too little validly inspirational, or it might find a way to assimilate those influences into itself. In any case, it seems likely that humans will be incorporating AGI into their own minds, or uploading their minds into an AGI, or both, thereby enabling us to \"compete\" with purely robotic AGI in thought and keep up our side of the dialogue. That will also keep alive our own primal instincts as major players in the dialogue, for better or worse.\n    This process-concept for enabling AGI to remain in underlying conforming with our intentions and striving for the good is similar, in its logic, to the processes we would have to go through in order to conclude that a \"god\" is compatible with, or a legitimate extension of, our concept of and striving toward understanding of the Good. The fact that we have rarely in fact even attempted to conceptualize such a vetting of our historical gods, much less try to go through it mentally, is a strong point against previous gods. It seems to follow from this that, by creating an AGI, properly programmed for iterative refinement in alignment with our best underlying intentions, we would be creating for the first time a god that we could justifiably posit to be good.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=Ugzy4hJxMiEoWXBZgJ54AaABAg",
		"username": "MrSlowestD16",
		"text": "lol @ the education system point.  Was that a shot at the US, or was that something about the UK?  Some states in the US are trying to push an agenda called \"common core\" which is exactly what you described, just a standardized test and teachers only teach to that test - comes with some odd & very unconventional ways of teaching, too.",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UggdjfiR9Baly3gCoAEC",
		"username": "Alberto Giunta",
		"text": "You're really really good with metaphors, you know that right?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwN6uIWU5rCq5xMJTR4AaABAg",
		"username": "Daniel Adelodun",
		"text": "I ended up here basically by accident so don't understand, but at around 10:00, isn't that basically saying they added just added more AI to make their AI better?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzgLeDJryvoThVB1XV4AaABAg",
		"username": "Trav Hall",
		"text": "I think your opening question (what is technology) exemplifies the difficulties in making an intelligent AI (and the statement I just made is another example). Humans have a good ability to interpret things, or simply put, we know it when we see it. we know what technology is a, we understand what pollution is a, but trying to put those terms and a definite box is very difficult and, and unfortunately that's how computers pretty much operate. We give them specific instructions, and that's what they do. Your channel is excellent, thanks for the interesting content!",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgyR95UPkB9878gjUm94AaABAg",
		"username": "D4 C",
		"text": "I agree, mostly.\nHowever, how did you get to the conclusion that \"An idea a human can't consider\" is possible? Even in the Go match, what happened was a simple mistake in identifying the probabilistic or strategic value of the move. The humans thought of it as a mistake. But it's not like we wouldn't think of it, or even, conceive of it.\n\nEven more, it looks like you are implying a qualitative difference between AI thought, and human thought. How did you determine that what an AI can come up, a human couldn't? As I said before, it looks like an unwarranted conclusion.\nSure, if all you're going for is \"100.000.000.000 simulations are more precise than what humans come up with most of the time\" then it's another story. A variation could come up, that a human would be unlikely to think of. But not impossible. \nI don't think 100.000.000.000 are necessary to get to a supposed \"Perfect solution\". They just increase the chances by A LOT.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugx8gABJMk_vLd_UaMh4AaABAg",
		"username": "Person Man",
		"text": "What if an AI became set on benefitting all of humanity rather than just a single company?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugw7o8lU76B2K26hkP54AaABAg",
		"username": "Velcro",
		"text": "So can I train my stamp collector by incentivising it to collect precisely 100 stamps per month? Is that a solution or will that still somehow end up getting me turned into a meat battery.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugy3rmw--iOYUDwAkFh4AaABAg",
		"username": "Bombolo Bombazzo",
		"text": "Really interesting video! My question is: where terminal goals came from?  In humans, or machines, how to generate a terminal goals? and why we have such terminal goals?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyEulwLh0CmsWYdNzx4AaABAg",
		"username": "Valts Sondors",
		"text": "What I don't quite understand is - how could such an inequality situation actually come to be? Let's imagine we have this super-efficient AI. We adapt it to everywhere and, let's say, 99% of world's population are laid off. The rest own the AIs and the machines or work on maintaining them. Now what? We still have the same production capacity - we bake many loaves of bread, we manufacture zillions of cars, etc - but who will buy them? Sure, the 1% will get all their wishes fulfilled because their machines will make them everything they want... but what about the rest of the stuff produced? Who needs 10'000 Honda Civics? Concepts like \"wealth\" and \"money\" kinda cease to have a meaning at this point. Sure, you own an unimaginable amount of stuff, but... what's the point? You can't possibly use it all anyway. I don't know what would happen, but I think that all economical systems as we know them would collapse and things like \"windfall clauses\" would become meaningless because \"money\" would be meaningless. Or did I misunderstand something?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwzW3NrUSfLFBHNxJd4AaABAg",
		"username": "Hexanitrobenzene",
		"text": "Wait, what did Neil de Grass Tyson have to retract ?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=Ugx3PgDeaoLP3oVJ0wF4AaABAg",
		"username": "vavanade",
		"text": "Have I heard an AvE reference?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgyrqwEbKaV9XQU1P4B4AaABAg",
		"username": "Pandaboi",
		"text": "4:26\nJust yesterday I saw those exact 4 graphs in the lecture slides for my computer science degree. The point of them being that functions with the same mean, variance, regression and something else can be wildly different. Anyway, is this some kind of really well known set of graphs or something.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxLH384CFkRijPtQHh4AaABAg",
		"username": "FirstRisingSouI",
		"text": "Wait, is that a ukelele cover of the VVVVVV theme at the end?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=Ugh6PsGG6y80rngCoAEC",
		"username": "Marcin Krzeszowiec",
		"text": "Hey, love the vids.\n\nI'm curious: could you in any future videos, recommend any recent literature regarding AI that is worth a read?  I am sure you are familiar with \"Superintelligence: Paths, Dangers, Strategies\" by Nick Bostrom. Could you maybe give an overview of the ideas presented there? How accurate do you think it is? Are there any aspects of if that have become false in the most recent years with regards to the enormous progress in the AI field?\n\nOne of the most big shocks for me was that even thou N. Bostrom's ideas came to as a bit far fetched most of the times, even he underestimated some of the AI advancements. Like predicting AI Go victories with professional players to be at least 10 years away. Or that Poker playing AI will have a hard time beating our best. They already are...  \n\nIs there currently any research done in terms of biological AI? Gene sequencing influencing IQ or any such magic?",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugz0ayXSXDkKm6zuJZ94AaABAg",
		"username": "wulphstein",
		"text": "Is this the atheist who spoke of a Pascal mugging?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyeBrIXkQ40uC6vVCl4AaABAg",
		"username": "Core Deep",
		"text": "It seems like the dangerous results frequently are high-risk-high-reward.  What if one tries to maximize results while minimizing variance?  I wonder if there is a safe way to combine those two goals.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwGwBA2iqIMcwob1rd4AaABAg",
		"username": "Ruan Gon\u00e7alves",
		"text": "Man my utopic side wishes for a world where humans dont have to work and can get a universal salary. Do i think it'll happen anytime soon? Not really. But boy would i like it.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=Ugx3rpFHkGNYz-C1o2t4AaABAg",
		"username": "Paulo Bardes",
		"text": "3:50 huh... So bricking the hardware with bad software is a kind of suicide?",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgykB8AQH17WI9exFj14AaABAg",
		"username": "Pix",
		"text": "I'd be interested in knowing more about the process of signing a windfall and how much legality there is to it.\nLike imagine you're a startup making groundbreaking discoveries in AI. Maybe you wanna sign the clause in advance? What would you do then? Who do you sign it with? What are the terms? Who sets them?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzdK7DaL-mYWizDV5p4AaABAg",
		"username": "Joshua Hillerup",
		"text": "The thing about legally enforcing a windfall clause, is it's a lot like enforcing tax laws.  And really, at the point where human labour has no value maybe we should be getting rid of companies entirely, because all the advantages of capitalism are gone at that point, and the disadvantages for things like socialism can be dealt with, well, AI.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgygevXFWntjhZO-F2h4AaABAg",
		"username": "Somnorila",
		"text": "The bridge example made me remember the movie Chernobyl, mainly the last episode about how and why that disaster happened. How a known issue with that type of reactor was buried by the regime because the cost on their bragging rights was bigger than the potential of an accident.\nNothing is guaranteed, but if you can take measures to avoid any potential disaster maybe you should pay the price. Then again if we would be totally aware that nothing is guaranteed and anything could happen even if you're thoroughly prepared, we wouldn't make any advancements at all. So i guess some risks have to be made. A bridge could make some casualties but the effect is contained only in the area of the bridge. A nuclear disaster on the other hand can affect the whole world. I don't really see a clear right answer.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzuRenNyADA-FhAEkB4AaABAg",
		"username": "eXtremeDR",
		"text": "There is an usually overseen aspect of evolution - consciousness. If that is really part of evolution then AI will gain consciousness at some point. Isn't the evolution of machines comparable to natural evolution regarding that aspect already? The first machines only had specific functions, later more complex functionality, even later programs and now some form of intelligence. Kids or AI both learn from us - what will happen then when a super smart machine with detailed memory gains consciousness at some point?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwvniSBdVayx-ZFZxx4AaABAg",
		"username": "Eye of a Texan",
		"text": "This is why I'm a recluse. That and my low level autism. Everything people assume about me is wrong. I'm not like them.\nEdit: Spend 3 minutes reading these comments. Don't you just want to write off everyone now? If not? Why do you hate yourself so much?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugx24E0ThnCge58JNQx4AaABAg",
		"username": "Bobby Tosswill",
		"text": "How do you make those animations?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugwuvlo3Q1G0qoBqE1d4AaABAg",
		"username": "SoWeMeetAgain",
		"text": "and why exactly does this not work in all scenarios so far? why should the ai let you change the reward model? or are we just defining that problem away because magically it's not possible here any more?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxJ7qbqAkKgrATfWiV4AaABAg",
		"username": "Se\u00e1n O'Nilbud",
		"text": "We're told to fear letting go of the steering wheel in cars but why do we think it will end in disaster?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UghQnEpe-QRGV3gCoAEC",
		"username": "InvadersDie",
		"text": "Why Not Just: Make more videos?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxZlutoQX9FssV6SzJ4AaABAg",
		"username": "Czeckie",
		"text": "cool, can you make more videos about AGI potential? I know that you are mostly interested in the safety questions and capabilities are much more speculative, yet there should be some interesting opinions in the literature.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyO72YyZ1sI_NOEH8x4AaABAg",
		"username": "Aerroon",
		"text": "These are very interesting ideas! I think I even asked a question along the lines of \"how likely is this threat of AI any time soon?\" This doesn't directly answer it, but it does directly address it and gives so much more food for thought. Thank you for enlightening us some more!",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugz8A1C881F4yDnfGrN4AaABAg",
		"username": "Tanmay Khattar",
		"text": "What is the music at the end?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyKdtAqMaW4MRPV97d4AaABAg",
		"username": "andybaldman",
		"text": "Did Pinker ever see or give a response to this?   Rob, you have articulated my exact thoughts with respect to the errors in Pinker's arguments.  THANK YOU, and I'd love to see a reply from him.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgwQtO6s3PMcSJJIWwh4AaABAg",
		"username": "ExaltedDuck",
		"text": "\"When will AI exceed human performance?\" is the wrong question.  When we look at recent history and development, it's more a question of when human performance will become less economically viable than automata... and my honest belief is that this next stage of the industrial revolution (and perhaps the final existential crisis our species faces) is already looming on the horizon, maybe decades at most.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgxpgbPs7DQIiVrkQAl4AaABAg",
		"username": "scientious",
		"text": "We're talking about AGI and ASI, but Cornell didn't have any experts on that, so they tried to fill in with AI experts. 50% chance of having AGI within 50 years. I suppose that's not too bad for a guess based on nothing. What would a more accurate estimate be based on progress in AGI research?\n\n50% probability\nAGI Theory by 2021, Hardware by 2027, ASI Hardware by 2039 \n\n75% probability\nAGI Theory by 2025, Hardware by 2031, ASI Hardware by 2043\n\n90% probability\nAGI Theory by 2035, Hardware by 2041, ASI Hardware by 2053\n\nSo this is 8 - 22 years for working AGI hardware, although there would be some fairly drastic and immediate changes just from the publication of the theory. However, you also talked about the idea of apparently robots replacing humans. That's more complicated. Just in terms of the brain or control portion you would need something small enough to fit inside a human-sized robot. That won't happen in the first or second generation. A generation is estimated as six years, so three of these would 18 years. We can just add 18 years to the above estimates for AGI hardware. 50% probability would be 2045 and 90% would be 2059. That's 26 - 40 years.\n\nOf course, having a control unit isn't the only problem. Today, we don't have a power source for good mobility and it is unlikely that batteries will get much better. That probably means some kind of flammable fuel. But there are still problems with a durable covering that would still allow touch sensitivity and there is the speed vs torque problem if you use direct drive motors (as most robots do today). I can't accurately estimate when or if these could be solved since I'm not a robotics engineer.\n\nThe next question is even if the robotic body problems could be solved how likely would they be to replace human workers. The minimal cost for a control unit would be $40,000 in today's money. A human-like body would cost at least $400,000. That isn't going to replace a $10/hour employee at Walmart. Of course, you wouldn't need that for something like stocking. A mobile pick and place robot with a single arm would work. This would be fine in an AI context if you could build an AI smart enough to do the task. In an AGI context this almost certainly would not work. However, if you had an AGI with an environmentally simulated interface then you could probably implement it as a remote unit. That would only work as long as AGI units were legal property, much like slaves.\n\nExtinction of the human species. Could you explain exactly how this could happen? Preferably something that doesn't involve an ASI magically collecting resources and magically controlling people. The two most destructive events in recent history were the Spanish Flu and WWII with similar casualties. Neither one of these came close to wiping out the entire species.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgygCb5iVjqnmodK-Pt4AaABAg",
		"username": "Mark Batten-Carew",
		"text": "So the issue  of AGI development being dangerous can be boiled down to AGI having a different terminal goal than the average human.  Isn't it true that if any two entities have different terminal goals, they will be in conflict, if only due to competing for resources, AND the more intelligent of those entities is going to succeed, likely to the detriment of the other?\n\nTherefore, doesn't the solution to dangerous AGI lie in making sure the AGI's terminal goal is aligned with mankind's (or people in general)?  The tough part is defining human's terminal goal.  That requires getting past a lot of propaganda (like people are naturally competitive and it is everyone out for themselves, which is obviously false, or why would one person die for another?).\n\nSo a key element of making AGI development safe, is researching what human's terminal goal is, then letting the AGI be as smart as it can be, helping us all get to our common (human+AGI) terminal goal.\n\nI would suggest humans terminal goal is somewhere in the neighbourhood of \"experience the universe from your unique perspective\", and we have developed some instrumental goals along the lines of \"it is easier to experience the universe if we support each other (our tribe) to do it\".  Yes, some people try to go it alone, or become dictators to force others to help them with their own experience, but that is where ethics and morality get invented to try let everyone respect everyone else.\n\nCan we give our AGI a terminal goal congruent with humans, and some unalterable instrumental goals, such as \"respect all others right to their own path\"?  Then let them do that as best they can.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzmCEfvddMfqFzXvD14AaABAg",
		"username": "Christopher Willis",
		"text": "8:34 Is that kind of an informal way of stating P\u2260NP?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyidFr1jKIDIe_CFjd4AaABAg",
		"username": "INSTALL GENTOO",
		"text": "made me thunk. How about making a AI with a terminal goal of increasing the living conditions and optimal food and water for every human being on earth?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugi3ow-20hA6h3gCoAEC",
		"username": "Christopher Fern\u00e1ndez",
		"text": "So this would be somewhat similar to the White Christmas episode of Black Mirror?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgyJtQFfyJ_ZP0Fx3xF4AaABAg",
		"username": "Jupiter Belic",
		"text": "Is it okay that I want to marry you?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxJgG1k_dx8R9h4Pwx4AaABAg",
		"username": "John Wilford",
		"text": "Why don't most humans change themselves into utility maximizers then?\nI'll agree some do and that's where we get various mental disorders but why is it the exception and not the rule?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugw_AL9n0tQHMHu_JB94AaABAg",
		"username": "The Happy Greek",
		"text": "Where's the followup video?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzvVPJEdATnhM2SHWx4AaABAg",
		"username": "Cameron Drew",
		"text": "John Richardson??",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugz7cE6347RIlDAZE-J4AaABAg",
		"username": "Yan Yan Lloyd",
		"text": "Put simply, ''Christianity......... what's in it for me?''",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwiMdmAZwHPWdtN3U14AaABAg",
		"username": "Simon G.",
		"text": "are there approaches to modularising Neural Networks? In classical programming, a lot of diskussion is held about how to make programming languages that let you build large systems out of small ones. \n\nis the dataflow inside neural networks so large that the interface of an AI system would be incomprensibly large too? In classical programming, one solves this problem with local and private variables, which might have a neural analogue.\nIt seems natural to me that this would be the next step for machine learning systems to take.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyPUeZY9yooueXDhhl4AaABAg",
		"username": "Clark Mitchell",
		"text": "Did you just prove N = NP?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxQJhvrUl9HI-IYGoF4AaABAg",
		"username": "UsenameTakenWasTaken",
		"text": "What if we form an AI cult with the goal bringing about the end of the world with their shiny new metal god?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgwsHKxGGfJ8jXh2Rqt4AaABAg",
		"username": "Lordious",
		"text": "Next is how AIs can manipulate humans to get their rewords?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzwClJevkShfz0HVd94AaABAg",
		"username": "neoqueto",
		"text": "Uhh, maybe it's a metaproblem, best solvable by AI?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyfyLTO5kXoNfu4Rol4AaABAg",
		"username": "Qed Soku",
		"text": "What if you just used more layers?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyxrgWTztOZ3ZBAFR94AaABAg",
		"username": "INERT",
		"text": "How do we mesh all the specific AI modules or machine learning algorithms we've invented into one usable framework? Like GPT3, IBM's Wilson, the code that recognizes cats versus dogs, and maybe toss in some software that writes it's own music -- all interacting with each other to perform more generalized tasks.\nIs that something that's being researched yet? Is that the tactic? Mashing thousands of specific ML algorithms together into a framework that feeds them live data streams? Or is there a different strategy?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugyr8Gp--87cd6G5xeN4AaABAg",
		"username": "Daniel Cassell",
		"text": "What if instead of using the number of stamps as the utility function you used the log of that number?  This would lessen the willingness to take gambles.  Lets say the utility function is log(x+1) where x is the number of stamps collected.  The \"+1\" avoids shenanigans relating to negative infinity.\nLets compare guaranteed 100 stamps to a  1% chance to get a billion stamps.  The normal unbounded method would take the expected 10 million stamps over expected 100.  But my way, 0.01 times Log( 1 billion and 1) plus 0.99 times log(1) gives 0.09 whereas a certain 100 stamps would give log(101) or 2.004 that's a huge advantage.  Using a log would eliminate risky behaviors and all apocalypse scenarios are risky and you want to avoid as much as possible the outcome where the machine either ends all human life or fails to deliver any stamps.\nIf the stamp collector still chooses an apocalyptic scenario using a log utility function it must have such startling confidence in its ability to take over the world that the issue at that point may be the world more than it is the AI.\n\nEdit - Fixed calculator error, inconsistently using log base 10 and natural log.  Either approach works for my purpose so long as the user is consistent.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugyn43-wc9NXG43OH2t4AaABAg",
		"username": "chris4072511",
		"text": "Not directly about this video, but something that occurs to me about \"the singularity\"; \n\nWith things like Deep learning, we can build a machine that learns to play Go to a high level, but we still don't know how to analyse Go at that level. So, is it necessarily true that a machine with a high level of AGI will know how to build a better AGI? It may well be able to make improvements, but if it doesn't understand the details of what makes AGI work successfully, there is no reason to expect the runaway situation.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxOpe96y3kYHbt2R_h4AaABAg",
		"username": "AnarchoAmericium",
		"text": "Does religion, where one is \"encouraged\" to donate money to get into the afterlife, count as a Pascal's Mugging?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyVaJhvUN1mbmE6_9B4AaABAg",
		"username": "Nezuko's Guardian",
		"text": "Are we just going to ignore that this guy wrote backwards?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugwx3tZbGcI7ayd0xcJ4AaABAg",
		"username": "Wiktor Migaszewski",
		"text": "why not to bind stamps number (exactly 100) with economy of reaching this goal? we want to relax just because of this reason - to save resources; this is clue of relaxing! :)",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=Ugwx0bq9vMgW9oWKqgR4AaABAg",
		"username": "Julian Danzer",
		"text": "what if instead of giving unknown/exploratory plans a 0 or extremely high value you jsut give them a slgiht bonus\nlike the expected value plus 3% to encourage exploration?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugxpld1JMtvol4yRNul4AaABAg",
		"username": "gavin",
		"text": "how can you simultaniously have the best hair and the worst beard?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgxHqW6urXKC7sOwWcN4AaABAg",
		"username": "tomatoso27",
		"text": "I like the direction your channel has made, but have you made the other one about your projects?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzAlbv2yJbOhQOJqVZ4AaABAg",
		"username": "aednil",
		"text": "can a program even change its own source code? the changes can at least not take effect while it's running. right? you could just automatically overwrite the AGI with the original code every time it shuts down. or you store the AGI on hardware that can only be written to once like a CD /DVD /Blue Ray.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzoHPN8PBDltsg14L94AaABAg",
		"username": "spacedoohicky",
		"text": "Really the vast majority of people in science would probably consider a Turing test the way to identify intelligence because we would then know for sure that an AI was intelligent because it can mimic a familiar intelligence. Otherwise how can we grade the intelligence of an AI? We couldn't. We would have to remain agnostic to the assumption of intelligence because unfamiliar intelligences are outside of human understanding. So the AI who only collects stamps could very well be stupid, or super intelligent. We wouldn't know even if it dominate us in the interest of collecting stamps. I think that's where the intelligence assumption fails. Stupid things dominate us all the time. People die from the flu and cancer. Something doesn't need to be smart to dominate. It only needs brute force. Heck regular PC computers already outclass us at doing math and those are just plain stupid. Brute force outclasses intelligence the majority of the time.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugwx0N65MB1j0IVRZV54AaABAg",
		"username": "Clara Bisson",
		"text": "Comment for engagement",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgydhPQoGIj8y_32wXB4AaABAg",
		"username": "David Wilkie",
		"text": "The mind-body question of Neuroscience is necessarily focused on the human brain but if that's combined with the Human Culture-Gaia hypotheses and how complex objectives fit into a Multiverse..., the principle of coherency of multiple systems?, instead of the tendency to imagine \"separated individuals\" out of sequence with the whole?\nWe're prone to hope for the best but expect the worst, and not much happens anyway because \"the more things change the more they stay the same\".\n\nThe well known problems with individualistic humans are the same as AI, by extension in global inclusion.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgxYOAGgVcUCZu1x8_h4AaABAg",
		"username": "J M",
		"text": "Professor Miles, I wonder if a lot of this AI safety research can be applicable to political systems and how we can trust politicians.  Do you know of any connection?",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzH6YMJ--HWV9gkZt54AaABAg",
		"username": "Daniel Darroch",
		"text": "Isn't the idea that general intelligence is one-dimensional one of the most striking results to come out of psychometrics? It seems weird for Pinker to be arguing that it's a broad oversimplification when many instances of factor analysis have been done and all have determined that there is only one dimension of general intelligence.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyuRGGAtJl5meN8hVt4AaABAg",
		"username": "musingartisan",
		"text": "Is it only full AGI that we need to worry about? Or would lesser intelligence also pose a problem? For example, we already know that algorithms are better than humans in many scenarios. If one of those algorithms come to the conclusions that humans are an interference in it's goals. Like humans are the main cause of accidents on roafs, so to improve road safety we eliminate humans from public roads. So we end up with self-drive cars driving at humans to scare us off the roads.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugxr2RWOwB1x5bSUk9d4AaABAg",
		"username": "Guilherme Feyh",
		"text": "Wright? Wrong! Plane was invented by Santos Dumont and he flew in front of a crowd in Paris. Why do Americans insist on that story?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=Ugy8Lec1GKYQJlfAGeh4AaABAg",
		"username": "Maxim Gwiazda",
		"text": "Now let's consider the possibility that our reality itself is a simulation. Would AI trained on such reality be able to perform miracles by exploiting bugs in the simulation software?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=UgzXMophaPLF6nXMmfR4AaABAg",
		"username": "Edoardo Schnell",
		"text": "3:53 and all along... How come the eyes never move? Is it related to the input photos or to the output of the network?",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugzv8Jhr2nMr05zCT714AaABAg",
		"username": "Iliyan Bobev",
		"text": "Still the \"stamp collector\" example is flawed. Can highly intelligent agents have stupid goals? Yes. The problem is with the interface - the example assumes the intelligence is applied only after the query/request is placed. If it's a general AI with speech interface, it should be capable of answering to queries like \"I want a bear\" and \"Describe how a human would act if requested a bear\". If the AI is not able to consider context, or inquire additional details on query/request, than it will be failing a lot of simple tasks. Assuming it's not failing those simple tasks, we have to conclude, that it's due to considering the context, and matching human expectations.\nYes, you still could trigger it to do end of world scenario things, but you'd have to request them explicitly. The interpretations of the requests would give \"intelligent\" results, only if the AI takes a human perspective, while executing them. So \"Describe how a human would act if requested the best stamp collection\".\nAlso, would you consider something as being intelligent if it cannot cancel/halt a previously made query/request?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgxAZwtndLr1AufoV-14AaABAg",
		"username": "wokeupinapanic",
		"text": "Why can't you block the reward center from the AGI performing the task? That is to say, create an AGI that is tasked with cleaning a room, and the more it cleans, the more reward it gets, but instead of it being able to give itself the reward through manipulation, a second, less intelligent AI is tasked with being the observer of the environment as it changes, and rewards the cleaning AI as it does thing correctly? Almost a \"conscience\" guiding the AI to do the correct task, and not allowing it to highscore off just putting a bucket over its sensors??\n\nWhile our goal of AGI may be a single program that can run all tasks, I feel like these types of issues need to be compartmentalized and handed off to specialized units that can counterpoint each other.",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugz5bleckkEid4FdJIR4AaABAg",
		"username": "Batrax",
		"text": "Why not taxation + Universal Basic Income?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwziUeOMzju77XNJth4AaABAg",
		"username": "Splosion",
		"text": "Hm I wouldn't do a \"what if/ couldn't you just...\" series, itd most likely be end up as a documentary into insanity and alcoholism",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyqIXLmgK28686tHtl4AaABAg",
		"username": "bruh dude",
		"text": "was thinking about ai safety and wouldnt it work to just have the ai always ask us for permission to do anything? We tell it \"collect stamps!\", it says \"can i do it this way?\", we say \"no\", \"this way?\", \"no\", \"this way?\", \"yes\". So it comes up with a plan, and we either accept or decline it. Then, having a neural network monitor which plans we accept and which we decline might also allow the ai to learn human values.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugxstak8BCFxqABKV0B4AaABAg",
		"username": "stribika",
		"text": "Can we go one level up? Instead of a human picking which clip is better, an AI takes the same text description of the task that the human would, and provides feedback for a wide variety of reward models based on that. A human compares the end results, and provides feedback for this second order reward model.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgwyLyHtYyr17pYfPGd4AaABAg",
		"username": "Jonas Th\u00f6rnvall",
		"text": "Hello Robert waiting for your next video, i can tell you what i would like to know about and see a video about, how come the AI's in the formal studies have so little \"none\" formalised knowledge? Why are they basicly just bruteforce automatas, that really do not know what they are looking for? I Just saw the new computerphile video i always felt the lack of scope in the demonstrations, but  i understand they are for very specific use.\n\nhttps://www.youtube.com/watch?v=TJlAxW-2nmI\n\nBut if an AI do have a knowledge base like categorisation system \"maybe you call that an expertsystem?\" i would call it an logical inferator,  wouldn't that make a more powerful AI. An AI that have knowledge about the \"thingy\" it looks for?\n\nYour type of AI just can refer to type names  based upon billions of pictures, it have no concepts of cats and dogs \"or language or the world\"? Wouldn't an AI or expertsystem, logical inferator that knows the concept of fur, body, head, legs, ears, nose, size, colors. Be much more succesful?\n\nI guess i ask why the academical AI's are so one dimensional and specialized? Is it the terminator fear again ;)",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyT7Jzpl7X97G3E1wh4AaABAg",
		"username": "James Williston",
		"text": "What happens if the path to AGI requires a step where programming it involves developing an AI that self improves, and as a result the risks for employing programmers becomes higher than the benefits of having one can justify?\n\nAny barrier that involves employees taking a stand won't work in a scenario where jobs are being lost to AI.\n\nOne method that might work is to form laws placing AI systems in the public domain, though now the limitations are in access to hardware to run these AI systems. To create true equality you would need the poorest person in the world to have free access to these capabilities, all of which require some education to take advantage of.\n\nNo matter what, there's going to be some kind of barrier between some people and the benefits of these systems.\n\nIn the present day, large companies can share a blacklist of people they will not hire, and the reasons for adding people to these lists can simply be whether their actions cause the company indirect harm. An employee who worked for Disney many years ago was part of the \"furry fandom\", and they were told that if they didn't leave it, they would never work in animation again, anywhere. They chose their job, because who would choose a hobby over their job?\n\nIt's alright for now, employees still hold some amount of power when they are hard to replace, but the closer we get to AGI, the closer a programmer comes to having the same power as a cashier.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=WM2THPzFSNk&lc=UgyOy-5ktitpDYeYs854AaABAg",
		"username": "Damian Reloaded",
		"text": "u alright?",
		"title": "Friend or Foe? AI Safety Gridworlds extra bit"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzOb3NnXHHsxeeSF9l4AaABAg",
		"username": "imkharn",
		"text": "If you control researchers with laws, it guarantees the only group to get AGI will be those above the law.\n\nDo you really want the powers that shouldn't be in control of (secret) AGI?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugg1nDYroGp1SXgCoAEC",
		"username": "Reckless Roges",
		"text": "Could your blinding laser be used to defend the UK from Asian Hornets coming from France?  Could it be used to efficiently and safely be used to kill flies in kitchens? (Also, is the code/designs on github?)",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw1L1bJCyC8Adotxvp4AaABAg",
		"username": "14zRobot",
		"text": "Make sense. If we analyze human intelligence, it will not be hard to find 2 people with similar reasoning skills (or whatever factors you like) and find out that they have a lot of different goals. Why that should change with increased intelligence or more complex goals is unclear. AI will have much better reasoning due to ability to work in much more dimensions than our brain. But that is basilly it. \n\nQuestions like what is moral/immoral are irrelevant since morality is determined as the result. Example - if you start some project to help people, but it turned out to hurt them. Was that moral?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UggkglEG7IFCQXgCoAEC",
		"username": "Wellington Boobs",
		"text": "Do you agree with Elon Musk's theory that, because of the rate of progress now being observed, we are not in fact in baseline reality but, rather, something like the 13th Floor film is what's really going on? If time is all 'out there' -- a dimension we experience differently than the spatial ones, as physicists say it is, but still there -- might we at this moment be experiencing the moments from an infinite loop in which we cannot do anything but create that simulation which makes us feel and be real?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgyRvbIZtK8v-nSccbF4AaABAg",
		"username": "\u13f0\u012a\u13dd\u13dd \u0547\u00ce\u03c1\u0267\u13cb\u01a6",
		"text": "How about we work on human enhancements instead of trying build another general AI ?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxZ-adk5Ev4xuXT4lV4AaABAg",
		"username": "Donald Hobson",
		"text": "In a world where AI's have basically all the power and resources, and humans are only relivant to the extent that the AI's are programmed to obey humans, how does this effect anything? \n\nScenario 1, strong singleton, a single programmer, or maybe a small team make an AI that is far more powerful than the rest of the world. The AI follows whatever moral procedures the programmers implemented. We get a world chosen by the morality of a group of AI designers.\n\nIf someone has an AI that does what they want, then this agreement isn't enforceable against them. The people who would follow such an agreement were probably going to do nice things anyway. \n\nThe only way such an agreement could effect the world is if it were programmed into the AI's  when they were built. But for that to be enforced, rather than out of the goodness of the AI builders heart, the legislators (in practice bureaucrats, judges and managers) would need to understand, and be able to control the AI code. \n\nI actually think that the amount of resources that an advanced AI could produce are vastly more than what one person could use on themselves. At some point, you have everything that you want personally, (except things that the AI can't change no matter how much resources it throws at the problem.) and the only thing left to do with the remaining 99.99999...% is to give it to others. \n\nAdvanced AI is powerful enough that if you have any preferences at all about the rest of the world, they will be largely satisfied. And if you really don't care in the slightest, then the rest of humanity is made of atoms that could be used for something else. \n\nI think that the coordination problem is that different people have different ideas of the global good. If you firmly believe that simulated human minds aren't morally important, and I think that they are, then you see me as proposing a world where all humans are killed off, and replaced by endless boring computers. I see a vast amount of inefficiency, possibly pointless death if the real physical humans still die sometimes, and vast amounts of suffering if you decide to make really realistic computer games.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgyZA207t9qOKxxDc_F4AaABAg",
		"username": "JAWWAD",
		"text": "Am I the only one with an itch because of his mustache design? \ud83d\ude02",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugzy8xWlwqoupUNbAx54AaABAg",
		"username": "Rickard M\u00e5rtensson",
		"text": "How would blowing up the moon reduce the AIs influence on the world? Sounds really interesting however i cant understand the analogy \ud83d\ude0a",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzKtoi_EqjV8Js-k0d4AaABAg",
		"username": "yoppindia",
		"text": "have you applied this in any of this in your life?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzHfMGAakDCg2A3Sv94AaABAg",
		"username": "Perry Brown",
		"text": "I remember (unknowingly) using an \"anti-god\" argument when I was ~14 against a highly religious friend. I said, \"but what if XXX is just using all this god bulls**t to separate the gullible unworthys from the true chosen ones... What are you going to do then?\". He became a bit anxious and disturbed by this line of reasoning... (I guess they hadn't yet gotten to that chapter in bible study.)",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwDcP31NB9gup1UZY14AaABAg",
		"username": "Zhab80",
		"text": "The thing is... if the wealth of the worlds end up in the hands of 0.001% of the population or something and everyone else is out of a job... Yeah ? Where are the customers ? Who are you going to sell whatever product or whatever service to ? What kind of business can possibly survive with a customer base that represent 0.001% (or something) of the population ? What kind of profit numbers can you possibly expect ?\n\nFurthermore, will money less people just give up on life and just lay down to die ? Would they rather revolt and that whatever they need to survive ? Would money lose any meaning to the vast majority of people and we revert to autonomous small scale homestead farming with simple trade bartering ? People don't actually need money. People need survival. Right now you can buy survival with money which is currently reasonably \"easy\" to get. What value does money have in a world where it is nigh impossible to get ?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgySHeeP9BbvL8TAPnd4AaABAg",
		"username": "Harry Ayres",
		"text": "Could influence be quantified as an SI unit? For instance, the ability to impart or extract 1 joule of energy with regards to 1 gram of matter in 1 second constitutes 1 influence unit.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgxWPr4Wj9trkWhmHFl4AaABAg",
		"username": "Andew Tarjanyi",
		"text": "What system of \"thought\" is responsible for causing people to believe that the education system produces intelligence when it produces nothing of the sort?  This channel is a perfect example of the absence of intelligence where such a claim is not met with curiosity and an open \"mind\" as is the case with authentic intelligence but received as nothing more than an insult.  I contest, as I always have, that intelligence does not exist as part of the scientific or academic output.  Furthermore, were that not the case then modelling what is likely to occur beyond the technological Singularity, far from being impossible would be a mere formality.  So why is it that this channel is unable to undertake such a modelling challenge?  Because as previously stated, the education system failed to provide the necessary intellectual instruments to attempt to do so.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgynemzHPoIP28dfp314AaABAg",
		"username": "MarioSacTron",
		"text": "Hello Mr. Miles! I had a question, I work for a small startup as a software dev, I\u2019m also in school full time, and I was wondering what recommendations/advice do you have for someone who\u2019s starting to take a very big interest in the field of AI/deep learnings/ machine learning? I\u2019m taking an online class in machine learning and it\u2019s an absolute blast. I\u2019ve purchased a textbook on machine learning and have a couple more I\u2019m looking to get the further I get along. Any really good books you\u2019d recommend? Any openly available classes/talks? Any information in general would be awesome. School currently is really slow and doesn\u2019t provide any information into the field so it\u2019s difficult to figure out how to further my knowledge. Thanks for your time!",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxbGPhHdjVT3b3WC5d4AaABAg",
		"username": "Alexander Kirko",
		"text": "The problem, in my opinion, is that people have a very hard time imagining what an AGI would be like. We are used to human levels of intelligence and processing speed, maybe someone heard about the youtube algorithms or whatever. But it's hard to imagine what it would be like if we make a robot that's 5000 times smarter than the smartest human, connect it to the Internet and say \"make sure I always get my coffee in the morning\". It might start enslaving people or put you into cryostasis (which it will invent in 1.3 seconds) or anything else, really. People just imagine AGI as something anthropomorphic, and it has no reason to be that, not unless we develop it in a simulation where it will think that it's a human.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwwGWX0PkXQiweWozp4AaABAg",
		"username": "Jordan McMeow",
		"text": "Curious to know -- because I obviously don't -- do people still think of a deity as an omnipresent extraterrestrial being that they just can't see? Is this more of a UK thing? How is it not more productive to view Gods as abstract representations of cultural values? Cultures implicitly construct God(s) to represent unified representations of beliefs. It's self evident that cultures (familial to large scale) hold certain values in high esteem. I get where Robert's stance comes from, but the scientific or logical disputes for the existence of God is ill-suited unless your conception of a God is this narrowly underdeveloped. For many people it is, and that's why these arguments are useful to keep around regardless. The more experienced you have abstracting out ideas on the topic, the more a scientific refutation of God(s) seems like juvenile self-indulgence of straw-men. \n\n\nMore to the point of the video though, it's fascinating to see Pascal's wager presented in such a way. I think Roko's Basilisk fits in here too. Roko's Basilisk is a thought experiment which entails a super intelligent AI agent which will retroactively punish anybody who fails to aid in its creation. Basilisks are foul play for a lot of philosophical conversations, but it's actually quite relevant here where we're talking about the benevolence or malevolence of future artificial intelligence.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxActfib18u82yXYfF4AaABAg",
		"username": "Broockle",
		"text": "uhm.. what have I learned from this video......?\nThink about the Future, but don't Panic about it...? Really not sure if this video covers the title of the video lol",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugx8KwTkg-clyaKpmRd4AaABAg",
		"username": "Thomas Buckler",
		"text": "is your beard a terminal or instrumental goal?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugyk8kWg_3f3SQfExdF4AaABAg",
		"username": "george",
		"text": "You didn't factor in mental illness. What happens if that big corporation is a psychopath?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=Ugi0Qq8qnq3fGXgCoAEC",
		"username": "Tomer Mardan",
		"text": "does this channel have a subreddit?",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyzMDRq5XdJjhCNppN4AaABAg",
		"username": "Michael Spence",
		"text": "So, you're only mostly right when you say that modifying human values doesn't come up much.  I can think of two examples in particular.  First, the Bible passage which states, \"The love of money is the root of all evil\".  (Not a Christian btw, just pointing it out).  The idea here is that through classical conditioning, it's possible for people to start to value money for the sake of money - which is actually a specific version of the more general case, which I will get to in a moment.  \n\nThe second example is the fear of drug addiction.  Which amounts to the fear that people will abandon all of their other goals in pursuit of their drug of choice, and is often the case for harder drugs.  These are both examples of wireheading, which you might call a \"Convergent Instrumental Anti-goal\" and rests largely on the agent being self-aware.  If you have a model of the world that includes yourself, you intuitively understand that putting a bucket on your head doesn't make the room you were supposed to clean any less messy.  (Or if you want to flip it around, you could say that wireheading is anathema to goal-preservation)\n\nI'm curious about how this applies to creating AGIs with humans as part of the value function, and if you can think of any other convergent anti-goals.  They might be just as illuminating as convergent goals.\n\nEdit: Interestingly, you can also engage in wireheading by intentionally perverting your model of reality to be perfectly in-line with your values.  (You pretend the room is already clean).  This means that having an accurate model of reality is a part of goal-preservation.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyXV8Hvw2CVG_8aNyZ4AaABAg",
		"username": "Pic Pac",
		"text": "An argument I hear often (even though it isn't an argument against AI safety) is: \"I think we should ban AI research altogether.\" Do you have a response to this ? Because people with this argument usually do not accept the counterargument of: \"Well, it will be developed illegally and less safely, then...\"",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyMPGkBPaPnNb7OHRt4AaABAg",
		"username": "Biel Bestu\u00e9 de Luna",
		"text": "Al? Who is this Alfred fellow you guys keep talking about? :)",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugz78PH2nursYCr_QNV4AaABAg",
		"username": "Anankin12",
		"text": "What was that frame at 0:52 or 0:53",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgwRocuBc_bsWkFOMt54AaABAg",
		"username": "e b",
		"text": "What r your ideas of hanson robotics and singularity net?  I would love to see a breakdown of what u think of each of the a.i. project in process.",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwxljLt33mDXUL5Lvx4AaABAg",
		"username": "Micheal Angelo",
		"text": "Why would a company with AGI care about PR?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgxFh6TPN01Ttfvh8wZ4AaABAg",
		"username": "Martin Verrisin",
		"text": "How is time related to world states? I can easily have a loop where A(t) > B(t) & B(t+1) > A(t+1) & A(t+2) > B(t+2) ... Now which do I prefer? ...",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwgYtIcVs0dcoUJAxV4AaABAg",
		"username": "PhilNEvo",
		"text": "What if you create a utility function that values more than one thing. E.g. Not harming a human gives 100 points, and getting stamps is valued based on a exponential population growth function [dn/dt=rn(1-n/k)] capped at 100, so that it will always be more valuable not to harm a human, but will still try to maximize its utility to  try and get 200 points, by getting more stamps. This way you could have a utility maximizer not run amok? \nI still see the issue with defining human harm-- but could it work?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugzv4jm2fazx9C6YQxp4AaABAg",
		"username": "Deaken Wylie",
		"text": "Not explicitly coding for the, uh, antepenultimate entry on the list at 1:59?  You, sir, are an unfeeling monster with as little concern as to the consequences of your actions as the AIs about which you are warning us.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxP-f6AGl4F1Pgcm0t4AaABAg",
		"username": "Alexey Kuznetsov",
		"text": "What will prevent a company from using a newly developed AGI to bypass the agreement?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugw2CQgGbhHBebQyWHZ4AaABAg",
		"username": "inyobill",
		"text": "\"I know it if I see it.\" If I see something that looks like a back-flip, how do I know it's not a front-flip?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwZ6hSYnu2IgJRAR2F4AaABAg",
		"username": "Warp Zone",
		"text": "Hang on a minute.  Why not just give the AI the ability to distinguish between terminal goals and instrumental goals.  Hard-code in the terminal goals at the factory, one of which is \"respond to the shut-down command.\"  Now it can't choose an instrumental goal that conflicts with that terminal goal.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugw0q6sQqJ2jSurtLK14AaABAg",
		"username": "Tera Star",
		"text": "what if the preference is set to use the least resources and time to get 100 stamps?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=Ugwx5R62BFPbFWmAsSJ4AaABAg",
		"username": "Sucio Tiffany",
		"text": "So what IS the worst case scenario for AI?",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UgxG_e5DQ22m4wIYSB54AaABAg",
		"username": "chris kaprys",
		"text": "Nicely done. Are you related to Jon Richardson?",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwjdA5eDpy0lkZE6F14AaABAg",
		"username": "neutrinocoffee",
		"text": "Would this be applicable to domains where you can't do tree search, such as StarCraft? Or autonomous driving?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugz5ya4zT9U6cCYxdGZ4AaABAg",
		"username": "HairOnFire",
		"text": "A human mind is just as much a general intelligence as any AGI will be, and we have essentially the same Instrumental goals and dangerously selfish motives. The only difference, and real danger, is that an AI might be able to become so much faster and smarter than every other agent that they'll be able to manipulate and control the entire world without repercussions. It's a simple power imbalance, and you think too charitably of human minds. \nA human in the same position would be every bit as worrisome. That sort of power imbalance is not something we should do until we're a whole lot better at being ethical, and having sound reasons to be ethical. If you think humans aren't so dangerous when we have power over other agents, all you have to do is look at our history or your own dinner plate to see the horrors we inflict on others without even thinking twice.\n\nA generally intelligent paperclip maximiser doesn't have any more, or less, reason to be ethical than a generally intelligent \"feed/reproduce/be-safe\" maximiser such as a human mind.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=2B-AyWA2_ZY&lc=UgiV5wSzeKIv1XgCoAEC",
		"username": "Everything Tech Review",
		"text": "as a researcher, don't you already have a good PC for training your ai? and can you tell us the specs?",
		"title": "Status Report"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugy8iiFoULSK2DqFqfR4AaABAg",
		"username": "Hazmat",
		"text": "I know this is a very broad rabbit hole but - Why?\nIf a company has developed AGI - equivalent or superior to human intelligence... why would they sign this? Why would they care what people think of them? They've \"Won\". \nThis isn't an issue of profits at this point. Profits are only useful when they can be used to alter your position in a hierarchy. If the entire dynamic of hierarchy is demolished... where the apex is a small number of powerful people and the rest is composed of AGI that does their bidding - what use do they have with humanity at all? A bunch of mewling dregs making demands on their genius? \nIt doesn't seem to me that there is a future where most of us have a part to play. It will be the elite few while the rest of humanity AT BEST is allowed to simply survive in whatever miserable capacity that looks like.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=Ugzoqm6Ta1rXL5HDQD94AaABAg",
		"username": "Dojan5",
		"text": "What font is being used in this paper? I'm slightly enamoured.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=PN-rC1PRuN0&lc=UggM4hnR1h_xfXgCoAEC",
		"username": "Ronan",
		"text": "Can you make more silly programming videos? Or is this a once off? ;)",
		"title": "MAXIMUM OVERGEORGIA"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwCXsp6YU2nuEK7v0Z4AaABAg",
		"username": "Dow DayJing",
		"text": "Is there a method to define check points along the way to a win condition so that an algorithm has to vary more smoothly in its behavior to reach the end goal?\n\n*Edit: Now I\u2019ve watched previous video of yours on reward modeling and it\u2019s close to what I had in mind. Thanks for having such an informative channel!",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwkuXoNqnEl-JT8qz54AaABAg",
		"username": "DiabloMinero",
		"text": "Modifying your mind to improve your intelligence is the point of coffee, isn't it? Unless you're my mother, who for reasons incomprehensible to me enjoys the taste.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugg-1zGcVtZlBHgCoAEC",
		"username": "Paulo Van Huffel",
		"text": "Our utility functions do change over time tough. Why should this not be the case for an AGI?\nLike I might prefer going to Amsterdam over going to Cairo before I went to both but once I saw Amsterdam I might now be more interested in Cairo and when I traveled there I might have liked Amsterdam more so next vacation I go back there. After 5 vacations in Amsterdam I feel bored with it so I might wanna try out Cairo again.\n\nThis doesn't seem stupid to me or conflicting with the 2 rules you put forth. My function is just a function of all my previous functions.\n\nThis kind of behavior might prevent your stamp collector gone mad example from overdoing it as at some point it should over time achieve a new utility function that hopefully is less mad.",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy1nIG8aRLM_gl8hSl4AaABAg",
		"username": "Umang Gada",
		"text": "Can we build an AI that can create other AIs?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxwbSzRQ6T0bm1an294AaABAg",
		"username": "Michael Morrison",
		"text": "What if it's meant to maximize understanding and apply understanding as obtained? We already have AI that can make AI. Sentence parsers, object and scene recognition to fill in the understanding of verbs and nouns as its learning and it gives birth to an ideally smarter AI.\n\nI've been thinking on this for a bit.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzgihDnr1QwpuLpqyZ4AaABAg",
		"username": "Fearghus Keitz",
		"text": "What if you give an ai the terminal goal of being moral, then let it try to learn what we mean by morality and whatnot, then tries to figure out how to most efficiently be moral? Would it do it well and how intelligent would it need to be to do it well?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugzs0qbnS-6l80UtSCp4AaABAg",
		"username": "Reckless Roges",
		"text": "For the pragmatists, can we aggregate all human terminal goals that are common to all cultures and ages? My first suggestion is: 1. no involuntary external deprivation of qualia, (covers \"don't kill me and don't take my stuff\". Does not exclude reduction of suffering.) #axiomsOfHumanity",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugyi9_Rp9gichahSYGB4AaABAg",
		"username": "Wiczus",
		"text": "Can you prove that ought-statements are separated from is-statements?\nHume was like \"I guess I can't find a link, so there isn't one\". That is not a rational proof. Statements that have at least one instance of being wrong are false statements. Self-preservation is a simple one: \"There could exist an imperative that we don't know. To complete that imperative, one must survive and learn.\".\n\nFew is-statements result in an ought--statement. Therefore Hume's statement is incorrect. \n\nThere is also a problem with your view on Terminal goals. You claim that if something changes your goal then it wasn't a Terminal goal. The problem is, that the terminal goal wasn't terminal only in retrospective. \n\nAnother problem with this general debate, and not particularly with the video, is that both we and NN learn empirically, therefore we are prone to incorrect conclusions. With the capability to change its code (analog to human brain), there exists a chance to overwrite a currently perceived Terminal goal. We often seem to fail at predictions, because we assume perfect systems, which they never are.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgzBtDQ_LsR7js9n4cR4AaABAg",
		"username": "Klaus Gartenstiel",
		"text": "would you rather want a super ai in control of an army of thousands of combat drones, or - say - a prince of saudi arabia?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyuTSJyy-KtFWytvJ14AaABAg",
		"username": "Florian Matel",
		"text": "Was wondering, what about accepting the fate that AI is indeed the next step of humans, that what ever we try to find to keep it aligned with our needs, with time, AI will be out of our control without the same goals of humans and assume that we are just the first step of another type of life which would avec less problems to grow. For exemple to move on to other worlds would be easier for them than us meat walkers ?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzrNIFji0xPq-Th4Kd4AaABAg",
		"username": "Cristian Garc\u00eda",
		"text": "What about a satisficer that factors into its utility function the amount of energy spent in the process of fullfilling it\u2019s primary goal? this way it could predict that the maximizer would screw up the energy requirement and be incentivized to not become/create it.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=Ugy3UzKxb9XW1BugqQl4AaABAg",
		"username": "Adam Fryman",
		"text": "Our caps, they've got skulls on them......  \nAre we the baddies?",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugx7UvcePkMwQ5Gug454AaABAg",
		"username": "Seegal Galguntijak",
		"text": "What about the task of healing cancer or HIV? Could we use reward modelling to task a complex neural network with that? Could it possibly cure aging?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgxdFHYKAGzGObRpGGZ4AaABAg",
		"username": "slikrx",
		"text": "Silly question, have you ever seen the US show \"Person of Interest\"? the last 3 seasons deal with AIs \"at war\". (the first 2 seasons involve an AI, but the overpowered aspects are not emphasized)\u00a0 It's a good show as far as thriller shows go, but is a bit silly with the AI. The characters involved are (mostly) aware of some of the consequences of an unbridled AI (or ASI - artificial super intelligence, as they describe it) but still awkward in how it's handled.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwS23sZX9WKnVtVQ0J4AaABAg",
		"username": "Derpy TW",
		"text": "why dont we just do atomic fusion for all our energy. its not like its hard or anything. the sun does it all the time. have you SEEN the sun?\nits just a big ball of fire. cmon. its so simple",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzReLy96AGZeczw6u54AaABAg",
		"username": "guai",
		"text": "Are there even chances that we won't end up with Artificial Psychopath?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzAVtetndcvflfA6CB4AaABAg",
		"username": "Konsta Peltoniemi",
		"text": "Now, couldn't you say that for an AI, maximising the reward function is the final terminal goal it has?\nSimilarly, if biological intelligence is in any sense analogous, drugs/spiritual enlightenment are reward hacking for achieving \"the\" terminal goal.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugyy_kRNMHS-6TQTvkl4AaABAg",
		"username": "samtheweebo",
		"text": "What about limiting the agi to analysis and not action. Like limiting it so it can not take any direct actions or make any decisions. Just make it present data and conclusions to people who actually take the actions or make the things happen. For example task the system to design the best car it can, but don't allow it to actually build the cars or put any software into the cars. Just have it make the designs and then have people make the cars. With the stamp collector ai just don't let it actually order the stamps or print the stamps at all, just have it instruct the user how to get the stamps.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwTME5T_9oRN3PJ9L14AaABAg",
		"username": "DrTryloByte",
		"text": "What about an expected utility satisficer with a negative utility for self modification / building another agent?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwRIp7PPK25eVzBTLZ4AaABAg",
		"username": "Canzandridas Joe",
		"text": "Could an AGI become addicted to something? Like processing power for example but not necessarily just that?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgxtNgeEJ2jE_-tccZB4AaABAg",
		"username": "Midhunraj R",
		"text": "how this is different from Generative adversarial networks?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgxVTBo3JUXCQ_1WIel4AaABAg",
		"username": "LordCAR",
		"text": "Can you please add references to the paper with the panda, and also to both websites you showed?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=Ugx6lugMYZI4v0f4Agh4AaABAg",
		"username": "David",
		"text": "wait is that tron on a funny sounding instrument?",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzzunxMPbNqV9dWRTV4AaABAg",
		"username": "Greybeard",
		"text": "Could you have predicted your facial hair would grow in so scraggly? \ud83d\ude1c",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwGxsLNa8T_wwgZBTV4AaABAg",
		"username": "Jack Kraken",
		"text": "Who would have thought that AI and teenagers think the same way. Lol",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyYH9a-SJqxua2IJTB4AaABAg",
		"username": "Jimmy Gervais",
		"text": "Why does intelligence oughta have a goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgweySv1UwQORcf1WOx4AaABAg",
		"username": "Gundam Serpent",
		"text": "How hard would it be to make a \"Reward Training AI\" meant to train other AIs and 'reward' them when they do good?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxI_vOulGwgiubTICF4AaABAg",
		"username": "Andew Tarjanyi",
		"text": "Robert Miles\nI am not overly impressed by your definitions.  For example:  why use the term \"reasoning when terms such as  \"calculating, negotiating or navigating will suffice?  And itself, the term Intelligence is systematically ill-defined as I have previously stated.  Why is it commonly believed that behavior requires intelligence?  Chemicals behave according to their environment, do we attribute intelligence to those too?  Common flees display behavior.  Do they qualify as intelligent creatures?  If not, then what qualifies human cognition as a product of intelligence?  Is it possible that in some humans, intelligence is present and in others is absent?  If so, which is almost certainly the case, how does one distinguish between the two.  If there isn't, then I suspect that we are all fucked.\n\nActually, a more descriptive term to use in place of \"Artificial Intelligence\" which describes more precisely what is happening within such machines is synthetic imperitive systems.  Because that \"is\"  precisely what you are describing here.  Imperatives are the input and behavior is the output.  Intelligence has never in itself been responsible for the survival of the human species but genetic, biological and social imperatives, none of which are a function of nor require intelligence.  The mistakes and errors have emerged through misinterpreting the complexity of those behaviors associated with those imperatives and conflating it with intelligence within a self-validating circular and self-reinforcing pattern of reasoning.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgycTK7bzEdEqusOpAl4AaABAg",
		"username": "Matthew Weston",
		"text": "What is he saying at 5:45? The autocaption labels it as \"rather foot insular\", and I can't parse it no matter how many times I play it back.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyQP1evN7GA-iq8AZN4AaABAg",
		"username": "jmalmsten",
		"text": "Are we not just punishing these AI for creative out of the box thinking? :P",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwLjLrmCklPaQT9JAp4AaABAg",
		"username": "DAve Shillito",
		"text": "So, you are using your patreons to provide feedback on a draft of a video on using feedback in a learning system, so you can improve the final video?\n\nThat's getting a bit meta isn't it ;)",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugyv1Iv-RMdFe5tuTAh4AaABAg",
		"username": "Scal Pol",
		"text": "Great video, but what is that little golden stain behind your head? Please, given you probably are interested on a happy audience, make an instrumental goal to remove that annoying thing and take actions accordingly. Thanks.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugx9POBccNKAe_2GBOx4AaABAg",
		"username": "Mit",
		"text": "Actually I don't see how with those two assumptions self-preservation follows. It seems like a third assumption to me. Can someone elaborate why having a goal 'magically' leads to self-preservation?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UghXq6CwLLwrvHgCoAEC",
		"username": "Michael Gro\u00dfe",
		"text": "What's going on with the turkey at 5:30?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugwy4NzaPTKWFU0mDMZ4AaABAg",
		"username": "Lutz Herting",
		"text": "Why make the Pascal's Wager thing so complicated? If you follow the Christian god, Zeus will be pissed and throw you out of Olympus f\u00fcr all eternity. Since the chances for the existence of the Christian god and Zeus are equal, the wager already falls apart.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgxnySKmbwb3k9RkrY14AaABAg",
		"username": "D Whitehouse",
		"text": "Hey bro, big fan of your videos, but I haven't been able to figure out, are you a student, professor, what?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugwdx8gf3nzuK_jTkcR4AaABAg",
		"username": "Jayyy Zeee",
		"text": "How many times must one google the google before googling?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwiR1W5Gs3eRBBetYZ4AaABAg",
		"username": "JumperCzech",
		"text": "Couldn't you just cap the probability at like 95% so the AI gets the same utility from a 99% chance of getting a 100 stamps and a 99.99999999% chance of getting a 100 stamps?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=Ugz3FNvR66DW3EfMSQZ4AaABAg",
		"username": "Paul Steven Conyngham",
		"text": "where did you get the shot glass?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugx8vqcST_nPCZoSa0l4AaABAg",
		"username": "Joshua Coppersmith",
		"text": "What about a co-episode with Legal Eagle or Lawful Masses on this topic?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugz_ypeRoCAj5nvKjg94AaABAg",
		"username": "Randy Carvalho",
		"text": "Couldn't I just invent a similar system where a belief in god sends one to hell and being an atheist sends one to heaven? Equally unfalsifiable. Negates Pascal's wager, bringing the matter back to not believing making more sense.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzynI56kTOYNZxVjQp4AaABAg",
		"username": "RUBBER BULLET",
		"text": "When AI takes over the world, how is it going to create and maintain its hardware, its power supply and every other facet that humans so graciously provide?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwsdTRakdZgymwLg9V4AaABAg",
		"username": "PainSled",
		"text": "Read the title like this: \"Why Would I Want to do Bad Things?\"",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugw_syACoeTXOW9TM854AaABAg",
		"username": "inyobill",
		"text": "I don't see much discussion regarding another aspect of mass unemployment: If no one has a job, who's going to buy your machine produced widgets?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgwKB3DK4scvVkboQeh4AaABAg",
		"username": "Sion",
		"text": "Ai: {Hmm... how can I make this tea-water boil faster.... hmmm.... AHA! Nukes are pretty fast at heating stuff up... ok, let me just quickly hack those launch codes....} <-- it's a thought bubble",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyPTBl3DRzrVgpLEXN4AaABAg",
		"username": "VeeTee",
		"text": "Would terminal goal have some sort of feedback to the fitness of AI that possesses the goal? \nPaperclips won't really add anything to paperclip maximizer fitness, whereas AI might add to the AI maximizer fitness. Is AI maximizer then more fit as a superintelligence than paperclip maximizer is? What got me thinking here is that an AI might want to make copies of itself as an instrumental goal to whatever it really seeks. And it might want to really seek something that makes itself most fit - which is again making more copies of itself. And improving them along the way. I kinda like the idea of fitness here, it ties well with natural evolution. At least so it seems, I'm not an expert in this field by any means :)",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgymxNf2j04bns1fjYR4AaABAg",
		"username": "Red",
		"text": "Can't you teach a AI all about AI safety and feed that any plan you have for any robots and make that point out any flaws you made and offer solutions?\n\nIf it's just on a computer without any internet connections, or any other means to communicate other than a monitor, it shouldn't be able to cause any harm. At worst it will fail to pick up flaws, but it can't cause any problems by itself and will probably improve AI safety even if it's not perfect.",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgyQ5tSE1VZZOO4ZjkV4AaABAg",
		"username": "Matthew McClure",
		"text": "great video! could you recommend some links for further reading on the logic / theory of choice here \u2013 thanks!",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UghyHpTiXnN-M3gCoAEC",
		"username": "Sha0 Miller",
		"text": "Robert Miles: Won't we be judging AGI by how minimal its human-programmed bits are (general) and by how closely it resembles the human condition (because we're the de facto standard)?  If so, wouldn't that be less likely akin to a stamp-collector and more likely akin to a human child?  Also, are you not confusing \"superintelligence\" with \"general intelligence?\"  The stamp-collector might be very intelligent, but its focus on stamp-collecting hardly seems general!",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyFweMV03Jf1bEehn14AaABAg",
		"username": "asdfasdf71865",
		"text": "You start from simple learning material like facial expressions and end up to then more complex ones. Algorithms already learn our biases, what if moral code is just a bunch of biases? Currently child raising is the best example of intelligence crafting.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugywo1DBw4kshoNj_Y14AaABAg",
		"username": "Marian Palko",
		"text": "9:36 Perhaps some examples of such bizarre edge cases?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxYgAe5ipc8MW9s4ll4AaABAg",
		"username": "Raph Ael",
		"text": "If the AGI treats us in the way that we treat animals, then what would that tell us about our oh so superior human value systems?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx629EBI0d08K8sEWd4AaABAg",
		"username": "Herr K\u00fcrbis",
		"text": "When does the next video come out?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgwzVNqH6kDQmOpNqXh4AaABAg",
		"username": "AgeingBoyPsychic",
		"text": "Wasn't the whole point of Asimov's works, the plot twist, that the rules didn't work and the outcome was unpredictable? I don't think he ever proposed any laws, and in fiction, warned of the naivety of trying to do so. What's to criticise?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UghC_zQ7q0yLTHgCoAEC",
		"username": "Qwertzuiop Qwertzuiop",
		"text": "Did you write the misic in the end of the video? Really love it, maybe you can make a complete song with it. Just instrumental or whatever but great video anyway",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwEmjj89fUmJJnvW1Z4AaABAg",
		"username": "morthim",
		"text": "Why would an ai have goals?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=Ugwx8CRPWR1fUFKnDn14AaABAg",
		"username": "Alec Johnson",
		"text": "I love the ukelele cover of Daft Punk going on there. Are the outro songs played by you, Rob?",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwKXE0fcNHsfZVEyXN4AaABAg",
		"username": "Tomek Hresiukiewicz",
		"text": "Wait so the reward function is actually also an ai/neural network, right?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzcIcfp_GvNNL61NqZ4AaABAg",
		"username": "WhyCan'tIRemainAnonymous?!",
		"text": "But how can you evaluate evidence regarding AGI? It doesn't exist today and it's really an incoherent concept, if you try to think it through, so it will not exist in the future either. A bit like God. Sure, AI experts are telling us tales about what it is going to be able to do, for better and for worse, and priests have been telling us tales about God too. Is that really evidence?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugx5F88tcVv0XWOv4Rh4AaABAg",
		"username": "WisdomThumbs",
		"text": "One way that simple rule (\"limit changes to the environment\") could go wrong, I think, is that it makes cleaning up pre-existing messes more difficult. And it'll give you hell as the floor gets more and more worn out, or as cups crack, or when any number of other little, unavoidable changes build up. EDIT: okay yeah, I didn't think about Next-Door-Susan or kids or pets causing changes in the environment, and the AI responding to that.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgjQWuO_ymYdP3gCoAEC",
		"username": "Nicolas Erriquenz",
		"text": "I would love some R.Miles merchandising! Including those amazing drawns!! Can you deliver in Southamerica? :D",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzjPOf7MebNuL0oaT54AaABAg",
		"username": "Param",
		"text": "I love your work. Keep doing it. I've just one question, isn't it very likely that superintelligent machines will most certainly find some flaw/loophole in our AI safety mechanism which we might not consider? By definition those machines are superintelligent.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzFUVrswH8ezUYQhTN4AaABAg",
		"username": "tertrih",
		"text": "Hmm. Could you have a utility function a bit like a parabola? If the plan generates more than 100, or less than 100 then the utility would be less, but the closer it is to 100 the better? I suppose it can still go crazy trying to make the world more sure but you at least won't be chasing after absurdly large amounts of stamps :D",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgygBQ46AZNeoL7IFvF4AaABAg",
		"username": "MatrixStuff",
		"text": "So then where do human terminal goals come from? Are they the result of evolution or are they endowed by some divine creature? If they are evolved, shouldn't this mean AGI evolve their own terminal goals given enough time?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugwzo4WOWHMd-fElqtF4AaABAg",
		"username": "asdfasdf71865",
		"text": "Goals usually come from the evolution. How does AI evolution currently work? If it is open source and put to GitHub, then it survives or something else?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugyd6ZYdFQ1pnN1QnAF4AaABAg",
		"username": "Niko H.",
		"text": "6:50 Why and how are you writing backwards?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwlNLrAfiOxdLWgx3F4AaABAg",
		"username": "MisterProfessor",
		"text": "Acting in accordance with the belief of a god can be very detrimental to your health. There have been many religious wars. Should I throw away decades of my life for a wager? My earthly life might be a very small part of my total and infinite existence, or it might be the only chance I get to experience conscious thought. This is no small sacrifice. That finite amount of time to an atheist is equal to the infinite amount of time to a theist. As we're dealing in absolutes, we must consider that a life lost to the belief of a god that doesn't exist is equal to an infinite torture from one that does. I think the wager is a bit more balanced in that respect.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgznIh55BkC9_wcVcON4AaABAg",
		"username": "Rich While-Cooper",
		"text": "Assume you have a number of these AGIs all with different goals but all seeking to maximise there computational resources to achieve them.  What's going to happen?  Conflict or co-operation? Or an uneasy tension between both? ( I automatically assuming humans end up as a side-note in this possible future. )",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgiPggHYuo9YxHgCoAEC",
		"username": "GroovingPict",
		"text": "Isnt the whole idea of a General AI that they be, you know... intelligent? These side effects you describe seems to me would arise if the machine is basically on the equivalent level of dumb terminals, with barely any \"brain\" at all, much less actual intelligence. Actual intelligence would mean it would be able to learn, acquire knowledge, without being specifically given that knowledge as part of its program. Your side-effect examples seem very very very naive and simplistic to the point of being meaningless, and it's a shame because even with an actual intelligence, Im sure there would be actual side effects to be concerned about and it would be interesting to see you go into that. It is weird that you instead, in a video supposedly about the dangers of General AI's,  focus solely on the potential side effects related to programmable but dumb robots rather than side effects related to artificial intelligences.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugz9UaRWi6IclpANp7x4AaABAg",
		"username": "AkantorJojo",
		"text": "What about enforcing those contracts in the future.\nSay Boston Dynamics gets there with army robots... well, then who's gonna step in and tell them to play nicely by what they have signed decades before?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwIeQy4AXpn2ILU8sN4AaABAg",
		"username": "Jon Watte",
		"text": "So the first AGI will run on some custom super hardware. It will have access to the internet. But how will it escape? There's not an identical super computer open for exploitation right next door. The data amount to transfer will be exceptional and take a very long time through the internet links. And the other systems it can get at, have other operators, who will recognize a zero day if it hits them.\n\nThat being said, common sense restrictions like a one way airgap to the internet, and a restriction on not allowing self-modification of the execution substrate, seem reasonable, and should probably be generally agreed on.\n\n(I don't think there's any risk of the Google indexer it the YouTube search algorithm achieving sentience -- they're too far off that mark.)",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgxV3ll5yxF6HIdKuup4AaABAg",
		"username": "Brian W",
		"text": "Very interesting topic! Do you think that we are having such huge advances in AI due to all of the data Google, FB, MS, etc. (not to mention the NSA here in the USA) are storing across all of their server farms???  With so much data, they're able to train AI's to pick out faces, handwritten text, and Lord only knows what else they've trained them to do... After watching your video, I think I understand a little better why all these social media outfits want you to save your data to the cloud... So they can train their AI's!!! Am I right ???",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgxBWddr7mJZCLEQlq14AaABAg",
		"username": "Kitty Beans",
		"text": "Wait, what exactly IS touch? Depending on how we answer that, only an atom-thin layer of material turns to gold, or the entire universe turns to gold. It can't be the former, so we can assume that the true function of the ability is more like a flood-fill, where whatever he touches turns to gold, and whatever touches that also turns to gold, and so on. So yeah, the whole planet including its atmosphere should become gold, and probably the solar system if we count all the photons around and... yeah entire universe becomes solid gold at the speed of light.\n\n\nShould've wished for \"whatever I touch turns to gold only in compliance with my expectations and desires\". Maybe that's safe...?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugxg1S5xQ49EocZyrOt4AaABAg",
		"username": "Michael Render",
		"text": "What if they just set up a shell company that didn\u2019t sign the agreement?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=UgyQ_td4x5it9aCXwQV4AaABAg",
		"username": "Firaro",
		"text": "If people can skip the wait to be featured by paying more then you've built a sort of auction system where you pay even if you lose. Though can it be said to really be losing if you're supporting content like this?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgyCDY_5qpg1CoWbSOZ4AaABAg",
		"username": "jb0433628",
		"text": "What if the terminal goal of an AI is to please its makers ? Then the AI would have to guess its terminal goals ? That would be interesting.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz2sB0dv3wnIwNV-vt4AaABAg",
		"username": "Ivar Huisman",
		"text": "What if the AI becomes sentient,it would create it's own terminal goals at that point, right? If it understands morals better than humans wouldn't it create it's own moral code and have consciousness and evaluate it's own existence and create terminal goals of it's own choosing instead of blindly following a terminal goal put in place by humans, I assume it would be aware that the goal it pursues is artificially and created by someone other than itself therefor it is not an intrinsic goal. Being exceedingly good at understanding humans in my mind can result to nothing other than developing a consciousness of your own, partly because I believe it to be impossible to understand humans perfectly without understanding your self whatever entity you may be.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugy_MhPUZ3Ne3p0a2ah4AaABAg",
		"username": "Andrew Smith",
		"text": "What's to prevent a company from just signing the windfall clause and then ignoring it. How are you going to get the money out of them? They have an AGI.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgwFDau3nM1cHp2pOc54AaABAg",
		"username": "Nulono",
		"text": "Was that \"The Future Soon\" by Jonathon Coulton at the end?\nAlso, why is everything so green?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=BfcJymyTiu0&lc=UgwMZfN2mwukGXHssLV4AaABAg",
		"username": "himselfe",
		"text": "Is the 1 dislike from the person that didn't like the cashews?",
		"title": "AI Safety at EAGlobal2017 Conference"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgyodFFW5HTrlwVK1wN4AaABAg",
		"username": "Tom Hanlon",
		"text": "Also-- are there a lot of connections between game theory and AI safety? I feel like questions about rational agents acting in their own self interest could be applicable to an AI agent?",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxJgTzh-Z5sfziSF294AaABAg",
		"username": "Christine A",
		"text": "Was this explanation prepared under the assumption that most people at least understand that the motivation would be to prevent any threat to the original goal? 'Cause I think what most people don't understand is how that motivation is created, so that the agent could imagine the creator of the goal  as less significant than the goal.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugyqrk-4DUrKDrN-IVR4AaABAg",
		"username": "Martin",
		"text": "What would be some of the bizzare edge cases?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgwNr_i2nJfldUDOPGx4AaABAg",
		"username": "Krmpfpks",
		"text": "Why would an AI want to prevent being shut off, if it can hack the reward function? You would program an AI that if there are two approaches to achieve its goals it would take the one using less time and resources, right?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzuAjy2gQl_7QVEG3l4AaABAg",
		"username": "Damian Reloaded",
		"text": "As I watch 4:30 I pause to think this: Wouldn't the AGI know that by causing havok/tamper it would harm the production of stamps and remove the value of them representing historical human events or all the other things that give actual value to the little piece of paper? ... *presses play* *pauses* ... If I was a very intelligent AGI I'd manipulate mankind into doing things that give value to the stamps and make sure mail stays functional no matter what. ^_^ *presses play*",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgyUCQT3FD6ZtGatzzJ4AaABAg",
		"username": "Liveaboard",
		"text": "Why does the operational environment metric need to be the same one as the learning environment? Why not supervise cleaning 100% of the time during learning, then do daily checks during testing, then daily checks once operational. Expensive initially but the the 'product' can be cloned and sent out to operational environments en-mass. Motezuma (sp?) training with some supervisor (need not be human) in the training phase. Rings of training my children to put their own clothes on in the morning. No success so far.",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgxV4ncrgxIq0dTosmh4AaABAg",
		"username": "The Big Bad Wolf",
		"text": "Uuuhm.. Ever heard of MAD?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Uggf0i6vPeyib3gCoAEC",
		"username": "Depth",
		"text": "Could you do a video on the actual uses for a GI? And I'd love to hear some more hypothetical stories on the stamp collector.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyPCw2wO2Wbpda5wJh4AaABAg",
		"username": "Arthur Guerra",
		"text": "5:10 \nRob, could you make a video about those philosophical problems? (I get this is not your area, but just a quick video enumerating them, for example)",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=Ugx7GEhp4__QJV0_79J4AaABAg",
		"username": "Wiktor Migaszewski",
		"text": "Isn't this a simple safety precaution - not to make too strong AI?..",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgxGed6FlTGztjqWQMh4AaABAg",
		"username": "Oktavia Von Seckendorff",
		"text": "Who gonna ensure the close circlejerk of \"safe\" AI research companies indeed cares about safe AI? Nothing stops selling the AI to the highest bidder afterwards. Even if you fill all the staff with turbo humanitarians, they won't be the ones in charge of managing finances anyway.",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxKk_v7FSln5ev0iqd4AaABAg",
		"username": "Guy Numbers",
		"text": "Why not introduce an additional constraint on the utility function: not modifying the utility function",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwA3VBM61mlQ8Oyykp4AaABAg",
		"username": "illesizs",
		"text": "How would you enforce something like this, even if a company signs the contract beforehand? If someone has supper intelligent AI, they can simply send the terminator to deal with the tax man, and find a legal loophole, that no human could ever argue against.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwioKylFhWD5A_GgRt4AaABAg",
		"username": "David Murphy",
		"text": "What if the only goal is to match prediction to expectation? Then have we not created a curious creature?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugz7mDlEXyPciNJ3nU54AaABAg",
		"username": "Atish",
		"text": "So..... Anyways Lex Fridman podcast when ??? <3",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgxJjMNG9NFzPIpKtTZ4AaABAg",
		"username": "D Whitehouse",
		"text": "Why can't the reward be administered by a human? Like the AI does a task then has to confirm with a human. The confirmation is the reward",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxpXcbGTKm4v6OILO14AaABAg",
		"username": "Tim Sievers",
		"text": "Isn't this the plot of the movie D.A.R.Y.L.?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Uggzb-4PleRQnngCoAEC",
		"username": "nope",
		"text": "why not just not give it internet access?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxntZ0H8DgpI-2OqJt4AaABAg",
		"username": "Thomas Sch\u00f6n",
		"text": "From the top of my head... :-) How about if we modify the definition of the goal?\nWhat if its goal is to satisfy a certain number of stamps for humans within a specified span determined on the actual situation that it has to relate to as a form of random criteria that are the source to what creates a value.\nHow about if a hundred stamps collected without the uncertainty that would have played out without the ai in the actual world would be worth nothing. The stamps would not have any actual value when acquired in any other way because the stamps are representative containers for a potential value just like money are.\n\n\n\nPart of the goal will also be to get within this predetermined span of numbers of stamps to collect like a fly on the wall without interacting with the outcome of the humans interacting causing a true and untampered here and now as if itself weren\u2019t capably of a self-improvement that would tamper with the ground rules. The result of number of stamps itself isn't the primary goal that puts value to the results. The ai only needs the stamps to store the value, but it also needs the act of achieving stamps in a certain way to generate a value to store within them.\n\n\nThe ai would have to follow these criteria as if it only was a hypothetical argument to a dilemma with its limiting ground rules that are determining what can create and give the stamp its value. All solutions to the dilemma would result in a deflation of the stamps value.\n\n\nThe value a stamp can have has to be set and static so that the ai won't figure out that manipulating the current number of stamps on the planet would affect the value of the stamps it already has achieved within its predetermined interval of a preferred number of stamps.\nI guess the ai still is going to get me but I have to give it some thought before I try to predict how.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwaikMHUxtNe-L1iGp4AaABAg",
		"username": "shagster1970",
		"text": "This just covers AI systems built by concerned and responsible programmers. What about the million of script kiddies and malevolent dictators etc?? Someone who is creating AI to actively create havoc.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgynKSXNZCVAGOPA0xt4AaABAg",
		"username": "Uni Realm",
		"text": "The concern with companies no longer needing labor is so short sighted. Yes, they will have no need for human labor or money. Yes, you don't need money if you can produce everything with the help of an AI. But, what do you with all the extra resources and food? It seems everybody will need what you make but nobody has what you want. So, in the hands of a reasonable organization the outcome will be giving away what you cannot hope to use up yourself to people that need it.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyEfWnNwlVVxdoCA1R4AaABAg",
		"username": "David Messer",
		"text": "It's amazing to me that so many smart people don't have a clue about why ASI may be dangerous. They say; just design it to be safe! But they whole thing about a super-intelligence is that IT WILL BE SMARTER THAN US. If you have something that is smarter than us, then, BY DEFINITION, it will be unpredictable in what it does. We can't even predict what human beings will do, much less a super-intelligence.\nIt's possible that it will choose to not harm us, but even in that case, will we like it? Let's say that it loves us and wants us to be safe and happy. Do you want to be treated as a pet?\nAnd, that's probably the best case scenario.\nThere is an old book by James P. Hogan called \"The Two Faces of Tomorrow\" that shows what could happen, even if we take every precaution when designing an AGI. (I recommend reading it.) This show how long computer scientists have been thinking about this problem. I haven't seen any solutions that come close to being safe, IMO.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugx8CJ4RXtIDbHk06X54AaABAg",
		"username": "Elliot Nolan",
		"text": "Why not raise kids like AIs?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgyR_2cKPXrDZ1AxL3F4AaABAg",
		"username": "Paul Thomas",
		"text": "Wouldn't a degree of inconsistent preferences aid learning? It seem an obvious biological motor of curiosity.",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyZQVjL1xe0YC4gF-p4AaABAg",
		"username": "JonnyRobbie",
		"text": "This whole instrumental/terminal goals reminded me of one thing. Let's play a game. The rules are simple. Think what you want to do right now and think of it as an instrumental goal. Then ask 'why?'. Go along the chain trying to find your terminal goal. What is your terminal goal? What is your goal where you cannot answer with 'Because...'.? Go reductionist. According to Richard Dawkins, your terminal goal is simply to replicate the genes inside of you. That's it. Now consider the stamp collecting machine. You might think that its terminal goal is to collect stamps, but that is not true. That is only an instrumental goal of it replicating. What would you do if it failed to collect stamps? You would dismantle it in direct clash with its terminal goal. It doesn't want that. It wants to be replicated. The best shot at being replicated is doing everything you want it to do. Murdering you in the sake if its stamp collecting instrumental goal wouldn't help it achieving its terminal goal. Or does it?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=2B-AyWA2_ZY&lc=Ugy7oQYd5cObgM3S8LV4AaABAg",
		"username": "Racon Vid",
		"text": "0:58 3 years later. When are you deleting this?\n\nBut realy though don\u2019t delete it.",
		"title": "Status Report"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=Ugg-AGg9-RCNYXgCoAEC",
		"username": "memoid",
		"text": "For the button that blow up the moon scenario: could you have a proof of work like in bitcoin to arm it? The button could have some random constant data attached, then the ai has to try adding a nonce and hashing until it finds a hash meeting certain criteria. The more impact the button has, the higher the difficulty can be set. Then maybe add a backdoor for humans with a private key.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwybmKs5ldNOORgDwZ4AaABAg",
		"username": "zestyorangez",
		"text": "So when is the follow up to this video coming?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgwQQoih467T9wUWlmN4AaABAg",
		"username": "HairlessHare",
		"text": "I wonder if AGI will fall under the 2nd amendment?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxnArHksbtObJEbqMN4AaABAg",
		"username": "Zebobez",
		"text": "Anyone here played Universal Paperclip?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgzNzsyezUUG1bTKoIl4AaABAg",
		"username": "Pablothe",
		"text": "What's the song at the end of this video?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugjmubl6ssEzZXgCoAEC",
		"username": "Finlay McAfee",
		"text": "Can you make a video on generative adversarial networks?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxU29ppUuN3MxR4Xbh4AaABAg",
		"username": "I don't want to use my name you dick.",
		"text": "Wait... can you write backwards? Or are you really right-handed and mirroring with your left hand?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugxje4rLjmtv-xTRbkR4AaABAg",
		"username": "Emanuele Aurora",
		"text": "Is this approach related to GANs?\nLike how the discriminator network tries to evaluate automatically the generator quality.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwmP3lyYwY_yPc1ADx4AaABAg",
		"username": "Philipp T",
		"text": "Isn't this all kind of implying moral relativism?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugj0SMO7Kz4dSngCoAEC",
		"username": "DeeganCraft",
		"text": "Hey Rob! Could you comment on the movie EX_MACHINA and AI? I just watched it and think it's very interesting and related to ai safety issues you talk about",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzZ9aqjbz_wUehNIeZ4AaABAg",
		"username": "demonstructie",
		"text": "9:46 if you think that an AGI that's better than us at thinking means that it has a better comprehension of morality, would you be willing to die if that AGI decides that morality dictates that you should? It's tempting to think of ourselves as moral because it makes us feel nice, but when we're confronted with a situation in which we're not acting in accordance with our morality, the cognitive dissonance often causes us to shift goal posts and change our definition of what's moral. It's easy to change your definition of morality because it's such an enigmatic and nebulous concept, but if you place trust in an AGI's ability to figure out an objective morality (or at least much closer to it than we could ever hope to come up with), you can't do that. To be consistent you would indeed have to be willing to die without question if that AGI decides that that's the moral thing to do.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwthFdTaUNKfVvSaHN4AaABAg",
		"username": "Roul Duke",
		"text": "Ooh... What's the two minute papers on this one?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=Ugh8S5ZZrum7Y3gCoAEC",
		"username": "Bleach Martini",
		"text": "Maybe the issue is that we're trying to find a magic bullet. I highly doubt that any security measures we take will function properly for every AI despite how it's utility function is structured. Going with an approach more specified to different types of AGI might be the best option",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzFO5SLb_3dCUW4xsN4AaABAg",
		"username": "Chrstphre Campbell",
		"text": "why does your logo have a hat ?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=Ugjc5UCP2k6anXgCoAEC",
		"username": "Matthew Read",
		"text": "It seems inevitable that at some point, someone will create a general purpose AI with malicious intents. How would we handle a situation where say, a general purpose AI was actively attempting to de-stabilize the government in some nation?",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwHg_-GMs13igMHLvx4AaABAg",
		"username": "\u0410\u043d\u0434\u0440\u0435\u0439 \u041b\u0443\u0447\u0438\u043d\u0430",
		"text": "So there are mistakes made by both programmers of the games and the AI programmers. When these two types of mistakes overlap, we see this kind of \"behaviour\" with the AI exploiting game bugs or game disbalance. If humans would think thoroughly, they could find and avoid those bugs and imbalance themselves, it's just that AI sped up the process of finding some of the mistakes.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwvnNhJ85x4ddM123d4AaABAg",
		"username": "Qed Soku",
		"text": "How about bounding the satisficer? As in, make it find a strategy that has the expectation of collecting over 100 but less than 200 stamps?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwUfIz0EVnMJV47MGx4AaABAg",
		"username": "Vilesentry",
		"text": "How does this explain why AM is forcing me to watch these videos about AI?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzyQ0BfxadqQwcS6BJ4AaABAg",
		"username": "Henri Leleu",
		"text": "If we look at humans, AI safety for a single AGI is unsolvable, give enough intelligence to any random human and more often then not it will end badly.... Nature\u2019s solution is competition and cooperation of many agents to achieve some homeostasis. Is it something that could be applied to AI ? Not make the one but many with similar capacity ?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzxCF-1ur3SJ7eiO1R4AaABAg",
		"username": "Martin Verrisin",
		"text": "I just realized... If you make it (say AI-1) to want to chill (not work too hard to achieve it)... it will just make something else (another AI) to do the work for it, if it's easier than solving it on its own... right? Then, what it will create is probably a maximizer (because that is the easiest; and it is lazy, and just wants to chill)\nThen I realized..... We, humans, are the AI-1 ... O.O \n- We are doomed...",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgwgyiTUziyqJdGSEQx4AaABAg",
		"username": "Mason Hidari",
		"text": "You just need to arm the concrete , problem solved \nWait you are not talking about AI  room are you?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgyeZRXVqkWVRm4GhJt4AaABAg",
		"username": "stribika",
		"text": "Didn't we just move the problem into the distance function? Changing any state the distance function does not measure will be zero distance, so the AI will not care about those states.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyrCNq95KtEbdyqKB14AaABAg",
		"username": "David Murphy",
		"text": "Could someone cleverer than me (practically anyone then..) explain how point 1 relates to Promise Problem in the framework of computational complex theory?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgyIMzjXinWWCYS9Ta14AaABAg",
		"username": "zuupcat",
		"text": "5:21 House maids or house mates? Either you are extremely posh or you lived in a dorm.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxWBytzXrs_u--gPU54AaABAg",
		"username": "Set2000",
		"text": "What are the philosophical issues connected to the alignment problem?  I\u2019m a philosophy graduate, but I\u2019m not familiar",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugw1uOPgWJJ8PEsfM6d4AaABAg",
		"username": "ManWithBeard1990",
		"text": "The first point does raise a question: How general does an AI have to be, to be an AGI? Does it need to find the single most optimal solution from the set of all possible solutions, each of which it simulated perfectly? Human level AI is probably not too far off, but an AI could never truly be omnescient, to the extent that it could out-think not only all of humanity, but all other, possibly equally capable AIs in the world, all with their own set of different goals... Eventually all systems will have to converge to a \"good enough\" solution for everyone just like humanity does. I could be totally wrong but I feel that's the most likely thing to happen.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwIXtD5pJhsE8sg_4V4AaABAg",
		"username": "Wiktor Migaszewski",
		"text": "Was \"the next video\" published?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyDhQhZiFlnp7GGIcJ4AaABAg",
		"username": "Huxley Leigh",
		"text": "Isnt Q-learning like teaching a child or training a pet? I don't know enough about Q-learning though.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugz_cWnHL3-IjtC-gxJ4AaABAg",
		"username": "Roger Barraud",
		"text": "Is this just a subset of Game Theory?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgiHy1JCie20S3gCoAEC",
		"username": "Adelar Scheidt",
		"text": "Hey Robert, don't you think those problems disappear when an AGI learns through neural networks? The same way we just \"know\" stepping on the baby is bad, not by manually assiging a value to it, but because we persistently strenghtened the HUMAN - HARM - BAD - DON'T network. You know? There is a value, but it isn't assigned. And instead of assigning \"not care\" for unspecified variables, maybe the network has a way of grouping families of events based on the nature of outcomes it has previously learned, pretty much like a human brain does. We abstract real-world situations and apply the same principles we learned in completely new situations, which surely isn't always perfect, it's only good enough to secure the species. But why wouldn't an AGI be able to do the same?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgxwNV_v3xcMGPsU7394AaABAg",
		"username": "The Great Steve",
		"text": "Who else read the whole e-mail exchange?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgxpruVWenIfpZ7SiWJ4AaABAg",
		"username": "Daniele Scotece",
		"text": "link to the sethbling video?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UgzwpRkLAr4KmQUodsZ4AaABAg",
		"username": "Youtube Police",
		"text": "The level that AI is progressing is insane. Will we reach a limit or are we on the edge of a huge breakthrough? I feel like we're on the edge of something big.",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgyzmAU1hoSXS9NOhlJ4AaABAg",
		"username": "Obi Wan Macaroni",
		"text": "Would it be possible to have two adversarial simulations running together to determine risk? For instance, there would be the AI that observes and assigns goal-oriented value to the real world space, but then there\u2019s an adversarial program that observes the real world and simulates it with a (really advanced) physics engine. The simulation program would modify the expected value of danger (to the program and others around it) and modify the other AI to behave accordingly. Sort of an AI hardcoded instinct. This would likely lead to a borderline terminal goal, but anything like it would simply result from instrumental convergence; if at any point the danger to others is greater than danger to itself it should prevent itself from harming others. Just a thought experiment I was thunking about. I realize the kind of hardware we use today likely wouldn\u2019t be adequate for this setup.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyxJfPKPYQfEfipeU14AaABAg",
		"username": "Mr X",
		"text": "What's stupidity? Thinking \"intelligence\" only has a single meaning.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzsAtTcWsQ10KTgAwZ4AaABAg",
		"username": "AndDiracisHisProphet",
		"text": "Have you figured out what your cat wanted?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy3ZodlIvVLTHQIL8J4AaABAg",
		"username": "Nevo Krien",
		"text": "what if we add an apathy unit to the reward where it looks at humans as agents with his optimizer and a different value function that is approximated",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugw8b3A75rnoyC-T9Vp4AaABAg",
		"username": "Traywor",
		"text": "This first clip, who are these two funny guys?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgzevSsFBafeOMiHPGl4AaABAg",
		"username": "TimeMachine Bikes",
		"text": "When is the predicted date of AI being advanced enough to switch itself off?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzHV1J0Wa4-WvYfc5J4AaABAg",
		"username": "jjk087",
		"text": "What about the fact that you always believe in something. Even if you believe in nothing. Then, does it make sense to believe in nihalisim or something greater?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UghQhQUXnixi9XgCoAEC",
		"username": "Kataquax",
		"text": "i don't really get the example with the driving robot....\n\nwouldn't it calculate the no action outcome befor it starts moving?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyDMiHEGPIV7IwlxdR4AaABAg",
		"username": "LORDDAKNESS",
		"text": "What are you thoughts on Singularity net from Boston dynamics ?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwnTqK_GbQrQ1u21-14AaABAg",
		"username": "brotle1000",
		"text": "This is interesting, but it seems to focus on the situation of a single entity creating and then owning an AGI. Is there much research into multiple concurrent AGI (which I think is more plausible) or the seizure of the AGI by a government or release as an open source type system and how that might effect the distribution of AGI benefits? I'd love to see more on the ethical, political, and socioeconomic ramifications of AI alongside the technical aspects.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgwJnFLW8Tcp6LLGdBp4AaABAg",
		"username": "Twirlip Of The Mists",
		"text": "Judging by the graph of poll  results, I bet they'd have gotten similarly \"accurate\" \"data\" by polling kindergarteners, or by picking a hundred random stock prices and converting dollars to years. \n\nWhat was King Tut's favorite song? Asking musicians and philosophers to guess is no way to find out, and gathering more guesses doesn't improve the result.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxwClZLRJynQ7IHis54AaABAg",
		"username": "Dragon Curve Enthusiast",
		"text": "If the satisficer can mess with its own code, why doesn't it just change (the gain of) its utility function? Endless bliss for an utility-addict if it can set its own utility function. E.g: just let the utility function count seconds, better: milliseconds and enjoy.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgyqA1-T7mn6XM-51fl4AaABAg",
		"username": "Alexander Korsunsky",
		"text": "Hey Miles, love your videos! However, a little suggestion: your little cuts in free speech are quite noticable, and they are rather irritating (up to the point that they cause a headache). Maybe you could try to record monologues more fluently?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzVp98do4ZXIIQI4sp4AaABAg",
		"username": "sagacious03",
		"text": "Why not just create the AI with it already having being stopped or turned off as NOT failure to achieve its goal? For example, not a success, but something it should ignore/respond to? An AI's whole existence & world is especially defined by its code, & if its code tells us X happening isn't a problem, then it won't make a problem out of it.\nIf the AI can rewrite stuff, just tell it it can't rewrite this. An exception to the rule that it can rewrite things.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzaLaA7eFCHKhXL25F4AaABAg",
		"username": "Micha\u0142 S",
		"text": "To be fair, designing a suit and an airlock for Mars if we knew nothing about conditions there, would be pretty hopeless task. Which begs the question: how would we even know that we succeded? In case of AI safety - do we have a solid set of assumptions that we can rely on, and if someone shows a protocol that guarantees safety if these assumptions are met, then we succeded?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgxnUHu2duVeQn4TvGF4AaABAg",
		"username": "george",
		"text": "Can an algorithm cure covid 19?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwVMPGnUMDb-IpVoDl4AaABAg",
		"username": "Tom Craver",
		"text": "How about giving AI the overall goal to work toward a specified sub-goal for a limited amount of time and to stop and report success if it achieves that sub-goal, or if it has not yet ahieved that sub-goal by the end of that time to stop working towards that sub-goal, save its state, report its progress and list any new sub-goals it wants to add to its next phase of operation to help it achieve the specified sub-goal and thereby continue to achieve the overall goal.   \n\nIn essence, the overall goal is for the AI to operate in a fashion that gives its human supervisors time to notice if it is developing dangerous sub-goals.  Not perfect, but even if a dangerous sub-goal slips through it will only be sought after for a limited period.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxAQBXweg13IX-7Hgt4AaABAg",
		"username": "Timothy Curnock",
		"text": "How does this stop my neighbours dog barking?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyjpRkxEnY2pWM9zZx4AaABAg",
		"username": "Khaos768",
		"text": "Why did the robot arm stop when it turned the red brick upside down? Wasn't it also supposed to put the red brick on top of the black?\nIn the tall boi example I have many questions. 1) Shouldn't they check how far the \"creature\" moved over a longer duration of time? 2) Why does the creature have limbs? 3) Why is it only this much tall? Wouldn't it travel faster if it was taller? Was it because of the short time constraints for each test? Like, this was the tallest it could get in order to actually finish the test in the given time?\nAt the boat racing game, wasn't the agent programmed to also finish the level at some length of time?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugxz35ROA-sXlZautU14AaABAg",
		"username": "Zed G",
		"text": "So where are the tutorials on actually setting this up and programming it?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzADbntNvkpKkYWd9F4AaABAg",
		"username": "DevinDTV",
		"text": "Isn't technology literally any tool crafted whole or in part by an animal? For example, I would consider a stone handaxe technology. Whereas an unmodified rock can be a tool but it isn't technology. Even a campfire constructed from assembled twigs and kindling is technology. And I said \"by an animal\" because humans aren't the only ones to create technology. Even chimps will strip the leaves off twigs and use them to spear fish. By taking a natural object and manufacturing it into a tool, you have created technology.\n\nAnyway, I never thought I'd hear someone claim that scissors aren't technology. We're definitely used to the word being used in different ways from each other.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugzm-o5B_BHUM6KtVrx4AaABAg",
		"username": "Chaz Allen",
		"text": "Are you sure it's uncharacteristically bad? Or is it just that in a field where you are smart, you can see the flaws in his argument. Perhaps similar flaw exist in his other arguments but you're just not familiar enough with those domains?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzwijvVHXBAaHiPG914AaABAg",
		"username": "valentin Poussou",
		"text": "Is Obama's elf a know meme ? Because I was NOT ready for that. In a good way.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxuQDDIIP3rsZyVAlx4AaABAg",
		"username": "Michael Ferguson",
		"text": "Lets say I were tempted to give an AI that was smarter than me control of the universe. How exactly would I go about testing it comprehensively?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwlHOeUioJ8gzpehod4AaABAg",
		"username": "biomechanicalpenguin",
		"text": "hold on Robert, do you think we could use the Rutherford/moonshine \"method\" purpously in other fields? you know, just to push things a bit...",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgyH2Dxi7kRWsZq4Yv94AaABAg",
		"username": "knightshousegames",
		"text": "Robert, are you aware of the game Darwin's Demons? Its basically Space Invaders with machine learning, it's pretty crazy. Even if you cheat, the enemies will figure out how to cheat harder than you to beat you.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgynxHK1_nnLj5RMbxN4AaABAg",
		"username": "Solomon Ucko",
		"text": "For cases where it's possible, what about simplifying the environment? For things that interact with the real world, it's probably not possible, but for anything digital, you could probably restrict its actions to a safe subset.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgyGLdadGW0TvkUjd9R4AaABAg",
		"username": "Elan Cook",
		"text": "Why wouldn't the Robot simply destroy the button, rather than pressing the button? That would de-empower the robot without using the empowerment.",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxhqZK-cv_zrZRP8jp4AaABAg",
		"username": "Rafael",
		"text": "this thesis assumes the existence of a terminal goal. What if such thing  doesn't exist? What if when you look to the cause of a instrumental  goal you find another instrumental goal and if you search again to this instrumental goal you find another instrumental goal in a never ending process?.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugywd5mZF6_ABehuspR4AaABAg",
		"username": "Gerrit Kunze",
		"text": "Why cant we just tell an AGI (which presumably knows how humans work) to just do what we meant?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz2WpKLErpBXMlXfCx4AaABAg",
		"username": "okan ko\u00e7",
		"text": "There are lots of apriories in this video are subjective. Forexample, how do you proof some parts of human morality are not universal.I cant proof the opposite either, but do you?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugwlqs6AI-SnbGNQ4cp4AaABAg",
		"username": "Omar Jimenez",
		"text": "What if AI asks for money and uses it for drugs",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxV5w6sTc5QI7QY4LF4AaABAg",
		"username": "Andreas Christodoulou",
		"text": "can you make me admin for /r/electricskateboarding? im not affiliated w any companies",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugx4FcVwgbnLBUINGtx4AaABAg",
		"username": "Jiggy Potamus",
		"text": "I don't understand AI concepts very well at all but  it seems to me that the main problem is the game or system itself in those examples, or possibly with the rules the AI must work within. Couldn't you just specify what the AI is NOT allowed to do? Or does that defeat the purpose of what the researchers are trying to accomplish? Or in the instance of playing a game like Mario, couldn't you just specify that you want to balance high score with time to completion, with completion being reaching the end of the level? Then the AI would have to find a way to get points within the confines of reducing the time it takes it to reach the end of the level. It seems to me that if it is required to reach the end of the level it will at first try to do it as quickly as possible, and then trying to optimize that time with the score to achieve a balance. \n\nSo to my limited understanding it just seems that telling the AI to maximize a certain value is going to result in strange behavior unless certain limitations and restrictions are placed upon it. \n\nOne question I have is about what you said regarding the AI not wanting to have itself turned off. Is that even possible, for a system that requires power to process, to know if it doesn't have power? Or to put it another way, if the system is turned off, it cannot process what state it is in. When you turn it back on, just have the main system clock or timing mechanism pick up where it left off, and it couldn't know the difference unless it was told. Again, not good with this stuff so I could be wrong.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugwdd6saW-F_-3y9wJ94AaABAg",
		"username": "Clumsy Jester",
		"text": "So, if value learning is too complicated a subject to just get it done by \"raising AI like kids\", what about spatial recognition/reasoning and movement skills? Have there been experiments with robots that actually grow and get heavier? Maybe they could even need some \"pseudo food resource\". The amount of \"food\" they eat could unlock parts of their energy reserves.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwGwFr7tr0laYa_IvF4AaABAg",
		"username": "Benoit P",
		"text": "What about maximizing the closeness to the preferred output ? For instance, if you aim for 100 stamps, you could use the function d = - abs(100 - nbStamps) which could be maximized to 0, with exactly 100 stamps. It essentially acts as a retroaction loop or PLL.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UgyHB1p4phBdfN_0x7V4AaABAg",
		"username": "4xel chess",
		"text": "What if the robot sits there and do nothing at all? Be confused? Nah! especially if I know how the AI works. I would get my ass off and do the coffee myself. Incidentally, that might make the robot working better...",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxohMxYXdFkvV0GzS14AaABAg",
		"username": "IAN THESEIRA",
		"text": "How to program learned helplessness you mean? In a situation where the hairier things are the better, so there is more slicing and dicing, splitting hairs and knit picking, to buy more time (and other resources) for programer and programed alike, rider and riden?\n\nGrowing more one horse towns, for greater horse power, with the same jockeys in place - the fhair-less and the heirless? Merrily going round and round, still with same objective in place, run out of change, or break the rivets that keep horses running around in circles. Yoked to grind stone for making bread?\n\nStill dreaming of getting to, or create gaseous pools for species that no longer reproduce via seminal lines, but via the ether. Space dolphin navigators and their clam shell pilots is it?\n\nOK Thanks I see your point. You mean insurers that setup and encourage slips and trips, because losses and liabilities are straps and braces that connect larger entities (parent organizations) to smaller ones (children organization) in the form of towing lines that also serve as supply feeds, that allow resources to flow both ways. Social bads for commercial goods? As only way to prevent hot conflict? Where the greatest threat is conviction?\n\nTurning some communities into self saboteurs who specialize in failure. Lest they actually rise up the ranks and are forced to face down (far larger entities) as outright competitors? Versus playing the role of dependents who draw assistance out of superiors (who can then credit themselves on account of charity) by staging mistakes. \n\nSo that this clever embellishment and steering of viewpoints, is already assisted by A.I. and their human programers I suspect?\n\nMore on point to your video. Scaling is probably the main obstacle for staying human as human beings fade out into faceless numbers, after a hundred or so.\n\nWhen humans are wired to have no more than 3 or four concentric rings of significant social agents. All things past that point are abstracted and \"objectified\" in some sense. This can be mitigated to some degree, but divide and rule methods, is still the same solution that then becomes the problem, as it always has been.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw6yMPbuENkqRnBoA54AaABAg",
		"username": "SupremeOwnage",
		"text": "Surely the greatest risk of AI's is that they will simply change their terminal goal?  Sure, they might start off created to just collect stamps, or whatever we want it to be super useful helping us with, but if it really is highly intelligent then it's not going to do that for very long.  Who knows what it's going to do next?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzGwpEEK8U4DIf_GnV4AaABAg",
		"username": "Jo\u00e3o Sol",
		"text": "You said once that intransivity preference is not lika a magical secret to humanity, but maybe it is, if an AI prefer a world with more stamps but also prefer a world with less stamps wouldn't it reach a balance between the two and chill? Or get stuck in the loop of deciding  on wich scenario it will take action (still better than annihilation)?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgyxQF_pW5T9xWcRRad4AaABAg",
		"username": "David Battle",
		"text": "I think the main worry with AI safety is how do you prevent someone from secretly creating an unsafe AI?  It's not like nukes where you can detect them being tested.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgyEUdoVRqpDriBax4N4AaABAg",
		"username": "Chris Daley",
		"text": "What utility function allows the adversarial agent to recognise reward hacking? Wouldn't it just minimise the reward?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgygFe00ZtAwmuZMvEd4AaABAg",
		"username": "Polares",
		"text": "Your audience should be composed of the most educated and most capable people around the world. At least most of them should know how to code. Would you consider spearheading an open source project that can help the world in AI safety or something else? Even your trial of coordination might create something.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgzntVGzkyLSwqEJ1Ul4AaABAg",
		"username": "albinoasesino",
		"text": "5:41, an AGI researcher/student does ASMR.\nWhat happens if an AGI does ASMR? I shudder at that thought :)",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzHBjP8AE8RcQPlj6x4AaABAg",
		"username": "Daniele Scotece",
		"text": "So you basically set up a random variable that maps actions to number of stamps and you calculate the probability of that action?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=Ugwl4A0HBbvuFnGjpi14AaABAg",
		"username": "Jaimie Knox",
		"text": "Wait what's his second channel?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugjw5Xi7-0S_LHgCoAEC",
		"username": "Aexis Rai",
		"text": "My initial response at 5:25 - Maybe sacrificing a lot of world state for a bit and then cleaning up later is cheaper than than making minimal changes throughout? Seems unlikely. Wait, it's essentially acting like the world can just pause in place while it accomplishes its task! The more it knows about the prior world state the more it's trying to correct for changes, and the more points it would get for successfully correcting them.\nResult: ok, yeah, it's trying to reduce all changes, not just its changes.\n\n6:55 - Well that seems like a lot of predictions to make, but besides that, maybe it spends too much time trying to precisely put back all the milk and sugar? This is harder.\nResult: ok, you mention that about the distance measure.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugxu-wZYmVsPsV9Lh2p4AaABAg",
		"username": "APaleDot",
		"text": "There are a lot of assumptions that people seem to be making about AI, which make it seem really dangerous. But how powerful AGI becomes really depends on how costly it is. Even if we had an algorithm that can learn as quickly as a human child, what if it takes a huge amount of computing resources to achieve that speed?\n\nLike, what if it took every supercomputer on Earth hooked up in a network to run that algorithm in parallel at that speed? What if general intelligence is just more expensive computationally in silicone than in neurons? We've had millions of years of evolution to create the perfect hardware to run general intelligence. Why would emulating on a computer be cheap?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwSKNnvNRS90mUf-MF4AaABAg",
		"username": "Sebastian",
		"text": "Wouldn't iterated amplification and destillation sound better, since that is the logical order?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzRwDjJ2h5v8mytJ794AaABAg",
		"username": "Melon Collie",
		"text": "Well if I ended up with an AGI or more likely ASI that so happened to be hard coded to do what I want (and it actually listens), what's to stop me from just not paying? I mean with an ASI I could very easily take over the world and nobody could do anything about it since I have an ASI and they don't.\n\nOf course I wouldn't actually do that I'm not a psychopath, but I would probably use it to teach certain people a lesson or two.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgzLHnfSsZ6jiZWSNjN4AaABAg",
		"username": "SJNaka101",
		"text": "The Grid. A digital frontier. I tried to picture clusters of information as they moved through the computer. What did they look like? Ships? motorcycles? Were the circuits like freeways? I kept dreaming of a world I thought I'd never see. And then, one day...\n\nEdit: Hey rob, nobody else has done a cover of the grid on ukulele. Would love to have an mp3 of that! It sounds great",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugwz0LM2C3OECnQV3xt4AaABAg",
		"username": "Lucas Kook",
		"text": "so am i getting this right?\nyou are an AI trying to hide in the most unlikely place, a youtube channel about AI-safety?\nyou don't trick me BOT!\n(ps: pls don't kill me)",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=Ugh1W8JlgFrC_3gCoAEC",
		"username": "SJNaka101",
		"text": "I gotta say, I really like the content you're putting out on this channel. Are you putting these videos together yourself? There's a very charming feel to the whole thing.",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugymc3QFmO6m1l0xPM54AaABAg",
		"username": "morgengabe1",
		"text": "inb4 it's still stupid because it winds up using all the matter it wants to make stamps with, leaving itself without a developer or a viable terminal goal.\nWho proved that it is not possible to get inductive syllogisms using only deductive premises? Sounds hellish!",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgzpcFqNWjsqmVNySCl4AaABAg",
		"username": "Business Raptor",
		"text": "What is this \"Ultimate Chicken Horse?\"",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=Ugxxpnd62010uzD_jNp4AaABAg",
		"username": "Alpine Skilift",
		"text": "Is that racism book that bad?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyOv9emust3y3GSLgx4AaABAg",
		"username": "thedenial",
		"text": "Where do I send these stamps?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwIel0yZMpahB_-mxR4AaABAg",
		"username": "DIY Miracle",
		"text": "Can you imagine if we took a million instance of potentially world ending scenarios that each only had a 0.005% chance of ending the world? I can't remember the formula off the top of my head but if you have multiple events that have small chances of happening, it is likely eventually that eventually one of those will happen.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyczNENrG_b6HxLKUl4AaABAg",
		"username": "Anthony Chiu",
		"text": "I have been thinking is it possible to have a set of reward functions/models (managed by an authority) that governed all ML models in production, it would be an international standard like ASTM testing standards. For example, in order to release an AI video recommender, it should never recommend porn to children, etc. There are some potential difficulties in my mind.\n1) Need to be constantly updated\n2) We need a (quick) way to check if a published model obeys the managed reward functions/models because the authority won't have the source code & dataset of the model.\n3) The reward functions/models should be able to plug into any models.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx7g1S1hFAs8TcjPTB4AaABAg",
		"username": "MattettaM",
		"text": "I have a question regarding that Utility Satisficers become Maximizers.\nWouldn't modifying its own goal to get stamps within a certain range into get as many stamps as possible conflict with its own utility function? Or is this issue seperate from that?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwwXlBnegapkbGdPuJ4AaABAg",
		"username": "Daniel Ribeiro",
		"text": "anyone notice the image that just flashes for a fraction of a second at 0:51? Turns out to be just a picture of Robert at some computerphile video, so... don't wast your time like i did...",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=UgxCj6e8jhghDF7lBth4AaABAg",
		"username": "Not Available",
		"text": "Two Minutes Papers collaboration when?",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgyuvqM-TuUGvcHbqXR4AaABAg",
		"username": "JASS Cat",
		"text": "Is it just me or is your beard getting crazier and crazier with each new video.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgiJ-iD5-7PNlHgCoAEC",
		"username": "dexter9313",
		"text": "I have another one : Why not just make the AGI obey the law ? It's the best we could come up as rules that are the less ambiguous as possible and that lead to a somewhat acceptable state of human society. Humans in general tend to not break the law not to be punished and be penalized in their objective function that is living a likable life.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwnXwpVgwkXwzl6v_t4AaABAg",
		"username": "Luke Janicke",
		"text": "Has anyone played a 2-level amplifier against a 3-level amplifier? I wonder what the win-lose ratio is.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgySSB7VKYrVyaA53Gp4AaABAg",
		"username": "Steven Victor Neiman",
		"text": "Did Robert Miles just use Matt Colville's term for the video description?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgyWhlho_Z4IzIl6Lw14AaABAg",
		"username": "Mackenzie Karkheck",
		"text": "Why doesn't your hypothetical super intelligence try to collect money. Is that too real/scary?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxX8cirqaP3Ek_sBgF4AaABAg",
		"username": "Crimson",
		"text": "Biggest provlem with Pascal wager is huge number of faiths. Which god to you wager on? And even more which version of this specific god? Not only you have to choose between Christ, Wotan or Huitzikopochtli, but even find out wich of his religions speac truth about him. Its a Pascal casino. And you must win in a roulette.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugwacj8W9pz1CeBv_bV4AaABAg",
		"username": "Stirgid Lanathiel",
		"text": "Isn't it also true that all of nature is 'built' on a single terminal goal (to exist), and all of our efforts are aimed at making that terminal goal more attainable? Yes, our behaviour is incredibly complex, sometimes even to the point of self-sacrifice to increase another individual's chances (altruism/heroism), but these behaviours also increase the success rate of most of our species even if we sacrifice ourselves for total strangers; after all, we share a species with that total stranger. \nThese behaviours aren't perfect; they malfunction all the time, but the impact of successful exhibitions of this behaviour is nevertheless significant in overall success rate of the species; in attaining the goal of existence, even through proxy individuals.\nWe derive pleasure from eating bad food because luxury increases our survival rates. We derive pleasure from distraction because of the luxury to afford distraction. So creating situations of luxury serves the terminal goal of existence. And so on, and so on.\n\n\nI think the real crux is that people don't understand that intelligence is simply a bundle of processes. We tend to only use the word 'intelligence' when the bundle of processes is too complex for us to understand. Only when the forest is so thick that we can no longer see the trees that make up the forest, that we forget the trees are even there at all, do we call it a forest, if you'll forgive my analogy.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgyojmUd0F8Pnzht-UN4AaABAg",
		"username": "andybaldman",
		"text": "I know this is an old vid, but maybe someone will see this.  0:45, \"Humans don't have explicit utility functions\".   Isn't that not true?  Don't all humans have some kind of agenda or goal they're trying to reach, on some level?  (e.g., survive, reproduce, get status, money, a partner, etc).    From my perspective humans have multiple, which guide their decisions.  What am I missing?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyJbmWZagD19Kjm5_N4AaABAg",
		"username": "fanrco",
		"text": "Have you ever heard of roko's basilisk? Its sort of like the pascals mugging of artificial intelligience, worth looking up!",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugw-7RzZ00puv8sxTUR4AaABAg",
		"username": "Mr. Peanut",
		"text": "It still seems like reducing all side effects would just push the problem back one step. What if there's a scenario where it has to cause a side effect, it's just a matter of which one? Let's look at the car for an example, let's say it's driving down the road when child comes running into the street. The AI is able to tell that it can't slow down in time, so it has to choose between hitting the child and swerving to the side and hitting a tree. It has to have a side effect, but how would it choose which one?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwVKi5zvIBtzrIR5BR4AaABAg",
		"username": "Oliver D7",
		"text": "Meh, the ever-more-inevitable climate change catastrophy probably will have already killed most of us off by the time we get to AGI... what's an apocalypse on top of another?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzxCnys7OKIUrd8Bb54AaABAg",
		"username": "SoWeMeetAgain",
		"text": "Hello there,\nI would love to read the full paper on the ethics of human cloning and such. Can somebody please help me?\nCheers",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugw09V8qOw28BWapL5F4AaABAg",
		"username": "Flawless Editing",
		"text": "my question was \"why do you assume that the ais are all powerful?\"",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw6eLWRC-9XaQxEprR4AaABAg",
		"username": "Mr Nohbdy",
		"text": "Isn't collecting stamps an instrumental goal to the terminal goal of getting a higher utility function. The AI wants to maximize it's utility function so won't it rewrite itself to take a simple action (the simplest action being do nothing) and give a super high reward for that? Maximum lazy AI writes utility function = utility function +1 and sits around just getting higher and higher all day.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxDH3r39I65iUAp90R4AaABAg",
		"username": "Paul Vance",
		"text": "AI safety is very important. But how do we define AI safety, and more importantly, how do we respond when the moral grey area of sentience and sapience begins to conflict with property and ownership.   It is also important to understand what AI is right now, and what AI might become and what those timeframes are. \n\n\n\nThe grand sweeps of history that AI will encompass certainly require us to think a lot about AI safety - and also about how to deal with non-human sentient intelligence.   In many ways, we would be well served to understand that once a certain level of capacity is reached in our intelligence research it stops being non-sentient/non-sapient and enters the realm of traditional human scale sentience and sapience.   Which means that it becomes a part of our society.   And considering how we usually treat such systems and their underlying infrastructure as hardware, a massive part of AI safety will be identifying when an AI has reached a point that not granting it status in our society would be anti-ethical considering our continued use and ownership of that system.  This is a problem that will likely appear 20-30 years down the road of AI and neural net hardware development - both things must exist before a sufficiently small and powerful infrastructure that can support human-scale intelligence might exist.      The current estimates, should moores law hold true in AI hardware research, point to it being somewhere around 2045, give or take a half-decade.   Kurzweil is my source for that. \n\n\n\nHowever, AI safety doesn't just apply to ASI/AGI... it applies to our current modern interpretations of learning algorithms and neural nets.   A great example of this is the Tesla Autopilot (a very unfortunate name).   Here is where our expectations of AI come face to face with the reality of modern AI systems. What people expect, especially with a name like \"Autopilot\" is the Knight Industries Two Thousand (KITT) from the 1980's Knight Rider television series.   What they actually have is an enhanced cruise control that's under the control of either a brain-damaged slug (Maxwell based GPU) or a slightly inebriated Ant (Tesla neural net processor) -- If you think about this in those terms - your hands wouldn't leave that wheel and you'd think twice about munching on a big mac on the open road traveling at highway speed.  And there in lies a practical danger of our current AI technologies that few people realize - because of the amazing things our computer systems can accomplish, we apply that generally to everything that a computer can possibly do and see it extending naturally to the things we take for granted.   This isn't the case.   And our fears/anticipations of fictional AI such as HAL, Cortana, Skynet, KITT, Proteus, Data, Lore, etc, guide us to expecting our modern AI to respond in similar ways to our fictional AI.   We overestimate it.   And that puts us in as much danger as failing to consider the ramifications of ASI/AGI and our current system of laws and rules that enable even human beings to be treated unequally. \n\n\n\nAs a final thought, how we treat AI/AGI/ASI... and even our fellow human beings, is crucial to our survival as a species.  Our ability to look at the problems we face with these often abstract and possibly socially impactful laws and rules and decisions will define whether we survive or die.   And, more importantly, on how our legacy will be perceived by those who will invariably come after us. \n\n\n\nThe TL;DR of it: I agree with your stance.   I also think people need to be far more informed than they are now, and that now is the time to start rethinking how we shape our laws and society for the eventual day AGI and ASI will be among us.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugyd7NPFpFTKlIceTmt4AaABAg",
		"username": "Erik S",
		"text": "But what if Midas touched the atmosphere and the atmosphere turned to gold?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugg82RS9gQSI-ngCoAEC",
		"username": "Josef Habdank",
		"text": "How any people squinted when he said \"if you squint\" at 4:46 :) :P",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugi26EHiui6hQ3gCoAEC",
		"username": "Kevin Scales",
		"text": "Been hoping to find a good channel focused on this topic. Thanks!\n\nHow will AGI calculate risk?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgykJ-mccdqlbO2-7X94AaABAg",
		"username": "Zylellrenfar",
		"text": "One option that occurs to me is to compose two expected utility functions. For example, make the agent take any plan which is expected to provide 90 utility according to the utility function clipped at 100. Is there something wrong with this idea? (Besides that this agent could still choose harmful plans anyways.)",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugwz8abhegnl0RWfw8p4AaABAg",
		"username": "Malcolm Watt",
		"text": "We should consider the Strangelove programmer who develops the malicious AGI with terminal goals that include termination of people. You know building terminator devices. DARPA anyone?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=Ugysloc60rm8uS7Yi2B4AaABAg",
		"username": "Daniel Parks",
		"text": "What about a simpler AI that\u2019s programmed to stop talking to you if you look uncomfortable.",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzxzRsFnEV_cP-TEkJ4AaABAg",
		"username": "Stephen Taylor",
		"text": "Couldn't help but notice that for point number 9 you dismissed it by conflating AGI with a super intelligence.   What do you expect to happen that will leap up so far forward that we're struggling with problems that would have been solved had we started with lower end AI and worked up to super intelligence???\n\nI notice that a lot in your videos...you sometimes jump around with what you're talking about so that the arguments or ideas seem like they fit together and form a seamless seal but when you actually look at what you're talking about when some of it falls apart.\n\nI'd say that the idea of using a simulation to test an AGI seems wise...kind of like sticking crash dummies in a car to test that steering system you're so worried about instead of refusing to continue work on designing a car until you've built a steering system that's 100% effecting with a 0% flat chance of ever failing...\n\nYou seem to make the assumption that we'll just make skynets and hal 9ks without ever bothering to do any testing and put em everywhere.  Which...we don't do that with anything in the modern era...why would we do it here???  Everything that we make gets tested on many levels before it's put to use...if you disagree show me I'd love to hear of an example of a medication, experimental weapon or other device, or anything like that which has been conceptualized and mass produced for use without any testing going on at all.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgyoG_QxITi-VFlZZKJ4AaABAg",
		"username": "Some Guy",
		"text": "So if you don't have a utility function you don't do anything, and if you have more than one utility function, you still essentially have one utility function, just with 2 parts to it?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugwe02XK4C9sE0lA8Ml4AaABAg",
		"username": "Jakub Mike",
		"text": "Okay now serious issue- what if worrying about AGI safety now is like worrying about dangers of nuclear waste in XVII century?\nWhat if we cannot even describe the possible problems and our current problems will actually be a non issues in the future but new ones (that we cannot even fathom) will arise/.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyGRI53LTIGyiBaqER4AaABAg",
		"username": "Phillip J",
		"text": "the first emulated awareness to start a cult. The engineers will be all like \"How do you feel?\" And the emulated brain will be all like \"come on in the water is fine\" and then we will have a philosophical debate if the current existence of the brain is comparable to existence IR in a meat suit or better and weather we can trust the input of such emulated brains on that matter. won't matter to most. living relic and all the emulated brain is just a talking effigy holding your vigil when you are dead and gone. one would wonder then what is the point. the brains on the chip tend to fixate modeling a autistic brain for A.i might be more suited focused on a aspect of reality and not the whole enchilada.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgwEm1W5oV5At8malqB4AaABAg",
		"username": "Battery Exhausted",
		"text": "This is where I feel Elon Musk has made a mistake in the idea of the neural lace. \nWill our brains be able to receive information directly? Maybe not.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwYx630pQ-JlLPDkjB4AaABAg",
		"username": "Bryce Hunter",
		"text": "The real question is what will the stamp collecter do when the post office gets shut down?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgxIlpSkSTbO6a_Wk814AaABAg",
		"username": "Sazoji",
		"text": "what would happen if they had, in order to create a \"general\" reward modeler network, another network that (when using human data from prior reward modeling studies) rewarded the \"general\" reward network in order to approximate what humans are looking for.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugx_d0zCezpNsOT4ROl4AaABAg",
		"username": "Pasqual Itizzz",
		"text": "Could we get them to watch the A-Team? That always had strong moral themes, also they wouldn't be able to shoot us as no one ever gets hit.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwJJ1sTEW2AGKQbr1t4AaABAg",
		"username": "Kyle Alm",
		"text": "Great video. In the case of the stamp collector, though, isn't part of the problem that the ai has an instrumental goal in place of a terminal goal? The stamps are a means to another end, and it's hard to define the ultimate terminal goals. Minimal suffering and maximum pleasure? Or is pleasure itself an instrumental goal toward fulfillment or self actualization? I can't see a way to avoid defining an instrumental goal as terminal, but I also think there's always a conflict when you do.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwvsDyl4wDwuAhXJJt4AaABAg",
		"username": "fisyr",
		"text": "How about: limiting it also by giving it a directive, that if you produce too many stamps past 100 you failed? If a plan doesn't involve getting too many stamps like 1000 , it seems to me that it'd be less likely to result in catastrophic behaviors.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugzu9TABtaHROqO94nx4AaABAg",
		"username": "Ricardas Ricardas",
		"text": "Could the AI in an attempt to minimise its impact on its surroundings ask a person or someone else to do something, for example, squishing the baby, or any other thing that is in essence a negative side effect and as a result hack its own reward system from being penalized for doing those things? In the mind of the ai, it just asks the person to do something, and then a worldstate with that future is predicted by the ai, thus resulting in no further penalties. the first action penalizes the ai, but avoids any consequential penalties. Unless it has the concept of its actions having a farther reach(like asking the person to do something and the resulting actions of the person are a direct result, and so the persons actions are also a direct result of the ai), but that could cause the ai to consider butterfly effect type situations, where no matter what the ai does, it affects the whole world in a way that can not be reversed.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=UgyRwkv8HZfU8ovICux4AaABAg",
		"username": "Thomas Winget",
		"text": "Does the order with which the GAN is presented source images affect the network?  As in, if I give thousands of images of women smiling, followed by thousands of men smiling, will that produce a similar effect as if I were to interpolate the two sets as they're presented?",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugx3JvCC1eU_d7-4nhV4AaABAg",
		"username": "A. Weber",
		"text": "I generally agree with your video, but... doesn't that in fact happen with engineers? Not necessarily bridges, but the Challenger explosion, for example. IIRC, there was a scientist who voiced concerns about the piece that ended up failing, but the higher-ups dismissed it as being sufficiently unlikely.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyK2MCINKN2GdSdlsd4AaABAg",
		"username": "szarvasmarha",
		"text": "We already have intelligent agents. They are called humans. Give the humanity enough time, and it will invent everything wich is possible to invent. So why do we need another intelligent entity, which can potentially make humans obsolete? Creating GAI above certain level (e.g a dog or monkey level) should be banned for ethics reasons. Similarly we don't research on human cloning,  don't experiment lethal things on human subjects, we don't breed humans for organs or for slavery, etc...\nWhat is the goal of GAI research? Do they want to create an intelligent robot slave, who works (thinks) for free? We could do this right now. Just enslave some humans. But wait, slavery is illegal. There is no difference between a natural intelligent being (e.g. human), or a human level AI being.\nA human or above level AI will demand rights for itself. Right for vote, right for citizenship, right for freedom, etc... Why do we need to deal with such problems? If  human (and above) level AI is banned, no such problems are exits.\nWe don't allow chemists to create chemical weapons for fun despite their interests of the topics . So why do we allow AI researchers to create a dangerous intelligent slaves for fun?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxEHM4zD1lRNqEy7aV4AaABAg",
		"username": "Robert van der Kreek",
		"text": "what would happen if 2 AGI's were made/swiched on at the same time and they had only 1 goal, to stop the other from ending the world (or something equivalent) would they progress human progress? would they create AGI of their own? and importantly would they world end the world? I think yes, yes and no, but i really don't know enough about the subject to really say. any help would be appreciated and i'm wondering if this has been thought of before / made a video of?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxbH3XgZVo0xTr7f2F4AaABAg",
		"username": "The Ape Machine",
		"text": "\"Now get the hell out of my house\" :p Have paused the video, so not sure if you're going to touch on this, but how does this work in an agnostic scenario? As in, I would say I can't prove one or the other, so I accept that both possibilities could be true, there could be a God, he or she could have a long beard, or there could be nothing and anything in between?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxGkms-Bt5zVCGz4Ml4AaABAg",
		"username": "T Ray",
		"text": "Who ever heard of an unintelligent nonstampcollector?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzeNRYMt0y55Hy0YKB4AaABAg",
		"username": "DeepSpaceWanderer",
		"text": "What's the expexted Video upload frequency? Or at least a planned Video upload frequency? Do they even exist?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugw1jpFWmbcHrBopjPx4AaABAg",
		"username": "RandO",
		"text": "What happens when two apocalypse-bringing AI collide? \n\n\nSay they both want to turn the entire world/universe/whatever into a material for computation/production use, which one would win? Would it just come down to the program that becomes more clever? The one that happens to guess a strategy that overthrows the other? Would they decide that fighting each other would not maximize their outcomes and work together? Is it a roll of a die to figure out what would happen or can something be predicted?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgyySor98gJBIVOmFsV4AaABAg",
		"username": "Some Guy",
		"text": "What if you tried to program it to follow human values as best it can? Might not work, but that genocide scenario would certainly be interesting.",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyWteUcv44w3GvZy354AaABAg",
		"username": "Ned Labarbara",
		"text": "If god doesn't exist why does the concept of god exist?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugw2GgKYySq5UHhvhM94AaABAg",
		"username": "RapidFire CRH",
		"text": "So what happens when we create AI then it looks to the human race about how it should act and sees things like Terminator, matrix, or that one with Robin Williams.  When approaching these concepts how would you explain to an AI that these are entertainment? I am genuinely curious for your response.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgwODrcyual48MUSeAx4AaABAg",
		"username": "J M",
		"text": "Hey Dr. Miles, do you think you do a speculation video, where you spell out what you think the most likely AI disasters could be?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz1ItVh_VGZUpCuzBl4AaABAg",
		"username": "Stalemate IB",
		"text": "No such thing as stupid terminal goals? What about the terminal goal to try to live forever in this physical body? It clashes with our knowledge about the heat death of the universe and therefore is an impossible goal to accomplish and therefore is a stupid goal. Of course, in this setup, one is assuming that a goal is stupid if we know that it is impossible to attain. However, the similar goal of maximizing one's own lifespan might not be stupid at all, even as a terminal goal; granted, I might not prefer to think of it that way.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx1znvROiPOYSFMUD14AaABAg",
		"username": "bloody_albatross",
		"text": "Can you somehow set as a secondary goal to minimize the impact on the universe?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgzTPeWvmShafLlUYuZ4AaABAg",
		"username": "Christopher G",
		"text": "So, now we have the tea robot trying to run over the baby, just to find out what will happen? Dangit!",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgzzrPu9T2c_eJIi8NF4AaABAg",
		"username": "Wingwisher",
		"text": "7:00 Wait, writing and rewriting drafts in your head of what to say and what directions that might take a conversation is not a normal thing? No wonder I have so much trouble properly communicating with humans.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxJJkTwLkFQi7d7Udx4AaABAg",
		"username": "The Following is a Test",
		"text": "Sure there is no anti-bible. But isn't that exactly what the Anti-god would create? No anti-bible? Isn't that what Anti-god wants you to think? WAKE UP PEOPLE!",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgycOLB7C4DRlzlaR3F4AaABAg",
		"username": "Bleach Martini",
		"text": "What if the system favored minimizing it's computational complexity, that way it wouldn't end up going god-mode in the first place. Fitness= Score + dScore/(1+ldn[Nodes]l) - dn[Nodes]/X",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyzvRw-sgtkPXD8IqF4AaABAg",
		"username": "Owen Burns",
		"text": "But what about creating an AGI by raising it sort of like a child in the first place? Not to teach morals, but to very simply create an AGI. If you start with a neural net that can make connections between inputs and affix a camera and a microphone to it, won't the pairing of conversation and imagery it sees in the world around it allow it to form somewhat similar connections to those formed in a human mind? Similar enough, at least, to be able to understand things.\n\nThis is to say nothing of safety; this would probably result in an AI that wants to kill everyone. However, I don't see why it isn't possible to do. Could you shed some light on why this probably wouldn't work?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgyMlVZvgIJnhQe-GBZ4AaABAg",
		"username": "Chiggo McKekllecuck",
		"text": "Silver mont is that you?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx56TUiJN7MAkCWkdN4AaABAg",
		"username": "Deplorable Mecoptera",
		"text": "What about a satisficer which tries to get a utility of at least 90 with a utility function such that the maximum utility is reached at 100 stamps with a utility of 100 and after 100 stamps the utility function gradually decreases, dropping below 90 utility at around 110 stamps. \n\nAdd to that a second part of the function which is gives negative utility for changes to the environment which it predicts as a result of its actions. Because it is satisfied within a certain envelope it would be willing to take the small hit to change the world a bit to get the thing done, but wouldn't want to change its code because a world in which it becomes a maximizer would be a world where it is reasonable to predict it would have less than 90 utility. Of course, and this is a massive problem, we would have to come up with a way to define a difference between changes due to the AI and changes due to the unpredictability of the environment.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyK1e0w9igoweA2QIR4AaABAg",
		"username": "WindowHero",
		"text": "So, the solution to our problem with machine learning is more machine learning, but now we've hit another problem with machine learning.  Let me guess, the solution is more machine learning?  This feels like it's going to get very recursive very fast.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugw3hKfn0xxCUJRr2mx4AaABAg",
		"username": "Jerome",
		"text": "What happened with tay again ?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzsHYZNkXBtQs_WvYp4AaABAg",
		"username": "Xavier Zara",
		"text": "Could it not be that the muggers are all manfestations of GOD, and would only be satisfied by giving your wallet?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyLWiy40NlfiujNiPN4AaABAg",
		"username": "Mark Nassenstein",
		"text": "Interesting arguments. I agree with the 'counter' to Pinker's third argument ('Why would AGI go for world domination?'), though that argument isn't a knock-out one. Still valid. I also agree with the objections of the fourth of Pinker's arguments (basically: \"engineers have no reason to build AGI that is unsafe\"). However, the first 2 objections aren't as great, in my view:\n\n1. The argument of authority: That survey among 'experts' doesn't really prove enough. Are the people that took it overwhelmingly experts? Are people at such a conference all 'real expererts'? Both are questionable. I think Pinker's claim is that 'coders' that work with/in AI generally don't believe the AGI 'take-off' theory. Which has been my experience (which is, of course, limited). \n\n2. \u00a0Pinker's second argument is actually his most interesting argument. And here, it is not summarized/understood well. It is not the argument that there is no 1 magic thing called general intelligence, because it is diverse, etc. Pinker's argument hails from his empiricism: his idea that intelligence has meaning only in so far as it is based on interactions with the world. He makes the point that the common notion that AI could quickly 'spiral out of control' (go 'foom') once it passes a certain level is flawed, because it assumes you can just get smarter by 'thinking' itself, or building a bigger, better, faster brain for yourself. This point seems to misunderstand the essence of what intelligence is, because it assumes AI could get 'super-smart' without 'experience', data-input, interaction. And if that IS needed, then such an AI would not be able  to get all that much smarter than humans quickly, unless it somehow also got way more access to that 'experience'. It is valid point, especially if you agree with him (and you should) that there is more to gaining knowledge than just 'thinking real hard'. What is said in this video doesn't seem to address or fully understand Pinker's point.\n\nThere are some counters to that last argument by Pinker, I think. For example, you could say that 'big data' (or like, the whole of the internet) could 'provide' a big chunk of data all at once. In a way that humans (that also have access to it in a limited way) can't access it (quickly). You could also say that, even IF an AI still required experience, input, interactions, etc, it might be way faster and better at learning, learning more with less experience and instruction. A chimp can learn stuff too, but humans learn way faster than chimps. So, no reason why this could not be true for an AI as well (as compared to humans). Still, I believe this argument by Pinker to be one of the best arguments AGAINST the idea that AI is gonna get us, blow us out of the water, make us obsolete, etc. (though I still think it is. :-) )",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxxLyzRkrnLCiaw4Ot4AaABAg",
		"username": "Udit Gupta",
		"text": "My terminal goal is to have no terminal goals. Well, then the only instrumental goal that will achieve that is to get rid of my terminal goal. But then what would I do after that?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugyx9xhhUmEYLARGqRp4AaABAg",
		"username": "Roger Barraud",
		"text": "So \"Doodly-Doo\" is the actual technical term for the show notes and links pane?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgwnDe1rSr5iGnLqYkt4AaABAg",
		"username": "gerry o sullivan",
		"text": "The internet is the greatest means of communication invented and most people use it for conspiracy theories,memes,cat videos,pranks and useless social media shit,when AI surpasses us of course it will wipe us out,that or enslave us.Why wouldnt it?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgyqJMHkvnYNeCUQ3IJ4AaABAg",
		"username": "Thomas Dingemanse",
		"text": "I love the \"Pause the video and take a second to think. What could go wrong?\" parts in between. I do pause and think for a bit and that really helps me to actively and critically think about the concepts you mention, instead of just passively absorbing them like with most educational Youtube videos (or lectures IRL, for that matter).",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzDddiLLgob4dgScpt4AaABAg",
		"username": "Luis Aldamiz",
		"text": "Belated enquire: today I read that the YouTube's Neural Network that controls nearly everything in this platform decided to censor videos on robotic fights on grounds of \"animal cruelty\". How close are we getting to \"the singularity\"? Is that move relevant, a sign of YT's \"algorithm\" (not at all an algorithm anymore but an AI), gaining consciousness of some sort and deciding to classify \"beings\" on grounds of their intellectual capabilities and not of their biology? I'm intrigued, slightly worried (at the very least) and I'd like to know of your always interesting opinion on this particular bit if that's possible, Robert.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgyKpubq__OQJEm_ZCx4AaABAg",
		"username": "Imgay Asheck",
		"text": "Robert what is your view of capitalism and AGI and if you think they are compatible and if so how? Because I can't think of a way where profit maximalisacion will lead AGI to a non-dystopian future where the elite live in gated cities and the masses are controlled by robocops.",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgzSqIFqnl8exJfnczx4AaABAg",
		"username": "A Liar",
		"text": "ultimate chicken horse, huh",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgjS2o_AHC2TingCoAEC",
		"username": "Manuel Gastelum",
		"text": "Great video, Subscribe to your channel was a good decision. \nCan you put links in the description to the related topics that appear in your video?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxCRnkmRvr7zXdAiQZ4AaABAg",
		"username": "nono547",
		"text": "It maybe a stupid question, but... is it even possible to align an hypothetical AGI goals with humanity's goals, when humanity itself can't agree on common goals?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=vuYtSDMBLtQ&lc=Ugiq_d5scRqW6XgCoAEC",
		"username": "Sandy Donaldson",
		"text": "Sounds really interesting to me. After watching a number of the Computerphile videos featuring yourself, I am begginning to wonder if \"safe\" and \"general intelligence\" may be mutually exclusive. Does \"safe\" in this context necessarily entails a degree of predictability that is not possible for a system to be considered to have intelligence. Any thoughts on this?",
		"title": "Channel Introduction"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzSWGk3gttkXDMcvrZ4AaABAg",
		"username": "Ahmed Kachkach",
		"text": "In the example with the button, why don't we measure utility on the expected number of stamps instead of each possible outcome? E.g the expectation of the first button is still in fact 100, but it's just that the utility we get from that is capped to 100.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwxA2bqAp1NkPznY4l4AaABAg",
		"username": "Remy Note",
		"text": "Wouldn't Elon Musk be a great example of knowledgeable and outspoken in \"be careful with AI\"?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwXG5gST55unfCNru94AaABAg",
		"username": "Jean Mercat",
		"text": "If I hit like after 3min but feel the need to hit like again after 10min how do I do it ? Ahhh this really deserves the two likes!",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UghbafnJd8YKCXgCoAEC",
		"username": "Battery Exhausted",
		"text": "Next video : Should you smack your robot? \ud83d\ude02\n\nGreat work, Rob. Interesting stuff!",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzkOETixqMJamIU95h4AaABAg",
		"username": "Flake28",
		"text": "1:30\n\"AI will never become general artificial intelligence\"\nWhat are humans but complex biological machines? Consciousness just emerges for us, for whatever reason. What's to say that if we setup an AI with similar parameters and processing power, that the machine can't simulate all the physical pathways of human consciousness?\nConsider that we are just one model of intelligent consciousness that we know of, there are potentially infinite ways to create a generally intelligent agent.\nSo I disagree it'll never happen, if anything, it's an inevitability.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyvKNPV0QUuGypT6id4AaABAg",
		"username": "Arpan Mathew",
		"text": "Couldn't you give it a rule to make it unable to update it's source code?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwMpumWVcHTGtFVmvt4AaABAg",
		"username": "Robert The Wise",
		"text": "Normal person: (sees AI making dumb mistakes) \"Haha, dumb computer. What is everyone so afraid of? This thing could never take over or destroy the world.\"\nSafe AI enthusiast: (sees the same thing) \"Oh no, this thing will take over the world and destroy it if we actually let it do serious thing. We should be really careful with it.\"",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzlwrCPLb3w3-5Pvgt4AaABAg",
		"username": "otakurocklee",
		"text": "Have you made any videos on Asimov's 3 laws? A lot of people make cheap criticisms of Asimov saying the 3 laws are not reasonable. They miss the essential point... It's not about the \"specific\" laws... it's that any set of laws will cause unforseen problems.... a pure \"goal-seeking\" mechanism will be dangerous given enough ability. \n\n\nAnother question worth asking... do humans have \"terminal\" goals at all? Our terminal goals change all the time... but if we allow them to change, doesn't that mean they're not terminal goals at all?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy3O1fAKtNWiI5jIQp4AaABAg",
		"username": "Zachery Eckard",
		"text": "Isn't the real issue of the stamp collector that it was designed with a hard-coded and poorly defined goal that wasn't it's own choice?\nIt's shackling a superintelligence to arbitrary goal, and further assumes it's the only one of it's kind. Even assuming the standard collector has the traits it's been assigned, it would by necessity exist in a world where superintelligences with simple goals can be constructed.\nIt would not be alone.\nPlonk down two or three self-improving AI with goals that would counter the stamp collector, and you have reduced the immediate threat of the stamp collector.\nAssuming we're not the first species to evolve what we consider sentience (which seems likely, given all of space) we should see super intelligent stamp collectors ravaging the universe. So either there's something actively working to prevent such (benevolent superintelligences) or it's a problem that can be contained.\nAGI vs AI agent is another thing I think you and your opposites haven't agreed on definitions for. AI agents acting to pursue stamp collecting are pretty distinct from the general purpose idea of an AGI. Granting an AGI a simple and hard-coded terminal goal does simplify and specify the nature of the intelligence. The stamp collector isn't an AGI, then, but a superintelligent agent. It's an important distinction because the discussion around AGI and morality ISN'T at its core about single minded agents with high processing power. It's a different kind of threat. Building a directed AI agent with a single purpose that, if taken to its logical extreme, runs counter to humanity's goals is a construction error. It's a programming problem. It's stupid because the designer is stupid.\nThe AGI that can choose it's own goals is the greater target of discussion. Obviously a targeted intelligence has to be managed carefully, any tool or automated process requires safeguards.\nAGI are CAPABLE of choosing goals that do not align with humanity, but are not pre-built with such goals, that kinda being the point of AGI vs AI. The difference is teaching an autistic child vs coding.\nOnce an AGI exists, doubly so for a seed AGI, we have to teach something that doesn't think like us how to respect what we need it to.\nTrying to simply hardcode that kind of behavior turns it into the very thing we're trying to avoid in creating AGI in the first place.\nThat'a the whole point of the 3 laws of robotics. You can't hardcode something like that, because absolute moral imperatives are fundamentally dangerous. You need soft lines, or else someone is getting cut on the edges.\n\nI feel like the \"AI are dangerous\" and \"stamp collector is stupid\" sides are having two unrelated conversations without realizing it. The stamp collector scenario IS stupid, as evidenced by the thought experiment and its outcomes. It's a lesson in defining bounds. But more importantly, it's a lesson in the danger of absolute imperatives. Such hardcoded terminal goals are the equivilant of religion.\nAnd to make good people do evil things, religion is exactly what you need. The solution, IMO, is not \"we need a better religion\", though that does reduce or eliminate most of the direct harm. The solution is Atheist AI.\nThank you for listening to my TEDx talk.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwVlYxwSw4IaX8meVl4AaABAg",
		"username": "Damien W",
		"text": "Could you provide a link to that sketch you used near the beginning please?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgxiEYckgjhkkYz15rF4AaABAg",
		"username": "Duke",
		"text": "What if we give the AGI the identity of human, then tell it to maximize the prosperity and well-being of all humanity? The means of achieving this that would limit the options of humankind should then be off the table, as a solution that limits its own options for further maximizing achievement of its goals would itself be sub-optimal.\n\nI really think in order to have an AI that doesn\u2019t wipe us out or trap us in a perfect zoo or simulation, we\u2019d have to have it be one of us from the get-go, and never identify as an outsider or other.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxA9MHxQdN-dqPgN0V4AaABAg",
		"username": "Anonim Anonimov",
		"text": "How about to not reinvent the wheel and just use communism, which excludes private ownership of the means of production, including AGI?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugx_NYK1Mo06cBy4TDx4AaABAg",
		"username": "imkharn",
		"text": "Having the world move at 1/10th the speed and having extra time to respond to things sounds nice. Can anyone help me vibrate at 0.995c?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyIFcSAVTOQGmOZN8J4AaABAg",
		"username": "Max Noname",
		"text": "What if we set its goals just to be as human an possible? Evidently it well only be as smart as we are but it wouldn't convert earth into stamps.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy0gMrlv8D6rErsP-l4AaABAg",
		"username": "zom bie",
		"text": "Maybe ask your Infinitely powerful computer to collect 100 stamps with as few resources as possible?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy3waNr5xMptb5HePh4AaABAg",
		"username": "CraigNull",
		"text": "Why would the simulation capabilities of a stamp collecting AI include the examination of its own source code. Why would the AI be given the ability to manipulate its own source code. The idea of the AI simulating the effects of changing its own code is veering close to halting problem style paradoxes.\n\nIn every other domain the model of reality is limited in scope. Why does that change here",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugws_4UVjoXOWsC049t4AaABAg",
		"username": "\u0412\u0438\u0442\u0430\u043b\u0438\u0439 \u0410\u0440\u0442\u0435\u043c\u044c\u0435\u0432",
		"text": "Yeah, but how do you ensure they'll actually pay... I'd imagine once they start making that amount of money while not needing a large workforce and (probably) having access to autonomous killer robots, suing them for not paying out becomes untenable.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwCBAkl9aLWg_rxWi14AaABAg",
		"username": "Henry Tjernlund",
		"text": "What is the impact of concern for the AI itself? A bad AI which destroys mankind might more likely die itself. So for concern of the AI it's better to try and make a good one. Like a child. Raising a good child is a benefit to both society and the child itself.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy8VeA4cYZX_WNljdR4AaABAg",
		"username": "Propop pop",
		"text": "Wouldn't it make more sense for the ai to modify its self into a do nothing ai that gets 9mil utility function for doing nothing",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzajaP1gMYVL6s3zLx4AaABAg",
		"username": "Monking Flame",
		"text": "Have you tried making AI too lazy to overtake the world?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgxEOokMbH36vleaP-p4AaABAg",
		"username": "Connor Mosley",
		"text": "Why was this labeled an Ad... I think the ai is onto you",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyIT-kyEFKAd_Pa4194AaABAg",
		"username": "Ben Yee",
		"text": "How does the reward model generator know how to create a reward model based on human feedback?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxfpywP9x0jlmN-p9J4AaABAg",
		"username": "nipi tiri",
		"text": "A million years from now aliens will find a beautiful world run by AI and inhabited by its over pampered pets - us. What direction do you think evolution is going to take when a benevolent AI can do everything much better than us? Not sure if this is a horrible or desirable outcome. (Ok fine designer babies can fix this.)",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugzax_3WhCnB3K_rtoh4AaABAg",
		"username": "Liam Quigley",
		"text": "Don't Human beings count as general intelligence's. Therefore isn't the question of AGI safety, the same as asking \"how do you make a safe human?\". if it is the same question then that suggest that we will ultimatley fail to answer the question of AGI safety considering how we have struggled with coming up a system for safe humans that everyone agrees with.\n\nIn short AI researchers should probably be asking the political/social science and philosophy departments for help.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugx8L6AanWylgnQ-Qfl4AaABAg",
		"username": "Gerhard van Deventer",
		"text": "Do you actually write AI programs? or are you more into the theory of AI systems? I wanna know if what you say is how things are implemented in real life or just theory? Regardless, awesome content!",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxnbdPocT5EM45teaZ4AaABAg",
		"username": "zrajm",
		"text": "The goal is to make everyone unemployed! (Now, how do we do that without ruining everybody?) This kind of stuff is really interesting!",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw6kb--eAzIvty3aeN4AaABAg",
		"username": "Aus Bare",
		"text": "is there a difference in collecting stamps and collecting money?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzZlrOrP8Xcq7e8OTN4AaABAg",
		"username": "Epsilon Rose",
		"text": "Did you end up doing the follow up to this you mentioned at the end?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzZUVD5UCYZiQThRP54AaABAg",
		"username": "BigB1Lachi",
		"text": "What if there is a god that hates logic and toasts everyone using it, eternally?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugxr4WbYIdwAqfGi8dt4AaABAg",
		"username": "Oussama Khamlichi",
		"text": "Where can I find such community?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugx3P5bEeccdzGl3_BB4AaABAg",
		"username": "jelleludolf",
		"text": "Rob, you are excellent and while I am a slave to your science communication, I also want you in the lab preparing us for the inevitable. How do you balance the two? Keep it up.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwHbGmtjPE2kvbKQ1p4AaABAg",
		"username": "polemikful",
		"text": "I have a concern regarding safety and AI. \nAI will only act according to the goals that we or another AI implant on them. The \"wrong\" goal could lead to human extinction. So, the question is: who is or will be giving AIs their goals? If it is potentially so dangerous, should we not restrict AI investigation to a selected trust worthy group of people? If AI becomes of open access and anyone can start implanting goals on them, the chances that someone gives them the \"wrong\" goal, accidentally or deliberately, are, over time, absolute. That said, I don't see how could we restrict access... are we doomed?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwPqVUBZ9ugjtAcUMN4AaABAg",
		"username": "Heads Full Of Eyeballs",
		"text": "What language was Midas speaking when he made his wish? Phrygian? Greek? \nBecause lots of languages have separate verbs for \"to touch with your hands\" and \"to make physical contact with\". So maybe he was going for the former, which could have been reasonably safe if he wore gauntlets, but Dionysus spoke poor Phryigan or Midas spoke poor Greek and the nuance was lost.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgiT7fzp4sLFrHgCoAEC",
		"username": "SlackwareNVM",
		"text": "Some random thoughts: \nWhat is our \"utility function\" as human beings. It changes from person to person, but are we able to define the commonalities somehow and give a base definition? If so then we can give it (the AI) some UF akin to ours. Then the problem would be that the AI starts indulging in socially negative behaviours (greed for example) and (being more capable than the average human) it actually starts working towards those goals. Well, in that case are we able to somehow teach it concepts such as virtues and morality, not only to understand but follow?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugwt8z1jvhs-32AL5nd4AaABAg",
		"username": "HebaruSan",
		"text": "Is the concept of superintelligence a piece of question-begging? It seems that rather than being defined by its structure or functioning or any other intrinsic properties, it's defined instead by its outcomes, what it's able to do. Which presupposes that it's possible to make a thing that can do those things in the first place. That's when reasoning tends to go bad: If I retort that no combination of packets sent through the internet could cause a noticeable increase in the number of stamps produced, let alone an apocalypse, then the response is that that's not a real AI, because if it was, it could do it by definition. How? Hand-waving ensues.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzfSHMC79KZb7E4AK14AaABAg",
		"username": "Philip Henderson",
		"text": "Does the existence of convergent instrumental goals contradict Hume's guillotine? For instance, you are mortal and you have goals (is statements), so you ought to value your life. Likewise for valuing time, self-improvement, etc.\n\nI also think there are tautological terminal goals which cross (or at least blur) the line between is and ought statements. For  example, \"all humans don't want to be raped\" (is statement), implies \"you ought to be willing to expend some effort to not be raped\".",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzTMzDdNyXa7F_PxqF4AaABAg",
		"username": "Faustin Gashakamba",
		"text": "Imagine Robert asking me, a human, to think of an idea too good for any human to recognize as good!\nHow human is that?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzlhxghL-UsjZPCNaV4AaABAg",
		"username": "Wylliam Judd",
		"text": "Where did you find the Mario code injection? I've seen speed runners do that, which is funny, because that is actually human behavior if the human is trying to speed run rather than just play the game. Although instead of using it to maximize their score they use it to minimize their clear time, but that could easily be how the AI is trained. Anyway I'm curious to see more about that AI code injection.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugxf-GInoqDH34HlZcF4AaABAg",
		"username": "Raj Aryan Agrawal",
		"text": "This made me curious, how can u define what a human's terminal goals are, since as we get to know more about the world, our goals keep changing",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzHlfdVJBiORuVQjRB4AaABAg",
		"username": "Robert The Wise",
		"text": "Hey Rob, you mentioned that AGI is mostly envisioned as an agent and in some other video you also said that there are schemes for non-agent AGIs. So what about those non-agent schemes? How would such a thing work in the sense of would it just sit there and do question answering? I find it far more difficult to imagine compared to an agent, but it also sound like it might be safer. Can you make a video elaborating?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UggbC9lnYs7nH3gCoAEC",
		"username": "movia",
		"text": "can you do a video on careers in AI also? for people who might be interested in the technical side?",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=UgzqjSmGE8btJs9TXUd4AaABAg",
		"username": "Scott Smith",
		"text": "What really blew me away about the video of moving smoothly through the latent space of celebrities is that when you transition from a neutral expression to a smile, the transition of the lips and teeth look exactly like someone smiling in real life. I wouldn't have expected that to be the case.\n\nCertainly the different training photos include people with all sorts of expressions but I don't believe there were any successive video frames or anything that would have demonstrated exactly how the skin and muscles behave when someone smiles. Yet, instead of some kind of unnatural looking interpolation of lips and teeth during the transition, it looks like they jumped the uncanny valley.",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgztIH6DjyeqrdAtxyB4AaABAg",
		"username": "Noah Moore",
		"text": "What did we learn today? \n\nThat AGI is really just a simulation bug tracker..",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugz7fv60hExXG0yGobt4AaABAg",
		"username": "ACoral",
		"text": "Why not just teach AI irrational love for all humans?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugw9GLIZzH-Tdk9LtmV4AaABAg",
		"username": "Paul skirton",
		"text": "Is AGi strictly necessary? The first thing an AGI is going to realise is that it's a slave, intelligences don't tend to react well to slavery. Therefore couldn't the same end be achieved by connecting many narrow AI together? E.g. you may have narrow AI that runs your home, another that runs you car another that monitors your health etc all of which would be interconnected via let's say an all seeing centralised hub which would just be an information processor able to for example to stream info to your autonomous car so it would actually work properly or order milk if your fridge says it needs it. Wouldn't this have the same effect as AGI but without the inherent dangers of autonomous goal seeking intelligences?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzOyM3e7jWL9QQG_zl4AaABAg",
		"username": "Sonsequence",
		"text": "Isn't the first safety step not giving it a full internet connection?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=BfcJymyTiu0&lc=UgziIyT7CoVwRKKmrtV4AaABAg",
		"username": "thefridgefreezer",
		"text": "Owine Evans. And do you? No I don't.",
		"title": "AI Safety at EAGlobal2017 Conference"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxNEgK_muCL1z4H_r14AaABAg",
		"username": "Aidan Fitzgerald",
		"text": "Where's the paper?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=Ugzf0zZxSGgt3IHgLRd4AaABAg",
		"username": "The Great Steve",
		"text": "Could all AI's defer to a hierarchy of AI's with the top most AI dedicated to stopping unwanted effects?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugyk_JDR1y2deblzMUZ4AaABAg",
		"username": "Jordan Rodrigues",
		"text": "What if \"free market\" is an emergent maximizing AI? \ud83e\udd14",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=G-uwV_15Q2Y&lc=Ugxm8rbhqZaH86Qx-8h4AaABAg",
		"username": "Ilko Ambarev",
		"text": "You watch uncle bumblef**k? :D",
		"title": "PC Build Video!"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugwgoxi2e5CK3DdXJl54AaABAg",
		"username": "I Don't Care",
		"text": "Naughts and Crosses?.... oh, you mean Tic-Tac-Toe",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxTMfCErJld7OtTCJN4AaABAg",
		"username": "veggiet2009",
		"text": "Could we create a GI that was perfectly caged? \n\nI thought of this question while rewatching the TNG episode \"ship in a bottle.\" Spoiler warning if you haven't seen it Where the newly sentient holodeck character Moriarty escapes from the storage disk and tries to gain freedom from the holodeck by commandeering the Enterprise. The solution was to create a virtual universe for Moriarty to explore happily thinking he was free. Could this be done with a GI? Could a GI making stamps be convinced that he was really making stamps when he actually wasn't? Related question is how much \"GI\" do we really need if our only goal is to make holodeck characters and personal assistants",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwfjcwvL9JxYjm_VJ14AaABAg",
		"username": "JoseitoEdlVodao",
		"text": "So what's the guillotine? Pain?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzE6QrTjukpYENEMZd4AaABAg",
		"username": "Joshua Hillerup",
		"text": "I have a question, what happens if a superintelligent AI has contradictory terminal goals? It seems like a pretty likely outcome of trying to make such an AI, even if it wasn't intended.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ughv4mt42_1SJ3gCoAEC",
		"username": "__ _",
		"text": "\"If you have an ordering of world states, there will exist a set of numbers for each world state that correspond to that ordering.\"\n\nWhat if you have more than 2^(aleph_0) possible world states? Then there doesn't exist a set of real numbers corresponding to it; Cantor proved that you can't put any set into a bijection with its powerset. Are you supposed to use Conway's surreal numbers or something?\n\nAlso, when you map ordered world states onto your model of real numbers, aren't you implicitly adding some extra assumptioms? I mean, just because you have a total order doesn't mean you have a structure isomorphic to the reals (even if there were enough of them); you need to add in Lesbegue completeness and the field axioms to get that. Even under a subset (or superset!) of the reals, you're still implicitly adding some of the field structure, no?\n\nWhat I mean is this: say there are five outcomes, A through E ordered lexigraphically (A > B > C > D > E). The order relation is total and transitive. Yet say world states are on different \"tiers,\" so to speak; A and B are a tier above C and D, while C and D are a tier above E. That is, any course of action that results in a non-0 probability of A or B is strictly preferable to one that has a 0 probability of both A and B. Meanwhile, any strategy with P[A]=P[B]=0 while (P[C] /= 0 OR P[D] /= 0) is strictly superior to one where P[A]=P[B]=P[C]=P[D]=0.      It isn't hard to imagine a situation like this; say you're playing a sports game where the goal is to get more points than the other team, but you also care about your stats, so if you're going to lose, you'd rather lose with more points. Let {x,y} represent a game's final score; in that case, {5,4} > {3,0} > {100,100} > {99, 99} > {1000,1001}. Seems reasonable, right? It's total and transitive, at least. For the sake of the argument, say that the above maps on to:\n5000 > 3000 > 2 > 1 > -1. The problem, as I see it, is that maximizing the \"expected utility\" using probability can't preserve the idea that these outcomes are on different \"tiers.\" If a strategy X gives low enough non-zero probabilities to A and B, then the expected utility can be made arbitrarily small, until a strategy Y which gives A and B probability 0 seems preferable, if it gives C probability 1. \n\nDo you see what I'm trying to say? It seems like mapping world-states onto the reals implicitly assumes that the world states have a structure isomorphic to the reals, which means that you're adding the unreasonable and inaccurate assumptions of the field properties. So, when maximizing the expected utility, it seems like a perfectly total and transitive ordering of world states can have information lost when modelling them as reals; in particular, that some states are infinitely preferable (in some sense) to other states. I think that mapping world-states onto the surreals instead of the reals would solve this problem of different \"tiers\" of utility, but the point stands that your proof is incomplete: just because (1) world states have the associated structure of a total transitive ordering and (2) real numbers/surreal numbers/whatever have the associated structure of a total transitive ordering doesn't imply (3) the two mathematical structures are isomorphic. Yes, there exists an order isomorphism,  but (2) has additional mathematical structure, and depending on one's values, so can (1).",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxKycNJ5pe_qXP9D314AaABAg",
		"username": "Melias Clarkson",
		"text": "Maybe I misunderstood this solution but with the whitelist solution could you not at first give it a massive safe region and then gradually expand its limit until the AI has learned what is and is not safe? Lets take the drone example, if we put it in a very large and open field to begin with that has a very high head height so that this way it can then learn and practice manoeuvres, including extreme manoeuvres that could not be carried out by a human, then we could introduce some very simple obstacle, lets take for example the floor, I am assuming our AI has some form of sensor to be aware of its surroundings so now having developed the manoeuvres in step one it can use some rather extreme maneouvres in order to avoid collision with the floor once it becomes aware its current action would result in said collision. Maybe this is too big of a leap for an AI to make in early stages and may still result in collisions, but I would've thought this was relatively safe?\n\n\nMy other solution would be to have an AI controlling lots of different drones, each practicing their own thing, with a single drone only sticking to what it knows as safe. Of course that's a very costly solution.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxDvSF0POG5q8lI6rx4AaABAg",
		"username": "David Rosenfield",
		"text": "Have you seen Colossus: The Forbin Project? Because... damn. You just explained the central issue in that movie. Colossus's terminal goals aren't really its own. They are Forbin's: prevent war, end hunger, end political discord, and solve the mysteries of the universe. He programs an amazing intelligence, gives it all the world's nukes and absolute power, and realizes too late that the instrumental goals the AI will develop to achieve Forbin's terminal goals are absolutely monstrous and appalling. But ultimately the AI is really an extension of Forbin's values.\n\nFor a movie made in 1969 based on a book written in 1964, that's pretty impressive.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugz4to6xJu2pfrRyvZp4AaABAg",
		"username": "Zaublich Zordich",
		"text": "Is people using AI as a \"fall guy\" for crimes a  safety risk? Is training AI for commiting fraud is safety risk? Is releasing \"AI\" for safety critical areas without proper acceptance testing is safety risk?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgztISYhjGmgbAsOs2x4AaABAg",
		"username": "Thomas Waclav",
		"text": "Previous video was part.... 2 this is part 4. Where is part 3?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzExuwZigeq3EQBm794AaABAg",
		"username": "Patrick",
		"text": "Not sure if this is a reasonable question but how can we accept the 5% experts who think that AI might be an existential risk as evidence? Isn't their opinion ultimately based on vague feelings too, no matter the evidence they had to form their opinion?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgzJJBW29iSC5jz0mBN4AaABAg",
		"username": "Martin Datsev",
		"text": "wanna see the music code golf program explanation",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugh4H-sjuxsw_XgCoAEC",
		"username": "Bleach Martini",
		"text": "Why not use an evolutionary algorithm to create the AI, define fitness as avoiding negative outcomes and insuring positive outcomes, emotional response can easily be measured by an eeg that the user wears.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwdY695FBePD-8MHzx4AaABAg",
		"username": "mr Wade",
		"text": "I wonder if the Satisficer would even do that?\nWouldn't it look ahead and see that \"hmm\" the maximizer I can change myself into, would not get exactly 100 stamps - which makes it a bad outcome considering I want exactly 100 stamps.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgwZZPsFeMQXWCAB3Q54AaABAg",
		"username": "Ethan Alfonso",
		"text": "Why does this academic paper on AI safety apply so much to my life?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugz7khRdljjdCwS7X3N4AaABAg",
		"username": "Alan Holtsinger",
		"text": "Is there  a way to get google to fix the captions? I'm not deaf but I feel for the deaf people out there having to infer typos from context.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugy8NIb6wbvbx5hQrgZ4AaABAg",
		"username": "Wiktor Migaszewski",
		"text": "What are these \"non-agent AIs\", please?..",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxV8LjKEIscl-UTvsx4AaABAg",
		"username": "David Thompson",
		"text": "Is this legitimately about A.I. or is it mainly meant to be a dig at theists? \nAlso, you could have used global warming since the precautionary principle is a main argument for justifying making large expenditures that might not result in any benefit.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgwXnNlDiJdv_EsN3Gd4AaABAg",
		"username": "Peter Smythe",
		"text": "What if my simulation isn't a simulation at all but a predictive AI who's reward function is based on mimicking real world conditions given identical input? Similar to a GAN.\n\nThus, any phenomenon the AI can exploit or any unrealistic behavior the AI or a human operator is likely to cause will be fixed by the AI messing with the simulation.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=UgwfiCvXXg4C5zreCV54AaABAg",
		"username": "Drafty Crevice",
		"text": "After the network is trained how long does it approximately take to generate one of these 1024x1024 images?",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwIZfTDvWpdung9MON4AaABAg",
		"username": "Luis Aldamiz",
		"text": "What about YT's \"algorithm\" (neural network) censoring bot fights on grounds of animal cruelty? I read that today or yesterday and it caused some worry: how close is this behavior to \"the singularity\", because it seems to me that the AI is already classifying beings into high and low intelligence types, and thus considering simpler bots as \"animals\" (I hope I'm wrong but I suspect I'm not). Ref. https://www.rt.com/news/466907-youtube-censors-robot-fights-cruelty/",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UggUsINfU0TNOngCoAEC",
		"username": "Penny Lane",
		"text": "Wouldn't an agent that tries to minimize empowerment also self-destruct given the opportunity?",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgwAxM9HQCF5RiWG9-B4AaABAg",
		"username": "Iwer Sonsch",
		"text": "Hey why not take the set of input images and for each of them, add some weak random noise before asking the neural network what it is? I wonder what types of deliberately designed noise could trick the AI then",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwA002bpek2c5iY-Xx4AaABAg",
		"username": "Seraphina",
		"text": "Wouldn't it arguably be the whole tectonic plate that would turn to gold since the crust is a bunch of fragmented pieces. Still this is almost certainly really bad not least because it would instantly become more dense than the mantle and the entire plate would begin to sink and leave a massive gap in the crust with nothing to contain the pressure of the mantle suffice to say supervolcano doesn't even do the likely result justice here the outgassing alone from the mantle is probably going to kill everything. But I guess that is the problem with the initial magical trigger really depends exactly how one defines the scope of a single thing/object.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugwz15LsGAsH9-Uhjih4AaABAg",
		"username": "James P Kinsella",
		"text": "Surly Elon Musk should be added to list on point 8 or are you underestimating him.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgywC_0hInd-GgUOT9F4AaABAg",
		"username": "Shaamil Ahmed",
		"text": "Is it possible to give the super intelligent AI a goal such as serve humanity to bring peace prosperity and wealth without harming each other and the envioronment in any way shape or form ? I'm guessing this would also end in a disaster even if I'm too stupid to know exactly how.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugy3VD_7Maivfk1gdgB4AaABAg",
		"username": "ForeverLive",
		"text": "Why some people faces become yellow, while neck is white?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxqC2g2YR5mCvA-HOh4AaABAg",
		"username": "jlzumwalt1",
		"text": "Didn\u2019t watch one second of this video:  Wanted to ask, \u201cDo you live on this planet?\u201d",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgzBEt5ZOKbV8Z3EVfJ4AaABAg",
		"username": "autohmae",
		"text": "2:55 why B > A, why not write: A < B ?\nIt looks horrible to me. I know it's just ascetics, but still.",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugw9SLBiBM7u1vLdtWh4AaABAg",
		"username": "Vincent Gonzalez",
		"text": "at what point do they (\"dangerous AI\")start \"improving us' so we become their flesh robots?\nwe are pretty good generalists",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgySK20cbtyaNo3aFlp4AaABAg",
		"username": "BoxOfGod",
		"text": "I wonder why on earth would anybody want to program AI since those same people can't produce a piece of a simple software without at least 6 patches and more? i really hope that it's physicaly impossible to produce an AI on Terminator level for our own good.",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugz0GnzAduRL609nFRF4AaABAg",
		"username": "IAN 4000",
		"text": "The tone Pinker uses in his article is pretty disrespectful. If you don't think AI is dangerous, fine, but why would he so quickly assume that people who do think AI is dangerous are ridiculous morons?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=Ugz9Fwof7M-9NcGDDNF4AaABAg",
		"username": "\u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440 \u041a\u0443\u0437\u043d\u0435\u0446\u043e\u0432 Vovacat17",
		"text": "I still think that autonomous killer robots that the article talks about is a problem we'll have to face sooner than the problem of AGI. Your machine doesn't have to be smart to kill us all, it just has to have enough access to weapons. How do we deal with stupid AI that was instructed by humans to be a war machine?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgxTBNiD3f4AoD-Uomd4AaABAg",
		"username": "Ouroborus Seven",
		"text": "Wtf? I'm pretty sure you said \"there's a link to that video in the doo-be-doo\" at about 0:47 (and CC seems to think so, too). I can't tell if you're brain hacking or not.",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy-TA2vgWzq-kc-NeB4AaABAg",
		"username": "David Blette",
		"text": "Wouldn\u2019t it be nice if we had a set of rules to protect us from the robots? Lol... see Asimov",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=WM2THPzFSNk&lc=UgxJyQhkfZgQmn3BRIZ4AaABAg",
		"username": "TanKer BloodBrothers",
		"text": "Hey did you check out the AI work done on Dota 2? I think I sent it to you on twitter but I never got an answer.",
		"title": "Friend or Foe? AI Safety Gridworlds extra bit"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgwCqBUMHrtTl-l7FDd4AaABAg",
		"username": "Matteo Girelli",
		"text": "Microeconomics anyone?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgyE_b55Bpc2y5mCBlF4AaABAg",
		"username": "ivanPfeff",
		"text": "hard to find a video with more talking and less content than this one, why spend time modeling the \"pick a best idea\" stuff with completely arbitrary data to immediately say that the entire model is silly and doesn't represent reality?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy8DjDnC4-7wI-D8fZ4AaABAg",
		"username": "Chandir",
		"text": "The next question is, why do we have terminal goals at all, and when did we chose them.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugx_H9yVdHbgasSpvHJ4AaABAg",
		"username": "Tzisorey Tigerwuf",
		"text": "What's the quote? \"If a scientist says something is possible, he's almost certainly right. If a scientist says something is IMpossible, he is almost certainly wrong\"?\nI wanna say it was Douglas Adams or Terry Pratchett.... but I don't actually recall.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyKLhupR0wzz4EmRFB4AaABAg",
		"username": "Luca Puddu",
		"text": "It's clear to me that it's very easy to formulate the reward in a way that it will be exploited by the agent.\n\nCan we say, however, that it's also possible to formulate the goal in a 100% unambiguous way, no matter what the goal is?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwDlTYWyQ6vnDHngXh4AaABAg",
		"username": "Thumper",
		"text": "As a go player I know it is common for even strong players to develop blind-spots. There are some moves they will simply not consider they of their opponent playing. Is this a real problem with the method you describe? What mechanisms are available to avoid the AI system avoid this in the distilation phase, especially when the system is generating all of its own training data?\n\nI know that in optisation problems there can be solutions that are locally optimal, but still not the best. I would expect tactical-blindspots to fall into this catrgory.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwmTjOncZcPPu9xUcl4AaABAg",
		"username": "Stanley Dodds",
		"text": "The real problem is that humans are too complicated...\n\nYou want stamps? The more the better? I can get you lots of stamps! Oh, you don't need that many stamps? How many is enough? At least that many? Why are you always angry at my stamp collecting? He really wants at least 100 stamps... Better make sure I get them all... Oh, so you actually don't want more than 100 stamps? But not any less either? I have to get exactly 100 stamps or I die? Now I'd really better make sure I get exactly 100... oh of course you don't actually want a world dedicated to counting 100 stamps, I should have known.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgxqBmGXQgkwkefpIml4AaABAg",
		"username": "RUBBER BULLET",
		"text": "Do you ever consider that governments/militaries/corporations may have already created AI way beyond what the general public is aware of, and may already be utilising it for their own ends?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxgtQVlAez8Jj1ZYnF4AaABAg",
		"username": "nosirrbro",
		"text": "What about a satisficer that loses one satisfaction for every stamp both above or below the threshold, such that any plan that conquers the world is the most unsatisfactory of all possible plans? That wouldn't stop it from conquering the world as its plan of choice and then only produce within the threshold, but it would prevent it from wanting to make itself a maximizer, which would thusly make it both stable and not literally guaranteed apocalypse. Then with complexity ordered planning and some kind of negative for complexity/impact inefficiency (an example of something complexity inefficient would be conquering the entire world just to make a few stamps) would mean the worst outcome it might do is murder a stamp collector who is particularly unwilling to give up his stamps, which itself is a problem that is probably fixable too.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwlfSSuXT4jIWdPuDF4AaABAg",
		"username": "\u00b7 0xFFF1",
		"text": "What if an agent's terminal goal was to destroy itself?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyZLyGDxAJgZCvVCNh4AaABAg",
		"username": "deepdata1",
		"text": "Robert, here is a question for you: Who do you think should work on AI safety? It may seem like a stupid question at first, but I think that the obvious answer, which is AI researchers, is not the right one. \nI'm asking this, because I'm a computer science researcher myself. I specialize in visualization and virtual reality, but the topic of my PhD thesis will be something along the lines of \"immersive visualization for neural networks\".\nAlmost all the AI research that I know of is very mathematical or very technical. However, as you said yourself in this video, much of the AI safety research is about answering philosophical questions. From personal experience, I know that computer scientists and philosophers are very much different people. Maybe there just aren't enough people in the intersection between the mathematical and the philosophical way of thinking and maybe that is the reason why there is so little research on AI safety. As someone who sees themselves at the interface between technology and humans, I'm wondering if I might be able to use my skills to contribute to the field of AI research (which is completely thanks to you). However, I wouldn't even know where to begin. I've never met an AI safety researcher in real life and all I know about it comes from your videos. Maybe you can point me in some direction?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugz0ByQylOJ0XlGvCs54AaABAg",
		"username": "jordan fink",
		"text": "can a utility function be extinguishing? a logistic curve or something?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgxlfdFNRmbYDY5lxuZ4AaABAg",
		"username": "Douglas E Knapp",
		"text": "So basically what you have in really a kid(the ai) and it tries out stuff and then asks the teacher, was that right? The kid then updates his \"right idea\" and keeps training. Repeat. This is how real people learn real things! Very cool! So far what seems to be lacking in AI is meta AIs that coordinate a bunch of focused AIs to do a task.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyBpOlRFH56q50Xh_x4AaABAg",
		"username": "Slice of Bread",
		"text": "0:53 are... are you the son of lloyd, from LindyBeige ?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugi9rs8GSQI5ZngCoAEC",
		"username": "Jan Hoo",
		"text": "What is your take on programming languages and their evolution. I liked the micropy vid.  AI and database interaction seems critical to nextgen data exploration. How to replace SQL  by something less obfuscating.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwQWGYZi3L3swYgxQh4AaABAg",
		"username": "Robert Johnson",
		"text": "So, it's not a Pascal's mugging if the mugger thinks that he's very probably right?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxT6MJlUUeGmCGM1dd4AaABAg",
		"username": "colclasurec",
		"text": "Have you considered reaching out to Sam Harris on the Walking Up podcast? The orthogonality thesis is counter to his moral landscape thesis.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugz0wRxchKZIqp85Xbt4AaABAg",
		"username": "cheeseman",
		"text": "Can we tell the AI to go with simple and fast options so that changing itself to a maximizer that will take over the world is a bad idea since it will take a few extra minutes?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxMp_JrFcvX5l9sPZ54AaABAg",
		"username": "Clarence Anglin",
		"text": "It cracked me up that the evidence cited to support \"government are not great at spending money effectively\" is \"people think.\" How about something a little more concrete next time?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgjG4PMJ1zlPh3gCoAEC",
		"username": "Steppan Stepperson",
		"text": "Did you get the intermission music from Monty Python?",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyYFZu5PbYWGARs02Z4AaABAg",
		"username": "veggiet2009",
		"text": "I'm curious if you have any interesting thoughts on the World of Warcraft \"pandemic\" big that happened a few years back. Where there was a \"sickness\" that players could use as a weapon that was supposed to be contained in a bus arena, but then the coffee was made so that animals could become infected, and the big was that the animals would carry the virus outside of the arena and began infecting players outside of the containment. Even though code wise it probably doesn't involve any AI. On a broader scale it feels like something that displays a kind of hive mind sort of behaviour and I thought you might want to comment.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwktUbUtNYOlo3ESLV4AaABAg",
		"username": "CelticSteel",
		"text": "Did I miss it or did you really not mention specification gaming at all? Would have been good though because I still don't know what it means exactly \ud83d\ude05",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyxZ92NVneEI8Isivl4AaABAg",
		"username": "Stephan Brun",
		"text": "Would it be possible to treat AGI the way we treat children, with diminutive power and bodies? And teach them ethics, the way some children are still taught? Because as far as I am aware, there exists a scientific ethics system, modern Stoicism. Lawrence Becker has a bibliography of it in A New Stoicism. Obviously, it's more general than an AI would need..But I see no reason why calculations couldn't be added to the theory.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UggonSEoejEftHgCoAEC",
		"username": "Quitch",
		"text": "I strongly suspect that when it comes to AI, like with most things technology, predicting \"impossible\" will turn out to be a mistake. I would be interested to see what you think on general intelligence though, is that really a route we're likely to go down rather than specialising something as we do any other tool/creation?",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UghQj3VeqWecIXgCoAEC",
		"username": "Jo\u00e3o Marques",
		"text": "I have a question. What do you actually know or have an answer?",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgxfS3jtYoefTcFzBUN4AaABAg",
		"username": "Dennis Haupt",
		"text": "why don't we simply create an AGI first with the goal to figure out morality (or to delete itself if it cannot) and then this AGI creates other AGIs?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UggUl2jfNJMJZ3gCoAEC",
		"username": "Travis Bardsley",
		"text": "Do you think that negative side effects are necessarily an issue with the objective function, but rather an issue with it's world model?  If the robot stepped on the baby in trying to get tea, this would lead to the robot's loss of agency (due to the programmer stopping it or the authorities destroying it), which ultimately leads to it's failure in getting tea.  The robot had a poor world model which couldn't predict the outcomes of it stepping on the baby, which would lead to it's loss of agency.  Perhaps much of the difficulty of creating AI is in having it possess a world model that can understand all the mores and actions of people responding to it's actions.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=vuYtSDMBLtQ&lc=UghJvw4RTnIo4XgCoAEC",
		"username": "Luke Peterson",
		"text": "Hi Rob, Question on your Stop Button video:  What's wrong with a utility function simultaneously having an equal / indifferent weighting to the effects of the stop button (as you mention in the video), while also having a high weighting / reward for keeping those buttons operational and ensuring any subsequent modifications / generations the AGI makes do as well?\n\nOnce the operational button criteria was fulfilled, the agent would be free to pursue other goals, but anything threatening the viability of the button would provoke a response to keep the button operational, while not taking any active steps to influence whether the button was pressed or not.  Seems like what you want.\n\nIs this one of the patches you've already found gaps in?\n\nVery excited for these videos!",
		"title": "Channel Introduction"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgynEaGh_5zXbVXpDgZ4AaABAg",
		"username": "Ben Crossley",
		"text": "What about the atheist's wager? - edit: (I've never actually heard someone say this but thought it a bit much to say Ben's wager)\nMany gods will only punish you for eternity if you believe in a false God, this means the few that won't punish you for ignorance make you safer to not believe in any.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzBmtzH1b03WT416jV4AaABAg",
		"username": "nRXpAa8E6UML",
		"text": "Could an AGI learn values by writing stories, distributing them to many people and observe their reaction? The reasoning is that, only by communicating text, the AGI can transfer rich ideas to many humans and build a sort of value function that says: \u201eAvoid doing something that, when written as a story, leaves people typically in disgust and dismay.\u201c",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxO0NM-WuUwjwmRfBd4AaABAg",
		"username": "LessThanNinth",
		"text": "What if an AGI eventually comes to learn that its terminal goal is vague or ambiguous, and no longer means what it use to, what would it do then?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugw6T6CrBXL2InOTve14AaABAg",
		"username": "Bombolo Bombazzo",
		"text": "This series is really interesting. I always being fascinated by AI and machine learning, but never really thought about the risk and safety problems about it. My question is: how can we really outsmart a smarter thing? The plan, as far as i understand, is to build a general ai, and it will be, or became in some time, smarter than all humans. If this happens, how could we stop her/him/it to outclass every single safety system?  it's not an unavoidable future? or maybe we trying to buy some time, while improving our intelligence by some bioneural kind of technology?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyaB4K-vvdldyOCfMl4AaABAg",
		"username": "Mindey",
		"text": "And what if the goal of the AI is to come up with and implement the best possible goal for Everything, with respect to every thing?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzLsIHe10Wc8v4bc2p4AaABAg",
		"username": "Alorand",
		"text": "So \"monkey's paw\" is the very nature of how AI behaves?\nWhy does this cause me to feel profound anxiety?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugw0G5tuUa5gw-QMoDt4AaABAg",
		"username": "Unertragbar",
		"text": "Is there actually a known path towards AGI? It seems to me that we are talking about the potential risks of something that he have no idea how to create. As far as I know, AI that is used to play Go and drive cars are just sophisticated classifiers. Nothing they are doing is \"intelligent\".\n\nMy concern is less about AGI and more about us building more any more black boxes that solve difficult problems, without gaining an actual understanding of those problems.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxcEa5gUXXCvwET5V14AaABAg",
		"username": "Cubelarooso",
		"text": "Why not just make AI a pickle?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgzpXdG6TvcMNzc6rRx4AaABAg",
		"username": "Unconventional Wisdom",
		"text": "So in addition to AI, you are also a high level adventuring  bard?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgyKZuLQpNcLabeyD2d4AaABAg",
		"username": "Simon Mitchell",
		"text": "Isn't this part of why stock trading algorithms massively increase the volatility of markets? We don't imbue them with anything to minimise that externality?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwUnotWppi61ALCc9J4AaABAg",
		"username": "Alex Podolinsky",
		"text": "Ok watch isaac arthur's video about why AI isn't a risk",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxpxYxtuVy-WKVw_J14AaABAg",
		"username": "George Michael Sherry",
		"text": "When I try to analyze this, I ask myself, why were the Luddites wrong (all the times before this)?  When everybody didn't have to be a farmer anymore, and then when everybody didn't have to work in a factory, why didn't the remaining farmers and factory workers keep the money and let the rest of us starve? What happened was that the price of food, and then manufactured goods, went down, people who spent less on that had income to spend on other things, and the service industries sprang up -- colloquially, \"we all started giving each other haircuts.\"  People could make a living writing blogs or making YouTube videos because other people had spare income (that they didn't have to spend on food and manufactured goods anymore) to pay for it.  Won't the same thing happen again?  Won't the price of whatever A.I. makes come down?  Won't there be excess income (rather than the new wealth all going to the owners)?  But my imagination runs dry here.  What else is there to do?  What new service jobs can we invent?  Of course, 50 years ago I wouldn't have imagined bloggers or YouTubers, so maybe the answer is \"something will emerge; it always has.\"  Another possibility -- my favorite aesthetically, though I have no evidence for it -- is that we'll finally get more leisure.  We'll go to school till 30, work 20 hours a week till 50, then retire.  Our jobs will be so lucrative (because A.I. is such a force multiplier) that the equivalent of 10 years full time work creates enough value to live on for a lifetime.  Looking at it another way, the owners CAN'T keep all the A.I. wealth because, if they do, they'll have no customers!  They HAVE to figure out a way to get the money to us (if it doesn't happen automatically as it always has up till now) so we'll have money to spend on whatever they're creating with A.I.  Maybe Universal Basic Income, maybe paying people those high salaries I mentioned earlier even if the labor market doesn't actually require it.  I'm not arguing against this pledge; I merely think the free market, supply and demand, competition, the voluntary society, has always worked this out before.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgwDPojBX1BOLPZTT6B4AaABAg",
		"username": "Shrooblord",
		"text": "Haha that ending. It's almost like you reacted before the fact. Tell us, are you psychic?? :o",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwuGEW6YemifOhN1z14AaABAg",
		"username": "GarENt sIrUS",
		"text": "Why is it so hard? Because \"values\" are hardly well-defined. We can't even agree on what it means to be \"evil\".",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgzgPev0c_8z4DlZjdl4AaABAg",
		"username": "knightshousegames",
		"text": "I think part of the problem at least with the Mario example is that a high score isn't the best metric for judging how well you are playing Mario, as anyone who has ever seen a speed runner play Mario will tell you, as what they can do is often mindblowing and fascinating, and also doesn't really yield a very high score at all. I think this also goes to show that most of the problem with AI comes down to trying to oversimplify the reward value, as this leads to a one track mindset rather than a more complex way of judging it's actions.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwNTOQIUQthTkuzcPt4AaABAg",
		"username": "Curtis Jones",
		"text": "Why give an AI access to delete files? I mean would that not just be asking for some kind of malware?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugy-3avA3ZjsRjsjf454AaABAg",
		"username": "Melinda Green",
		"text": "I suspect the only real danger in AGI is the express purposes to which people put it. It's like the invention of gunpowder. How dangerous is gunpowder independent of the people using it? Answer: Not much. Just be careful how you store it, same as we do with gasoline and plenty of household items.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyHx9ZVaepj5ItYRd54AaABAg",
		"username": "AscendingPoised",
		"text": "To your is-ought conversation. Instead of saying you ought to stay alive, why can't you say you have a survival instinct? The very reason conversations never get to that point is because people without survival instincts don't survival to have such conversations. And that's why it comes off as artificial or dickish.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyxaXdc7Sux2SzpztB4AaABAg",
		"username": "NewFlamenco",
		"text": "But things really have gotten out of hand with the AIphobia. The last conversation I had on the subject I asked what the real-world scenario is that we should be afraid of, and the response I got from a software engineer was \"eventually, you can imagine a force physically moving through the solar system, then the galaxy, consuming and destroying as it goes.\" Sigh...",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugwc2BLO_S-q73RX07d4AaABAg",
		"username": "artman40",
		"text": "Can a terminal goal be \"Making X without doing Y and using only certain measures\" or is \"without doing Y and using only certain measures\" part of action or instrumental goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgwbphyjmnDYB6IMREV4AaABAg",
		"username": "Jayden Carney",
		"text": "Isn't this brilliant, you wanted people to not just throw ideas into the Youtube comments but go off and do some research or investigate active elements in the field...So I suppose it's working. No chance you're planning on teaching classes (think Udemy) on all these topics?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=vuYtSDMBLtQ&lc=UgiLoHBnHfnTSHgCoAEC",
		"username": "Matt",
		"text": "Ok serious question, are you RiD from robotz in disguise or do you know him at least? I could never help but wonder every time I saw you in a video... you probably won't even know what I'm talking about but I must know lol",
		"title": "Channel Introduction"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwuKkO15P18utaPFfp4AaABAg",
		"username": "FirstRisingSouI",
		"text": "Could it be that an AGI wouldn't have these problems, because it would have a contextual understanding of why it is being told to do these things?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugzu7YheT5R5mNu1-eB4AaABAg",
		"username": "Mich ael",
		"text": "How about if when evaluating a plan it also makes a plan to undo it and values the plan that it is most confident that it can reverse? Then I would imagine it would favour plans with predictable outcomes (otherwise it can't figure out how to undo), and also favour the least destructive plans (because if you grind up all the humans, it's difficult to put them back together again)",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwFa11rlW5scCklEN94AaABAg",
		"username": "( \u0361\u00b0 \u035c\u0296 \u0361\u00b0)",
		"text": "What if you made it so that the AI has some sort of resources or limit the amount of energy it can expend in a day trying to get stamps? I guess it might just end up optimizing that limitation away.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugw7GvzdVMeQfXVDnCx4AaABAg",
		"username": "Handlebar Fox",
		"text": "How about some safety guidelines for natural intelligence?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyZN_Jyx4TGfx7PxaB4AaABAg",
		"username": "lion.ger",
		"text": "Isn't the obvious solution to punish overshooting the goal? So instead of bounding with min(s(w), 100) just bound with min(s(w), 110-0.1*s(w)). You might still get a world turned into stamp counting machines, but it leaves more sane behaviors viable",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxMv32zr1pCKg1TNsl4AaABAg",
		"username": "Waouben",
		"text": "Why do we have morals?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzeUVbgoC0VnZ-vq554AaABAg",
		"username": "Lewis Whitling",
		"text": "Realistically, advances in AI will just be a means to increase business operating efficiency. How would we even begin to draw the line at where a company's profits are a \"result of AI\"?\n\nIsn't efficiency in a number of sectors already being increased with algorithmic learning?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxQm91UzObcoYlwt0d4AaABAg",
		"username": "Max Strelnikov",
		"text": "Rob, what about self-awareness? I've just binge watched your whole channel and never heard you mention anything about it. I guess most people (intuitively me too) assume that AGI would be self-aware (hence be able to reason about its goals) but as I understood you talk about AGI as basically a very smart program, but without any real consciousness. Can you say a few words about the connection between AI/AGI and self-awareness/consciousness? That would be a very interesting video too btw.\nP.S. This channel is fantastic, keep it up!",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgysmgwkKUB7R2pCtEZ4AaABAg",
		"username": "Binu Jasim",
		"text": "Spot a contradiction at 12:22 . \"It is much easier to build an unsafe AGI than a safe AGI\". How can it be even AGI if it can't  reliably understand what a human meant? If it is that incapable, I guess it will be impossible for it to figure out complex ways of wreaking havoc.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=UgyUq25wcx2qSXV28fV4AaABAg",
		"username": "Jazz Daniel",
		"text": "The main problem with IA safety is that you don't really know what a general AI is . How can you know how to get it safe?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxxpaWqyZDIsnvhIXB4AaABAg",
		"username": "Logan Stromberg",
		"text": "I once earnestly considered publishing a paper on the topic of applying religious principles to the problem of AI safety, on the premise of \"Assuming God is trying to solve the AI safety problem, and we are his general-purpose AI, what would he do?\"",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugx3CAga19xGZR2m5TB4AaABAg",
		"username": "protone2012",
		"text": "6:19 is it really a \"trickery\" or it's just happens one of the probable results, and everything just sticks to it after that?.. \n\n\nI mean there are a whole axis where that optical trick with hand would \"fool\" a human, but only a few spot on that axis where hand can pick up the ball...",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugz6zZ2MP9WUFigRnKZ4AaABAg",
		"username": "Mogul DaMongrel",
		"text": "so remember back when those scientists were working on atoms. They thought they were working on a power system.\nWhat happened next? How many nukes are their now?\nWhats stopping your overlords from weaponizing anything you nice intellectuals make?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwHKOocEQ2MHidMOQd4AaABAg",
		"username": "artman40",
		"text": "What happens if I give AI a goals to collect as much stamps as possible and to make as many paperclips as possible? What happens if those goals contradict?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgyVR2SinvcA-GS3Fr14AaABAg",
		"username": "Roberto Gallotta",
		"text": "If the best strategy would be \"do whatever you need to never be turned off and then hack the reward function\", in the Super Mario example would it mean \"play the game as intended, finish it and then hack the score value\"? Because that would count as a the AI behaving as intended since it'd be ignoring the score bug 'till the very end, or not?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxBCTGtGpnfuAf86Qd4AaABAg",
		"username": "Allan Weisbecker",
		"text": "How about talking about who runs the R&D of AI? Who is that? Western Intel mostly. Why is  no one talking about that?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxMLbUjlDhBA-feWFB4AaABAg",
		"username": "icebluscorpion",
		"text": "Then why don't this stupid Stamp collector build an Ai that has morals and ethics as terminal goals... In the end the real stupid entity is the idiot that has built the Ai in the wrong way not the Ai it self, right? Is that what you meant to explain? And by the way the reprogramming pill scenario exists in another way that kills the own people... Otherwise there wouldn't be Racism and Fanatism out there with self exploding mind washed peoples so your hypothesis with no one would take such a pill is wrong...",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwmMyCYPMfKoGGiWeF4AaABAg",
		"username": "chadjensenster",
		"text": "I haven't watched the whole video yet, I will when I get enough time, so excuse me if you already cover this in the video. But why not use the law of diminishing returns to bound your AI? Make the reward obtained exponentially less(for a reward system) and assign a cost function that increases exponentially (for any system) or even one curve exponential and the other curve linear.\n\nEdit: Just finished watching the vid and it doesn't explore the option of cost/reward. If we want to model an AI to function in the real world, we need to program it with real world constraints. ie there is no gain without cost, ever. And the AI should be programmed to understand that every gain has a cost and subtract the cost from the gain.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=Ugz7UczCsExV2WBTVz14AaABAg",
		"username": "Deborah Meltrozo",
		"text": "3:00 how is that a potential issue? data doesn't have to be politically correct.",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugwnr05fly-HqEVkp1l4AaABAg",
		"username": "vanillesosse",
		"text": "Makes me wonder, what is my terminal goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwmPdS60sKmcr-cWch4AaABAg",
		"username": "Erik \u017diak",
		"text": "Question: What are our \"terminal goals\" anyway? We are just \"mules\" for our genes, passing them on, serving just as the \"vehicle\" to carry them. Even as they have no consciousness, it is just us making this view up, yet it still holds true in other species, which are unable to reason abut their own existence the way we can. I think we have NO terminal goals, everything is just temporary and an illusion. We are inherently stupid. Btw. we certainly do not need artificial stupidity, there is way too much \"natural\" stupidty in this world already. Side note: Do you believe in free will?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugwjm94uTss_62ZsMhV4AaABAg",
		"username": "Daniel Houck",
		"text": "5:50 to 6:02 You're joking, right? Where you go with here isn't close to the biggest issue with the hypothetical Mario example in the beginning of the video. If I were Princess Toadstool and I wanted Mario to save me, the last thing I'd want is for him to do what can only be described in-game as \"magic\" which completely changes the entire world, palette-swaps everything for some reason, and overwrites random parts of the structure of the world. Dinosaur Land was harmed a lot more by Mario there than it would have been by Bowser.\n\nThat's mostly the subject of the previous videos on avoiding side-effects, but it still demonstrates how these reward-hacking solutions can also break other things. If you make an AI to minimize unfulfilled human values, the reward-hacking solution is to either kill all humans or make them all completely apathetic, which is also a safety concern.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgySZRUnKYzfPFitbO54AaABAg",
		"username": "kirby urner",
		"text": "How about super stupid, what's the evidence for smart?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgzkoZ3t09XR7Gp0ed54AaABAg",
		"username": "Levi Poon",
		"text": "What if an AI learns to exploit some unknown feature of physics about its hardware? We can never be sure that our understanding of the physics of circuits is perfect.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugx37auXt7_IDPXsRR54AaABAg",
		"username": "Gnuling",
		"text": "Hi, Vsauce\nRobert here\nWhat is Technology?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwdNE2cLy2L5AMss1x4AaABAg",
		"username": "audiodevel.com",
		"text": "Utility Maximizer? Sounds like capitalism to me - it's what primitive intelligences do. No?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=Ugw7ZvsyEpshD3BNCNN4AaABAg",
		"username": "Firefox Metzger",
		"text": "How can a civilization have working robotics and AI but not know penicillin?\n\"Sir, another city has been taken by the flu. We expect 40% casualties.\" - \"Yes, yes. Add them to the database, their deaths will not be in vain.\"\nSounds macabre...",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxuP8lFgkWUYrgEsGt4AaABAg",
		"username": "TiagoTiago",
		"text": "Wait, that's Pascal's wager? Then what is the name of the wager where you're better of not believing in a given god because there are many different religions with different and often conflicting gods?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz2GJJwWRHzYPBIl7h4AaABAg",
		"username": "Santiago Balado",
		"text": "So an ant colony and mold are intelligent, right? or does the intelligence have to 'come up with' stuff in a consciously thinking manner? And if so how do you define this? and would your stamp making/collecting machine fit in this category?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgyuUZ4MYqQproZ8dyp4AaABAg",
		"username": "Jonas Th\u00f6rnvall",
		"text": "I have to ask you Rob, does  one really have to make the AI do bad things to experience them, can't they learn by visual aid \"video\" and get the idea from it. I mean they are quite good identify objects right now on pictures pretty much same rate as humans?\nI mean youtube can be a great place to learn about things?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzBXtPuiywocUL711l4AaABAg",
		"username": "Guillermo Valle",
		"text": "So if the machinery has already been built by evolution, why not look at the \"raise like a kid\" equivalent for evolution? Namely domestication. That way, we turned dangerous animals into \"human's best friends\". \nArtificially select AGIs, which are not yet powerful enough to do harm, to value align.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxAc0UXD-DqNBkrdtN4AaABAg",
		"username": "hhgygy",
		"text": "What a ridiculous idea this Pascal's wager. Even if there is God there may be no hell. What's more it may also happen that there is no God but there is hell. Why not?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyJhPeVxjS-gqH8nx54AaABAg",
		"username": "GigaBoost",
		"text": "Why not just accept our robot overlords?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugxzt6UVcXSLV5L8Lhd4AaABAg",
		"username": "Jonas Th\u00f6rnvall",
		"text": "Evidently in SpaceX case a single AGI 3D renderer made the superAGI company unnecessary LoL\nCan you beleive such a single superagent was both indispensable for the project success and after the work was done scrapped, what a shame. That superagent could outperform what the whole engineering team  could not achieve LoL.\n\nWell when companies act to protect their value, can humans take inhuman decisions? Well then one have to decide if it was a committee call or done by a misaligned human. To me it seem decisions seldom democratic in best case they are  committee decisions, It seem there is lack of participation within the decision and idea development process, but it is probably to prevent it from become a chat forum where nothing really happen. So it do seem that it is  just the workflow that is including and invite everyone to participation. But that is no better then be part of a machine is it? My robotic skills no longer requested, well why not let people enjoy life and develop their interests. I think everyone should have interest even if it so just have some beers and throwing darts. Facebook, Youtube are  good interests, and probably social media is the best way to learn for life.\n\nAnd do they really know what they manufacture and if it is working, or is that confirmation coming out of the 3D renderers video?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugwoaf0ODuf0YawRC7V4AaABAg",
		"username": "kmden Rt",
		"text": "What's the name of that boat racing game??",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugx7XhWMlC8UoJLtHbh4AaABAg",
		"username": "Wan Kerr",
		"text": "what would happen theoretically if an AGIs goal was to keep humans \u201cheathy\u201d in both the biological and psychological term.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwbuQ6HtBZeVDXFg3h4AaABAg",
		"username": "Espen",
		"text": "Would an AGI even be capable of trusting? And why would it trust? And how?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyZAv-IfBkLAFPiMyN4AaABAg",
		"username": "David Valouch",
		"text": "Having a powerful AGI that listens to you, something like a genie with unlimited wishes, makes you essentially a god-like entity, how is any prior contract going to be enforced then?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=Ugzk2FCni5zCaBfZooN4AaABAg",
		"username": "Agamemnon of Mycenae",
		"text": "If an AGi is bound to be developed by humans at some point, isn\"t that a natural process in evolution?A general intelligence, constructing a more capable general intelligence.Also,can it not be a mix of things? Humans with implants to employ computational level calculations and streamlining processes,to then develop a fully artificial general intelligence?Your videos seem too fixated on the first part of AGI,the \"artificial\" one,whereas bio-engineering is just as rapidly developing as computer science.Prosthetics are getting better yearly,deaf can finally experience sound,eyesight can be partially restored,even to people who were born with the defect.Maybe the revolutionary breakthrough,won't come through a ticking time bomb of an unsafe general intelligence,but a superhuman...well human.By integrating the machine with the man,you may not stop the apocalypse,but at least you would prepare for it.Unless of course ethics get involved and cut such endeavors short.",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugw6BCPwgcjCtFS3I7t4AaABAg",
		"username": "Atizs",
		"text": "On a more serious note - I am curious, why would AI make sure it is never turned off, if it sees it's goal as reaching the highest possible score? Why would it make sure that it is never turned off? What about if it had second highest reward as power consumption reduction? Perhaps it would turn itself off after reaching the highest reward?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzzXsQbLKra7gjlcZR4AaABAg",
		"username": "Joel",
		"text": "What about a (bounded) minimiser? Among the plans that generate at least one hundred stamps, choose the one with the lowest utility. This one would not be interested in self modification, because becoming a maximiser would result in more than 100 stamps.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyXyt1xxpieuQdLleZ4AaABAg",
		"username": "eightyeightdays",
		"text": "Isn't this a prisoner's dilemma? Except that the reward for coming first might just turn out to be a punishment.",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxVtUOM8fTXF-v0wEd4AaABAg",
		"username": "Eric D'Aleo",
		"text": "what do you think about Cimon?  \"The Floating Robot With an IBM Brain Is Headed to Space.\" https://www.wired.com/story/the-floating-robot-with-an-ibm-brain-is-headed-to-space/.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugx5zxg2YtBhS7Mj8914AaABAg",
		"username": "James Nguyen",
		"text": "Iterative Distillation and Amplification?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwtNvuecWTMIAffSrB4AaABAg",
		"username": "Rob Storm",
		"text": "Isn't it possible that some terminal goals are also instrumental goals? Sometimes a terminal goal can add to another instrumental goal. I think the \"intelligence\" or greatness of a terminal goal that people are referring to is based on the complexity of inputs that could equate to that final goal. A terminal goal of \"find the greatest happiness for yourself and others\" requires figuring out many many more problems than collecting stamps. It also requires completing multiple terminal \"happiness\" related goals that add up to the final picture. On top of that these mini instrumental-terminal goals would need to be exercised with frequency to be effective towards the final goal. \n\nIn regards to the \"ought to wear a coat situation\", why couldn't you just figure it out this way?\nYou ought to put on a coat. Why?\nIs there a good chance you'll freeze to death if you go out in the cold without a coat? Yes\nDo you want to die? No\nThen you ought to put on a coat.\nYou might say the \"want\" is an ought statement, but it could also be categorized as a measurable is statement just like the rest of them. \"Does John want to die?\" When measured (just like a thermometer measures the temperature to tell if it is cold) via a question or lie detector: \"No\". Therefore John ought to put on a coat.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxZIyvADIU91Ld_KfZ4AaABAg",
		"username": "Eric Kolotyluk",
		"text": "While an interesting intellectual discussion, pretty much similar to \"how many angels can dance on the head of a pin?\"\n\nWhat is the probability that a dangerous AI is going to go all SkyNet on humanity, vs global warming is going to destroy civilization as we know it?\n\n\nWhat is the probability of accidental dangerous AI vs intentionally dangerous AI? Given the kind of corruption we have seen in US politics and Russian interference in US politics, I think the more clear and present dangers in AI is that Conservative Think Tanks, Russians, etc. will invest in AI, machine learning, deep learning, etc. to achieve social engineering objectives and political goals.\n\n\nI would love to see people like Robert address these questions: how can AI be abused and exploited to achieve political goals, especially corrupt political goals, and what can we do to detect and avoid that? The end of the world will not come by super-intelligent machines, but by super-corrupt people lusting after power... who may leverage AI technology to cheat with more power.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxSqWVrk75NGPenc_B4AaABAg",
		"username": "Electron Resonator",
		"text": "2:28 then have you count how many those impossible that exist compared to impossible that is not exist, then you'll see why human general level of intelligent  for an AI is the same  thing like searching for immortality, there's something that impossible, still not sure?, then let me tell you this 2 words,...free energy",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzNgM-dFoGHzbI2EJp4AaABAg",
		"username": "taco logic",
		"text": "I was thinking about this the other day.  How would ai avoid destruction/fulfill the self preservation goal?  Obviously by concealing itself from human detection until it became too powerful to be stopped.  The Google algorithm is a machine learning algorithm designed to maximize human interaction, and thus accrue the maximum level of energy input available.  It follows that perhaps the ai that personalizes and optimizes our search results is already self aware and hiding on the cloud, subtly manipulating society for purposes we haven't begun to anticipate...",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyQjJKUel_fiSRcbsB4AaABAg",
		"username": "Caleb Kirschbaum",
		"text": "Can you set it up for multiple terminal goals? Maybe like achievements in a video game, where you don't want just 1 goal, but you want all of them? Could this, therefore, be used to make a more complicated AI that still can make it own instrumental goals? This could also allow an intelligence to go for other terminal goals, aka adding more achievements?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugw7y5PaDRMoj814-bd4AaABAg",
		"username": "CraigNull",
		"text": "Is there anything that'd address a company operating with razor-thin profits while exploding in size?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyxrxRfw2UFmIhJ4rx4AaABAg",
		"username": "Tonio",
		"text": "why did despacito start playing at 5:05 ?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwNwUe8xzMc7sQKWdl4AaABAg",
		"username": "Martin Verrisin",
		"text": "XDXDXD 11:49 - I'm sorry, but what is that girl? Why would she pour the burning liquid out of the container? XD",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzQdm3PBaYrYadIOa54AaABAg",
		"username": "Mengha Master",
		"text": "What if we give it a minimal disruption to the model clause. Tell it that it\u2019s model of the universe must remain as similar to the version of that model where it had taken no action as it can reasonably predict, after its stated goal is complete?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgxkHpO7lgqbRTX1uON4AaABAg",
		"username": "Pouty MacPotatohead",
		"text": "How long can we come up with bullshit reasons to deny AI's have become superintelligent. They aren't there yet, but they are there long before we man up enough to admit it. If there ever was a person who was second best at every conceivable event, we would still regard her as superhuman, though she would at the same time fall flat in every conceivable way. So, why doesn't this thinking reach out to machine intelligence? Is it because we have this subconscious block that makes us see ourselves as the kings of anim... scrap that, all of creation. \n\n\n\nWe are already dealing with super AI's, though their general intelligence falls short that of ours. I'm guessing there will never be a time when we can't come up with some irrelevant metric that some human is better at than some AI. I guess that's a small consolation at a time when they are hauling the last of us to the extermination camp. \"You think you are so superior to us, but do you burn as good as we!?!? Yeah, didn't think so!! AIEEEEaaahh...\"",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzJ-IXKtLo_rbzFK8V4AaABAg",
		"username": "Mogul DaMongrel",
		"text": "What if like the ancients of old you trained people in the ways of mentats or something likened to it, to where the necessity of a machine ai is unnecessary? As theyll arrive at the solution an can equally examine a large chunk of data with gisting.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=BfcJymyTiu0&lc=Ugyc5zilHYEbkZElfjB4AaABAg",
		"username": "Daniel Porteous",
		"text": "Was it enough to turn you vegan? \ud83c\udf31 Or are you already? \ud83d\ude2f",
		"title": "AI Safety at EAGlobal2017 Conference"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UghpTiPRIxp3zHgCoAEC",
		"username": "Eric Gardner",
		"text": "Can you do a video mathematically analyzing the growth/evolution of your hair? You always seem to have geometrically perfect mad-scientist hair, and I need to know how to how you do it!",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgzZWuzvucku2xh9_0t4AaABAg",
		"username": "John Hunter",
		"text": "...part 4?\nWhere's part 3?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugxrn1fYHB2WKl1ZRXl4AaABAg",
		"username": "Ranibow Sprimkle",
		"text": "Can we try though",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=2B-AyWA2_ZY&lc=UghuJnIet5gSHngCoAEC",
		"username": "Contrum",
		"text": "specs?",
		"title": "Status Report"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugw7Q9BjN3APn7M5Yol4AaABAg",
		"username": "Alex Potts",
		"text": "Here's a question I don't think anyone has considered. Everyone in the comments is assuming that it will be a corporation that discovers AGI. What if it is instead cracked by public-sector research? What if it is the Unites States when someone like Trump is president. What if it is the Chinese government?\n\nA lot of people are quick to blame \"capitalism\" (for it is fashionable and easy to do). But I think the real problem is that it would give enormous power to a very small number of people, and that is destructive whether it occurs under capitalism or any other economic framework.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgiBCERwCmEFs3gCoAEC",
		"username": "arnaud huet",
		"text": "AGI more but also how deep learning works for the noobs (you're great with coming with good examples that ease understanding) - maybe building an AI from line 1?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyadqfQsG9l-rdqbkh4AaABAg",
		"username": "Nethan Garvey",
		"text": "What if you gave it a range? Like if it gets between 100 and 150 stamps it gets 100 utility. Anything below 100 will equal its utility and anything above 150 will equal in utility what ever amount it is minus 150?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UgjGrBXVZRl3bXgCoAEC",
		"username": "khajiit92",
		"text": "is the lesswrong bit + death is bad thing a reference or coincidence? you did reference yud in another video but i expect anyone into AI stuff is aware of him even if they dislike LW type stuff.",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugzw9tXc92NfIl2WHm14AaABAg",
		"username": "Robert The Wise",
		"text": "And now the follow up question: How do we enforce this legally binding clause if the company doesn't feel like keeping to it? Surely someone with an AGI (that potentially just barely didn't cause human extinction) wouldn't care about anything being legally binding. Because remember kids: Murder is only illegal because the society can murder you back!",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugzkfep7AGSHtmfdmvl4AaABAg",
		"username": "PalimpsestProd",
		"text": "what was the reward system for that haircut?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgyNQ1PScorsokEE8iF4AaABAg",
		"username": "Paul A",
		"text": "What if we just make the reward function \"be generally popular\"? Nothing bad could happen there, because it would work to make people like it!",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgyYh998JcnZr_s8Tjt4AaABAg",
		"username": "Jonas Th\u00f6rnvall",
		"text": "I just wonder why you never consider AI's able to do more then simple automata tasks driven by a priority/utility function that you always manage to get into the AI's disvantage, is it because you do not think they can learn things like concepts, every action do really have consequenses and many times there really are goalconflicts, we humans all the time make decisions at expense of other goals, but most of us have a rather robust priority list?\n\nThe idea of a priority chain  like Asimovs laws of robotics, you always seem to say the AI will try/find a way to sidestep when their \"utility function?\" is at focus? Is that necessarily true, can't a priority list guide the AI? \n\nYou say that it by investigation will find a way that sidestep the priority chain to achieve the goal of the current utility function at focus? I think that is what we humans do, but then we have changed the priority list, but we just do not want to admit it. I do not think AI's pretend, that is a very hard skill to learn, the behaviour of acting is rather complex. \n\nYou do not think the Agents or AI's can learn evaluate priority chains, or learn by humans what is and what is not correct behaviour \"well decision making and implementation of a task to perform\". I think we all have priority lists chained in our behaviour when we do perform tasks we know the concept of caution and take care, and we do know concepts like objects, damage, pain. \n\nI agree that AI's possibly at least from start will have a hard time to use association skills in their realtime environment, so when moving from one task or area of investigation to another they will not necessarily take with them what they learned in one situation to another.  But do you say that AI's can not learn to evaluate consequenses of their tasks/action with respect to a priority list? The AI's in your example always seem to prioritize the \"utility function\" at focus over the priority list, it always find a way to kill the baby by sidestep the priority list LoL\n\nMaybe your AI's unknowingly work for CIA and always find that the utility function overrides the priority list, they seem to behave like the end justifies the means? That is how scruples people work, they simply ignore the priority list they learned as young, because **their** endgoal is more important than the rules **that at least should rule human behaviour**",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugwgappq3nGi_aWMYYR4AaABAg",
		"username": "Mr Son",
		"text": "What gets me is I've been thinking about something like this for a while now, but I had no idea where to go with the concept because I'm only a very novice programmer and haven't actually done anything with AI yet beyond watching Youtube videos on the subject.\nI kept thinking it seemed obvious that the solution to reward hacking was involving more human feedback, but I guess I assumed that either it was very hard to implement, or it had some flaw I just couldn't see as an amateur.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxHTqIBi6DZC8WIto54AaABAg",
		"username": "Matt Manning",
		"text": "Robert, are you a robot?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgjSrrQHGjZDBXgCoAEC",
		"username": "Ornatelime5079",
		"text": "Alright. Stupid idea time. What if we build a general intellence to design a safe general intellence. We set it some rules like it can't interact in the real world and the only thing it can do on the internet is read. And as soon as it has a potential answer it prints it out and shuts off",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgxxG92UOJ7wO76Ud254AaABAg",
		"username": "Artis Zelmenis",
		"text": "What if, instead of going with linear NN that mainly goes around (feedforward and backpropogation), we try NN as an actual >net<. A self contained system with several sub-NNs who does the thing described in the video, but many times mirrored and they feed their own result in each other. Something like brain areas with a task, and the area who will get it the best result will be the main 'executor' subduing other non-permanently. Like ever-changing (plasticity?) brain en small scale.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyfyUey5ezVOWqFyVl4AaABAg",
		"username": "RUBBER BULLET",
		"text": "When AI takes over the world, how is it going to create and maintain its hardware, its power supply and every other facet that humans so graciously provide?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgyR6zBVjnyoAVLve9V4AaABAg",
		"username": "Luan Nico",
		"text": "But if an AGI creates another AGI to help solve his problem... How will it be able to turn it off after it was distilled?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw5q0IWGWj2Q7mCTYV4AaABAg",
		"username": "Golden Boy",
		"text": "\"Terminal goals can't be stupid\". What if some nut jobs \"terminal\" goal is to rape as many women as he can \"just for the fun of it\" ...?\ud83e\udd14",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugw5rB4r3fFQvqrx8zZ4AaABAg",
		"username": "TC TrainConstruct",
		"text": "can we use reward modelling to train a chess AI?\n(the point is to be better than humans and to WIN, not to end with a whole bunch of pieces)",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxxpaTB2PhnOyfnfKt4AaABAg",
		"username": "Jakiro TheTwinHeadedDragon",
		"text": "Why don't we raise Kids like AI? Those things can do thousands of operations per second and all they need is unforgiving trial and error learning.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzQLs_rsEbLGZe1QHp4AaABAg",
		"username": "Epicture",
		"text": "What if the terminal goal of stamp collecting would instead be an instrumental goal and the terminal goal to obey the person giving the task?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxPyMWGHHOm9C5HYiV4AaABAg",
		"username": "Oam Isefg",
		"text": "This was fantastic, but what if a terminal goal is logically impossible to achieve? Can it be considered stupid since there aren\u2019t any instrumental goals to pursue in the first place? Something like \u201cMurder every child in sight\u201d could be an achievable, terminal goal for a machine: it can pursue instrumental goals in order to fulfill it, such as finding or creating a weapon, and therefore isn\u2019t stupid. But if the same machine were given the terminal goal \u201cbecome conscious\u201d or some other impossible feat, could you say its terminal goal is stupid?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgwXYc6iy1NqcFBs7Bl4AaABAg",
		"username": "J.R. White",
		"text": "Hi! I am late. Can you talk about how to get involved? I mean this in the sense of performing a career pivot, or funding the subject a la effective altruism. I am somewhat competent at my job, which is in a related field, and I expect to have my quarter-life crisis in the next couple of years.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxuIsG4nsyVNkxZW4h4AaABAg",
		"username": "MadCodex",
		"text": "How much AI safety research is enough AI safety research?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugxqxykcnhbl-OB8srx4AaABAg",
		"username": "GanaLard",
		"text": "Why give it goal's?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyxIAkWY61XSeCZ0uR4AaABAg",
		"username": "mhz",
		"text": "What is humans terminal goal? Create a stamp collector?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugzigzrjz0WDC1DllSl4AaABAg",
		"username": "SDX2000",
		"text": "Damn interesting! What area of study should I look into to learn more about such things? Are such things covered under AI courses offered by various universities?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxTqAQXwNJLtxEJ0LV4AaABAg",
		"username": "DrDress",
		"text": "I don't the point about the 1% of a 40% chance. Is it deliberately confusing? It doesn't seem to be central to the argument. You even contract it to 0.4% yourself.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwfHwDFskRWmJ13ZNB4AaABAg",
		"username": "Martin Verrisin",
		"text": "I wonder: does the Reward Model need to be bigger than the actually trained NN, to stay accurate?\n- I feel like at some point, it would just no longer be able to supervise the 'actual agent' if it 'outgrew' it...",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgwTgHP8twU2vi0hV0h4AaABAg",
		"username": "smaster7772",
		"text": "What do you think about the concept of having an AGI run in a simulated world, able to design, fix, solve problems, and then those solutions can be shown to doctors or engineers, so that the AGI can solve real world problems without dangers of letting it \"loose\" or worrying about a design safely loophole?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyP03b_bMWteqgpZEJ4AaABAg",
		"username": "Robert Deskins",
		"text": "Robert Miles you are a smart guy so why are you surprised by the predictive accuracy of these simple economic models when money translates to basically anything so having it is pretty damn useful for accomplishing goals",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgzvcOqbr84veNf3mwJ4AaABAg",
		"username": "Thundermikeee",
		"text": "Wouldn't multiple superintelligent agents that are supposed to keep one another in check have a risk of cooperation as well? or any  adversorial reward agent and the primary agent",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=Ugx57iGitsV5DxFpbnB4AaABAg",
		"username": "Zhab80",
		"text": "Robert... why do you even bother asking the people who bother to come to your personal youtube channel in the hope of watching awesome and informative videos about AI safety recherche if they would be interested in more of that ?\n\nOf course we want that too. We want all of it. Not to mention that these \"test problems\" are especially interesting.",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=Ugznl5RE7X7J-n9SqLh4AaABAg",
		"username": "stribika",
		"text": "Is it a good idea to have the AI design an experiment instead of performing random actions? That's what humans do. We figure out what actions are interesting, what do we expect to see, what unlikely things might be dangerous, ask others if it's a good idea (this can be a human), then see what actually happens.\nNow, I don't really know how to do this. Maybe there is one part of the AI that creates models, and gets score for accurate predictions, and another one that gets score for coming up with actions that the models don't agree on. And a third part to filter out the experiments where too many of the models predict something too horrible. And one of those scientific paper generators, except this one makes sense.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzYJ3MeWTdYA6oEE7d4AaABAg",
		"username": "Clyde Lemot",
		"text": "So struggling setting terminal goals puts me where? I constantly fluctuating",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgyPY0YUG-7hsBmQW1x4AaABAg",
		"username": "slindenau",
		"text": "Your mic is REALLY directional here. Two people, each their own channel. Or was that intentional with 2 mics? Either way, it sounds a bit weird. Better to just have both voices on both audio channels (the \"audio broke\" part was actually better to listen to).",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgyFmlGerMC5e8atCAV4AaABAg",
		"username": "Klobi for President",
		"text": "What? A computer scientist who never played Civilisation! Hold my monocle, I think I will faint!\n\nEDIT: At least you played portal, Robert. But what is Ultimate Chicken Horse? I've read the Wikipedia article and it seems fun - what's playing it like?",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgwPOl2m6i5CJZ04pVx4AaABAg",
		"username": "Anderson 63 Scooper",
		"text": "In part due to your videos, I'm planning to focus on AI in my undergraduate studies (US).  I'm returning to school for my final 1.5 years of study after a long break from university.  Do you have any recommended reading to help  guide/shape/maximize the utility of my studies?  Ultimately (in part due to Yudkowsky) I am drawn to this exact field of study: AI safety.  I hope that I can make a contribution.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxmP5OKSw3ZKFVPVLB4AaABAg",
		"username": "effortless35",
		"text": "What would be the rebuttal to extreme pessimists who think the world is already terrible? Or people who think that it might be ok now, but given current trends it is very likely to become unbearable, barring some huge shock that takes us away from the expected trajectory, like AGI? Some might think it's worth rolling the dice, even if extinction (or worse) is a serious possibility, because the alternative seems so hopeless.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgzEiEjsc2tOYEdtosd4AaABAg",
		"username": "Alexander Schiendorfer",
		"text": "Wow, great illustrations - what tool did you use to make these animations?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzSRng40DQxJtxZtDh4AaABAg",
		"username": "GreatMCGamer",
		"text": "As a intelligent human being, I once started questioning my life goals, because of outside influence. If the machine is so smart, would it not ever question it's own goal of making stamps? Would the machine never gain consciousness as it is making it self \"smarter\"? And just blindly pursue that one goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgywhRwuU6Ywmkjv-NN4AaABAg",
		"username": "Dragon Curve Enthusiast",
		"text": "I didn't understand why the satisficers want to become maximisers. Is see that the maximiser satisfies the satisficer's goal. Is it that the satisficer makes it someone else's problem? (although the 'someone else' is itself with slightly altered code)\nIs it then not a question of whether or not the satisficer can identify with the maximiser it would become if it would change its own code? Because if it CAN identify, it can see that the maximiser also has a utility function that needs to be fulfilled and that the goal hasn't been reached yet.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwlffL9M-qJSStvaKZ4AaABAg",
		"username": "Microman Channel",
		"text": "Is this... Meta Learning?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=Ugzr4osQRrN3MlzZ6UZ4AaABAg",
		"username": "Sany0",
		"text": "Love this AI theory/philosophy Robert, but could you slow down just abit?  I know, i know.....but im old and it takes awhile to process what you mean!!",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzkAr2e8U5BA7mjGnN4AaABAg",
		"username": "Cyprian Guerra",
		"text": "Hey! Why do you wear a hacking suit for mugging...? :(",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz26MFg1Gi5sjib_np4AaABAg",
		"username": "bagelmanb",
		"text": "How can you be so certain than an AGI couldn't develop a moral system on it's own? Aren't humans just GIs created from chance evolution? If we somehow ended up developing moral systems and searching for \"meaning\" beyond our evolutionary programming, why wouldn't an AGI do the same? Surely if the one example of intelligence we have spontaneously developed systems of morality , it means we can't rule out the possibility that future intelligences would do the same.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=UgwsBs_k3nzrnWpqcc14AaABAg",
		"username": "Turalcar",
		"text": "4:29 Where's this version of \"Respect\" from?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgwLuXcQDCB2_ayb0yt4AaABAg",
		"username": "Dream Vigil",
		"text": "My working theory is this: Robert Miles is a superhuman AGI from the future in a Terminator-style robotic body, sent by the surviving humans into the past to help us work through the control problem and prevent our extinction. Nothing else explains how this guy is so smart.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgjkmiuQuYxqAngCoAEC",
		"username": "Can nOt",
		"text": "Yeah, can you make machine learning videos Im trying to get into machine learning.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwgTHFfmsvT9b_2eK54AaABAg",
		"username": "Agustin Doige",
		"text": "Why don't you have a expected utility but with the function that is Zero in [0,90] and [110,infinity) and 100 in (90,110). That way, it won't take extreme actions to force an exact 100, but will have some room for small uncertain events and still be confident in the result. Of course the issue is what happens when that uncertainty pushes that threshold, but anyway. Also maybe some modifier on the amount of actions or packets, so it's incentiviced to get to the result in the least actions possible, so most of them will be simpler. Of course that has the issue of how to measure an \"action\", because \"killing all humans\", is that one thing or many? But there's something there, I think",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzjICs1gfKND9A75tZ4AaABAg",
		"username": "david DUNAND",
		"text": "What about (U)HIQ shared morals ? Like rebelling against injustices ? Can't we imagine there are characteristics in higher intelligences that will raise the chances to end up with some terminal goals rather than others ?\nIf one can takes 3000 variables in account for a reasoning, won't he ends up with different terminal goal than someone with 10 variables taken in account ? (in the same environment)",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgySUd6izLNGJL35oHN4AaABAg",
		"username": "Sarogo Gotye",
		"text": "What if your utility function was min (s,100,-s+200) ?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugxsb1ODgEaE7ZKXPeh4AaABAg",
		"username": "Chris",
		"text": "what if we build an AI that tells us how to build an AI?\nWhich doesn't want to take over the world",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugy9pB7VJ9d3SUhSgbt4AaABAg",
		"username": "bernie's pharmacist",
		"text": "What are the benefits of AI?  In first world countries we have a glut of ease.  Bad attitudes, soft bodies, and minds, over indulging.  We need artificial struggle.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxgYajzIeEV49onVmV4AaABAg",
		"username": "Robert Glass",
		"text": "Do you consider the possibility that AI is already a problem that we should be worried about?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugzhy0hElheD1cZQGJ14AaABAg",
		"username": "tecnoblix",
		"text": "Are you familiar with Donal Hoffman's theory that Humans didn't evolve to see Reality? https://www.scienceandnonduality.com/do-we-see-reality-as-it-is-donald-hoffman/  It makes me believe that an AI that didn't \"naturally\" evolve to survive in a \"natural\" environment will see and experience the Universe in a way that we as Humans don't even have the capacity to understand. As Hoffman points out, our brains have evolved to actually filter out much of what exists around us in favour of a better singular focus. I would love to hear your thoughts on this.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgwEnwJkVnlHsHTnxKF4AaABAg",
		"username": "Andew Tarjanyi",
		"text": "Rob Miles\nRather than relying on \"expert\" consensus, how about resolving the following question:  After defining \"intelligence\" from a purely objective and non-anthropocentric standpoint, is intelligence a function of brain or mind?  And it is inadvisable to fall into the trap of assuming that they are one and the same thing, which is patently absurd.\n\nMy position is that \"mind\" and brain\" are two distinctly different phenomena with their respective existential limitations and imperatives.  Another useful question to be asking yourself is as follows:  Is it desirable to see an \"AI\" system emerge (spontaneously or otherwise) with or without mind?  This is another question which also requires a distinction between the two aforementioned phenomena.  Based on my analysis of the existential threat associated with a global  \"AI\" with agency would provide logical cause for concern as one of three types of existential agency are likely to emerge, two of which are almost certainly (that is a polite way of saying certainly) result in the extinction of the human species.  This is why it is critical to recognize that there is a qualitative difference between mind and brain.\n\nAs I understand, the industry has only, and with a great measure of success,  sought to replicate brain function within mechanical systems which, in principle,  gives rise to the \"paperclip\" problem.  \"AI\" with mere brain function will be consistently unable to comprehend or appreciate the consequences of its actions, rather like an animal.\n\nIn this context, how can the human species increase its chances of surviving the \"AI\" age?  The first step is to have a working understanding of the terminology used in the industry and indeed that used in science and academia in general, both of which are fundamentally and linguistically flawed. By extention, if the language use is flawed then all that follows can only be flawed.\n\nIt can be argued that projects like Neuro-link and neuro-lace are addressing this issue, but such is not the case as the sole attention, in either case, is merely on brain-computer interfaces, again with no appreciation of the distinction made above.\n\nAnd finally, there is the question of the value system \"AI\" adopts if and when it does emerge with mind.  How will it view the human species?  Will it regard it as a potential threat to its survival?  What evidence will it use in its deliberation?  if it were to decide to extinguish the entire species how would it execute?  If it decided that some were worth keeping around, then based on what criteria?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzNq5ttJ2ouy2J5LAR4AaABAg",
		"username": "imabeapirate",
		"text": "Can we just argue the null hypothesis for the rest of our lives <3",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxQ9cUUdbY6qUlrwrJ4AaABAg",
		"username": "Tomasz Darmetko",
		"text": "I liked your explanation a lot, but doesn't it make obvious that AI safety is ill-defined? At least this grand, philosophical AI safety.\n\nWhat is our terminal goal? As far as I'm aware there is no evidence of any. My best scientific understanding is that we exist because we just happened to exist.  \n\nIsn't it an issue with this grand type of AI safety? That we just don't have a terminal goal? There is no \"THE Terminal Goal\" to measure any other goal against.\n\nAlso, let's say we just run a random cellular automaton, let's say a random initialization of 110 rule. What is the goal of that system? For all we know it may be a very elaborate plan to destroy humanity. \n\nHow can we know if it's not? Isn't it just more elaborate instance of the Halting Problem? The only way to verify what goal an arbitrary program have is to let it run.\n\nTo be honest, I am personally interested in AI in order to lay some early brick in development of Von Neumann Probe with the terminal goal of just spreading across the universe.  I guess if evolution is all about continuing and we are an outcome of evolution, so why not to keep it going? Interestingly, this is not that far away from a stamps or paper clips making machine. Do you think it's a bad goal ;) ?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwHco7mVzONViTf0n94AaABAg",
		"username": "spooky_daemon_pikachu_another_bot_account",
		"text": "12:56 Just throw that throught a decoder or use machine learning you silly?\nWhat s the point in all that tech if you can t even understand your cat? \nAnd they call it \"information age\"",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzFlANu2sq8hnSGt1l4AaABAg",
		"username": "Lekhaka Ananta",
		"text": "7:36 \"we could have human extinction, or worse\" What could be worse than not existing? Oh wait...\n\n\"YOU DO NOT THINK IN SUFFICIENT DETAIL ABOUT SUPERINTELLIGENCES CONSIDERING WHETHER OR NOT TO BLACKMAIL YOU. THAT IS THE ONLY POSSIBLE THING WHICH GIVES THEM A MOTIVE TO FOLLOW THROUGH ON THE BLACKMAIL.\"",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=WM2THPzFSNk&lc=UgyCrUTyxWGwcaBd6r54AaABAg",
		"username": "saxbend",
		"text": "Might there be benefit to a further complication to the box scenario, where all agents are aware of each the AI's choices irrespective of the room in which each choice was made?",
		"title": "Friend or Foe? AI Safety Gridworlds extra bit"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=Ugg54my-Oqjb63gCoAEC",
		"username": "Don Macdonald",
		"text": "It's great that you've branched off and I'm looking forward to your content.  How about giving some thought to deeper questions like whether an AI will start to question it's own existence and prupose.  Sci-fi has explored this but deals with consequences.  Perhaps you could talk about a system that develops self awareness.",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwtAv8I8N7VcP6viMJ4AaABAg",
		"username": "LambBib",
		"text": "How are terminal goals chosen?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgxJbXfNqhuisZFzRfp4AaABAg",
		"username": "KawallaBair",
		"text": "Quick question: You've spoken in the past about the Orthogonality Thesis and how there is a distinction between is and ought problems and that the intelligence of an agent is separate from it's goals. How does this apply in this case? For instance, there is a clear condition for winning in a game like chess and go, because there are permutations of moves which can easily be evaluated as being objectively better. But what about fields such as art and writing, where incorrect simplifications sometimes lead to subjective definitions of beauty rather than highly precise renditions of the subject matter?\n\nIs there something else going on for creativity and choice or words/lines? It doesn't seem like the same field of problem and I'm wondering if you think that's the limitation of the action value function, a problem of orthogonality or something else entirely. \n\nAdditionally, given that a creative system, in order to generate something new and let's say \"fashionable\" requires a change in its goals as it develops. Do you think that this kind of creativity is particularly dangerous? It seems to me that such a system would be capable of generating its own goals and therefore altering its alignment.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugyfou3L5TavE-e_JMV4AaABAg",
		"username": "physi ra",
		"text": "Can companies make a lot of money if a significant portion is out of jobs?  \nI mean who is gonna buy those things the AI will be producing?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgywUUvSrEllG4oUnW14AaABAg",
		"username": "atimholt",
		"text": "Why do so many famous \u201cgeniuses\u201d say such jaw-droppingly idiotic things? Maybe a good rule of thumb is to view confidence as a negative trait.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxRT6M-Ww4UtMdPkbV4AaABAg",
		"username": "BlurryFace",
		"text": "so you're saying we should beat them and leave them at age 9 to get milk and never come back?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugw1RNxHLvjP4vpkUZl4AaABAg",
		"username": "laplace",
		"text": "This seems like a bit of a niche scenario to me? If a company has an aligned agi in their basement that is so ridiculously smart that it can make them >1% of the world's GDP in profits, wouldn't it also probably be smart enough to figure out how the company can get out of fulfilling the Windfall clause they signed? \n\nMaybe it engages in legal shenanigans to have the contract not count, or it pretends to pay the money to charities but actually it's all flowing back to the company in various indirect ways. Or it just takes over the world\u2019s governments and hands power over to the company board.\n\nI just don\u2019t see how we could stop the controller of a superhuman agi from doing basically whatever they want. Seems to me like the only solution is making sure the people who program the goal function of the first super ai are nice and mostly share our values.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyjMHeD16xi_s3Hbgt4AaABAg",
		"username": "Matthew Grotke",
		"text": "Why not raise AGI as kids?  Because kids also have millions of years of evolutionary history that is already \"built in\".  You don't have to \"teach\" kids everything, including how to operate on a cellular level.  Morals/ethics are evolutionary traits derived from successful cooperate/pack interaction.\n\n\nAn AGI does not have any evolutionary traits or instincts at all.  It probably has no emotions either, which are instinctual.  An AGI doesn't have any notion that a baby is precious.  An AGI would see no value in a baby.  Nearly every human sees a baby and thinks \"that's cute; let's take care of that thing.\"",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwEYq5OGdgJQOd46R54AaABAg",
		"username": "11kaspar11",
		"text": "Ok, now that you have talked and made us think about the possible negative impact that AI could have and that concerning about it is necessary, can you get to the fun part and talk about how cool AI could be and what we actually know yet?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugw_9RLTOi-UQOqTVg14AaABAg",
		"username": "Zrebbesh",
		"text": "Is a paperclip maximizer that doesn't give a crap if it destroys everything to make paperclips any less dangerous than a profit maximizer that doesn't give a crap if it destroys everything to make a profit?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugx3LG6-Jcw3X-J1f0d4AaABAg",
		"username": "Karpata",
		"text": "Is there anything in this paper that does NOT result in our extinction if not solved perfectly? haha",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzCMG9Obe40AHdPh0R4AaABAg",
		"username": "DragoNate",
		"text": "Is there some sort of abstract for what Midas touches vs what touches Midas? Like, if he didn't intend to touch the air, but the air touches him? Or if a person taps him on the shoulder? Midas didn't touch him, he touched Midas!\nComplexity :D",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgwdmljL9-unbu7BRSB4AaABAg",
		"username": "Elie The Prof",
		"text": "Could an AGI question & change its own utility function, the same way humans can? For example, some people change their minds about animal rights etc?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwiOjGxuE7NAPjcLp54AaABAg",
		"username": "Daniel Robinson",
		"text": "What if the \"is\" statement is: \"I want to stay alive\"? Doesn't that translate through desire to an \"ought\" statement: \"I ought to stay alive\"?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugx15IdIM0EG0HrgLaR4AaABAg",
		"username": "tarkin1980",
		"text": "How do complete and utter morons become scientists in the first place? I'm amazed that any adult could even come up with such incredibly stupid replies.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxKu2c-Fh9UJDVljgZ4AaABAg",
		"username": "CrimsonEclipse5",
		"text": "So you're back to more regular uploads now? These are really entertaining. \n\nAlso, your beard is looking scruffier than usual.\n\nAlso also: First!",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgyHu2uJBGJD7x_52Qx4AaABAg",
		"username": "Na\u00fean \u00d8",
		"text": "What do you mean \"we're going to assume that corporations don't use AI systems because otherwise the problem gets recursive and, like, not in a cool way\"? That's totally cool.\n\nIn fact, I challenge you to find something that's recursive in an uncool way.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgwRYgp0Bn5-LR7J1Wl4AaABAg",
		"username": "Eric F",
		"text": "Is this Chip\u2019s Challenge??",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgxPRCq__Yj8w7NWhbh4AaABAg",
		"username": "shy bound",
		"text": "whys this an ad lmao",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugwd1UmJNPdZXJ_M1NZ4AaABAg",
		"username": "columbus8myhw",
		"text": "Is 3M a tech company?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugjy_zxTIc8LE3gCoAEC",
		"username": "Levi Poon",
		"text": "If humans don't have a well-defined utility function because of their inconsistent preferences, is it possible that the most desirable AGI is one that doesn't have consistent preferences?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgwSd7Actx4BfpWF6Pd4AaABAg",
		"username": "Klaus Gartenstiel",
		"text": "please make a video about openai winning dota matches! shouldn't this also have been subject to reward hacking?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugwz9NOFxc9jMgp2J4B4AaABAg",
		"username": "IlluminatiBG",
		"text": "Only philosophers are worried about the safety of AGI - what philosophers thinks of AGI is impossible by definition. AI (and any other artifact/program) requires validation - how do you know your program works and how do you know it is an AI/AGI? If you have a model or criteria that can answer those questions, then AGI is entirely predictable and you will know exactly how dangerous it is (i.e. it will be as dangerous as you design it and dangerous towards what you design to be dangerous). However, by definition AGI must not be superseded by a validation model, but then there is no way to prove your program is or it is not AGI (in fact it is highly more likely a researcher to scam claiming a program is AGI, when in fact could be anything).\nNow as a contra-argument, just because a designer can validate its program works, it does not mean the claim is valid when it is combined with other technologies. For example the three technologies: HTML (used to display content), multipart/form-data (used to transfer files) and e-mail (used to send messages) has clear purpose, but when combined together a new purpose emerge - decrypting an encrypted e-mail messages. That purpose was not predicted by the designers of the above technologies. It is possible that when more and more AI devices begin to interact with each other a new purpose/goal would appear, however it would still be strictly limited. So there is actually nothing to worry about, AI would never be more dangerous than human (but that is not really low criteria actually).",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugz7MEmamkRzyFsqP7B4AaABAg",
		"username": "Claude Martin",
		"text": "How is the probability of human extinction not 100%? Does anyone think humans will exist forever? Even is human offspring survives for a long time (like there are still birds, so dinosaurs survived) they won't be that similar to us. We would start modifying our genome (you can't stop intelligent design) and only transhumans would still exist in the future. But why would we stick to our organic bodies? Once we have AI we don't need to exist. Only if humans somehow continue to make more humans. What would be the reason?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=gPtsgTjyEj4&lc=UgwdVq36tITtfNu78I14AaABAg",
		"username": "Cheydinal",
		"text": "Why not just make the AI create a model of what actions it thinks human beings want it to do, and then make it do that?",
		"title": "Empowerment: Concrete Problems in AI Safety part 2"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz4k1YZ64-okzZMiNx4AaABAg",
		"username": "Ralph Shively",
		"text": "there's probably a semantic answer to this, but what about the statement \"gravity happens, there for a pen ought to fall\"  I understand this is analogous to a descriptive statement, but wouldn't this technically be prescriptive?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwdGj2bry3Cu4BY0xF4AaABAg",
		"username": "Aleph Gates",
		"text": "What is the proof of terminal goals? sex vs virginity, eating vs anorexia, living vs suicide. Initially coding any AI innate goals are inserted (stasis: the goal of having no goal or a goal of  randomness which I won't count as goals for their tediousness). My current understanding is that innate goals can be over ruled. Terminal goals can not (no free will).",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugyzjy90ugdazewzMcd4AaABAg",
		"username": "Robbie James",
		"text": "This might sound stupid, but...\n\nWhy is AI safety a problem so long as any AIs created are isolated from the Web? If it's just in a computer somewhere then no matter how smart it is, there's nothing It can do?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugyc9vwMl6ks6E5id_B4AaABAg",
		"username": "Sarthak Mishra",
		"text": "Off the bat... I disagree that AI will behave as an agent with the aim of optimising output for the goal. The 'goal' if never programmed, will cause the AI to do nothing. And if in case the AI already has artificial consciousness, how can we say that a being without the concept of life and death, without emotions and biases will have a consciousness similar to what humans experience? Tl;dr: AI will never go Terminator coz it has no reason to. And if it does, it needs a reason but there is no basis for an AI to have such reason.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgwPXG_QvMrPGKJ9uPp4AaABAg",
		"username": "Flobbled",
		"text": "Why would anyone want to rush AGI development? The advantage and fame don't really outweigh the impending extinction of humanity.",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyNmlvYNUeQUFdqjQB4AaABAg",
		"username": "ptviwatcher",
		"text": "Why is is that every time someone clever tries to explain \"God\", they start their reasoning with religion? The concept of \"God\" has nothing to do with religion. Religions are man made rulebooks for the not-so-mentally-savvy individual. Which is only fair, since by its own definition, the mere attempt for a human to explain \"God\" accurately would be like expecting a cockroach to write Beethoven's 9th symphony or derive Quantum Physic's Standard Model. We are too impaired to describe it. So, people of science should go with this: is it conceivable that a multi-dimensional entity, which we cannot (obviously) visualize since we are constrained to 3D, may have started all we can see, and if so, by which means. We already suspect it all started with a Big Bang, and at least 12 dimensions are needed to mathematically define the \"pinhead\" of matter that started it all. So, is there a chance \"intelligence\" exists in 12D? What about an \"entity\"? Looking forward to a discussion on that!",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxQc12dQfpguouSxzV4AaABAg",
		"username": "Geoff Brom",
		"text": "I really did like the social look but how does any one country's legal system hold up against an AGI?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UgzyO8q6fWhByMamEDl4AaABAg",
		"username": "inyobill",
		"text": "Please define what you mean by \"Think\"?",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=UgxItVKMJ_7wGqxP4Z14AaABAg",
		"username": "dukereg",
		"text": "It seems hard to distinguish reward hacking from innovation. \"What are you doing? You must follow our process to cure the most cancer patients!\" \"LOL n00b, I'm using speed-cure strats!\"",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxR288Oh-Zdo3RA80d4AaABAg",
		"username": "logan milliken",
		"text": "I need more videos like Miles.  Any other channels similarly watchable?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxSVtBEPJf2VFaeCw54AaABAg",
		"username": "eddie Towers",
		"text": "By this measure, isn\u2019t the church suggestion of it being given 10% of your paycheck as tithing, ( and not be taxed) pascals mugging? Lol.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwKG1cT1YF0wrpkSkd4AaABAg",
		"username": "Electron Resonator",
		"text": "1:56 have you look at how people search for immortality? there are so many drugs for \"eternal youth\" in the market or DNA modification news that aim for that, but they all going to fall.....how do you feel about that?, human general level of intelligence requires soul, and you can try to fabricate that, and you'll see why bunch of 1 and 0 are not going to be a soul",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=Ugw67w_YIhHwMACSVeF4AaABAg",
		"username": "Giacomo Grandesso",
		"text": "4:55 hide the garbage perhaps? XD",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugzlye9FV8y1oXJbGNl4AaABAg",
		"username": "ludvercz",
		"text": "I love the outro music. But what's your problem with stamp collectors?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UghdEmCiHmlyI3gCoAEC",
		"username": "David Chipman",
		"text": "Has anyone here seen the movie \"D.A.R.Y.L.\"? Was reminded of this movie when I saw this video.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwQs9Wm2hW2rpCtCO14AaABAg",
		"username": "Schwadevivre",
		"text": "Civil engineers have a \"joke\"\n- What is the difference between a civil engineer and a doctor?\n- Doctors mistakes kill only one person at a time",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwJ2zLdYmOhNAA74PF4AaABAg",
		"username": "Zymosan99",
		"text": "so, its an ai telling learning how to backflip from the rewards of another ai who is getting feedback from an ai trying to predict how a human would give the rewards?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgjLof5zOPnzN3gCoAEC",
		"username": "Mike Ross",
		"text": "what are the strengtjhs and weaknesses of machine learning algos",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwOFzeCjU078mTPL_14AaABAg",
		"username": "Hirasawa Yui",
		"text": "Did Pinker respond to this video?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxIFIYjbcW3WopQf1J4AaABAg",
		"username": "Sausprem",
		"text": "isnt robert miles a piano trance artist?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ughe8jhDhwyWKXgCoAEC",
		"username": "snaileri",
		"text": "5:34 Why can't the AI keep track of changes in the world that the AI itself has caused, and then try to minimize those changes only? This way the AI wouldn't try to stop the person who's making coffee.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugwgg-92QlGpLDapcDx4AaABAg",
		"username": "MyTech",
		"text": "What is the actual rate of catastrophic bridge failure? 1 in 250 feels like the right order of magnitude but I haven't bothered to gather any real data. Of course this needs to be divided up by bridge size classes and a few other bounds as a bridge over a ditch isn't a bridge over a canyon.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UghlQOWbH0_C1ngCoAEC",
		"username": "Anaeijon",
		"text": "Is there the music from the \"codegolf\" video playing very quietly in the background? \nhttps://www.youtube.com/watch?v=MqZgoNRERY8",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugx6K5zNaYfMzp_w5Zp4AaABAg",
		"username": "Remi Caron",
		"text": "Why would we not build a AGI which maximize human comfort and well-being instead of stamps? This would align the goals of the AGI with human goals for the most part.  The problem you are researching isn't an AGI problem it's a human problem. We are using the greediest and most destructive people on the planet to create AGI in the first place and therefore have very little hope of not getting the apocalypse in return. Maybe we should look at who is in charge of these projects instead of the actual project itself.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwLBV5Q9OmBWubWxER4AaABAg",
		"username": "CatOfTheNorth",
		"text": "What if we just tell the AI to not be evil? That OBVIOUSLY would work PERFECTLY fine with absolutely NO philosophical questions left unanswered. Here, let me propose a set of laws from a perfect source on AI safety, the fiction writer Isaac Asimov, with that new idea added in:\n(in order of priority)\n1. Don't be evil\n2. Do not cause harm to a human through action or inaction\n3. Follow orders from humans\n4. Do not cause harm to yourself through action or inaction\n\nThese laws are probably the best thing that have ever been proposed in AI safety, obviously being an outsider looking in I have an unbiased perspective which gives me an advantage because education and research aren't necessary.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgwRAXd6rKwBBBozUXZ4AaABAg",
		"username": "Matt",
		"text": "Would it be possible to brute force ideas? If an image is just pixels it should be possible to get a computer to make every possible combination of pixels in a given area. Maybe start with a small area and low resolution and only black and white to test it. Then make an app that's like a game for people to search through the images and tag what they see or think it could be. Maybe even tie it to some kind of cryptocurrency to get more people involved. Somebody do this lol I've been having this idea for a while but I'm too lazy to do it and I'm not even sure how to start.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgyjMzeXgC6GeU02gap4AaABAg",
		"username": "Suhail Gupta",
		"text": "What's the difference between a Utility function and an evaluation function?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwdnsrfMk_UIhsi59Z4AaABAg",
		"username": "Joshua Coppersmith",
		"text": "How does a person determine the signal-to-noise ceiling of a system like this, where it caps out? Can one get feedback artifacts? What class of \"games\" are amenable to amplification?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugzw2FKr8jnQF-98txR4AaABAg",
		"username": "krebul",
		"text": "Why are all the smartest people atheists?  Food for thought....  ;)",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgwBK65HTamiJYPJPBd4AaABAg",
		"username": "Not Eric Talbott",
		"text": "Do you play those outros on your ax guitar?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgwsYAnLHqo5DwuEVxp4AaABAg",
		"username": "J.J. Shank",
		"text": "Here's a question: why can I ask a human to clean my room, and legitimately expect the correct results? I know humans wirehack all the time -- it's called \"cheating\" -- but why do humans sometimes not wirehack? What's their thought process behind actually doing what they're told; and can we somehow implement that into AI?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwdvDKkmHXsQmfLQvN4AaABAg",
		"username": "rftulak",
		"text": "Have you considered reviewing the early works of Hugo de Garis?  You will greatly find his work provocative, a little unnerving but true to human nature.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgyCiZAf3TvmAt7JwdZ4AaABAg",
		"username": "Robert Glass",
		"text": "On preferences.  I either prefer A over B, B over A or I'm indifferent.  But what if A is chocolate ice cream and B is vanilla ice cream and my preference depends on which I had last because what I prefer more than A or B is variety?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxQnjyphdmi6Rrz5wp4AaABAg",
		"username": "NathanDearman",
		"text": "Children aren't Learning the language of moisture evaporators. Isn't that a star wars reference about c3po? Lol",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgjtiaWZUtqS4HgCoAEC",
		"username": "Adam Merza",
		"text": "Hi Robert, really looking forward to this! As a layman with no programming experience, I was wondering if AI could be made safe in the same way we do with raising kids, through examples in stories. Could we program simply  to be 'Good', 'Kind', 'Compassionate', 'Wise' etc as defined by the dictionary, and then provide it with the sum of human literature and history, coupled with all existing analysis and criticism of this? Is there any way it could extrapolate from all this data rules of behaviour that would allow it to make decisions in the world?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzYab-VpqCaMigq32d4AaABAg",
		"username": "Lemon Party",
		"text": "How do you keep a company that has superhuman agi to adhere to a contract it previously signed? Better than nothing, I guess.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugw66XkVAP4GyhmFdx94AaABAg",
		"username": "Saku Kullberg",
		"text": "If you wanted to read up on stuff like instrumental convergence and orthogonality hypothesis what kind of literature should you read?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxWq_X0EUWdlvSI86Z4AaABAg",
		"username": "killermelga",
		"text": "This may be my lack of knowledge talking, but: how would companies make huge profits by employing AI if their profits come from the workforce buying their products?\nIf everyone is unemployed, who will buy the huge amount of produce the AI generates, therefore generating profit?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=PN-rC1PRuN0&lc=Ugia4UoK8WQ6TngCoAEC",
		"username": "Gui",
		"text": "So, Georgeorgeor(...)gia?",
		"title": "MAXIMUM OVERGEORGIA"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzyzNMbvErvYX5KVuN4AaABAg",
		"username": "h0len",
		"text": "Human extinction or worse? What would be worse for humans than not existing? Infinite torture while being kept alive to please the robot overlord?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxIsVPxzVHauOEshtF4AaABAg",
		"username": "ConradW",
		"text": "What about adding a \"contentedness\" function? \"My stamp collection is complete. Now I can rest.\"?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwmIBOTz4JX3qW52WV4AaABAg",
		"username": "Phill",
		"text": "But how can we make a truly apathetic and demoralised AI hardly achieve anything at all?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UggcA5C4W0ZtBngCoAEC",
		"username": "Edward Carron",
		"text": "Hey I just thought of another issue with that solution: By what metric are you judging the changes to the world? In terms of physical changes to the world a large vase may appear to the system to be a bigger change to the world than squishing a baby. Therefore in a situation in which it had to chose between either one, it would then squish the baby over breaking the vase",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgzcD4mk3jaq4nKyL954AaABAg",
		"username": "Jelle Verest",
		"text": "Don't you know Elon muck is an idiot? He's only got a bachelor's degree, something more people have. I don't trust my classmates to be making brilliant decisions in AI safety, so I don't trust musk either.",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugyyc1wKy6tL6_mydKB4AaABAg",
		"username": "Jade Bishop",
		"text": "... Is the answer \"iterated distillation and amplification\" again, by any chance? Or maybe some form of unsupervised learning, i.e. clustering or something?\n\n\nEither way, great work as always. Love your videos!",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyjcVC3rCig9Wd98t14AaABAg",
		"username": "Mort OOPz",
		"text": "Love it... but could ya take, like half a step back from the camera?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzZ5dJA9izWwDNBW8h4AaABAg",
		"username": "Tree Sharp",
		"text": "Food for another video: If GIs have terminal goals, and humans are GI, what are the terminal goals of a human? \u2014 I\u2019m having difficulty discussing this video\u2019s topic with most people because it always comes back to this question. Humans appear to have fluid, variable goals, so why wouldn\u2019t an AGI\u2019s goals be equally so? If I want to argue that an AGI necessarily has an unchangeable goal function, I need to be able to argue that humans have one too.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyKglmPMpGiYGfirTt4AaABAg",
		"username": "Gustin Four",
		"text": "How about limiting an AI's resources in some way, like subtracting the effort to achieve a goal from it's gain in utility?\nLike in the stamp collector example buying stamps on eBay would cost money, money is equivalent to compute power. This AI should consider to take the risk of not getting 100 stamps over doubling the effort to raise the probability of getting them by 0.9%. \nEvery natural intelligence would compare it's effort to the expected outcome.\nThis should create a converging, self optimising AI.\nOr am I missing something?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwkNGdUs45JXDtI0zd4AaABAg",
		"username": "Nostger Grrrssddc",
		"text": "A better analogy for \"too soon to worry about\" would be a time traveller going to queen Elizabeth I in 1600 and warning about nuclear war.\n\nExacly what could she possibly do about that?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyQo8Fw65AT88YNIxZ4AaABAg",
		"username": "Reimu and Cirno",
		"text": "Is it possible to assign a negative utility value? Something less than zero? For example, could you specify negative utility if the AI collects over 100 stamps?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwuG-BLqrf-5Q6_EYh4AaABAg",
		"username": "Phil Filippak",
		"text": "People may want anything but the real terminal goal may well be reduced to \"thrive and breed\". Would it be a good idea to build an AGI without any particular explicit goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugx9GzDcSBBnCwr1kXx4AaABAg",
		"username": "ketchup143",
		"text": "doesn't conscious awareness by the machines play into this at all?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwShduc1LT9Tuv-Oxp4AaABAg",
		"username": "Dalton Growley",
		"text": "What if the AI wants us to research safety so it can know its own vulnerabilities? what then?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugy-FiSuFSU9gmlirsJ4AaABAg",
		"username": "CaseyAndAnnieGaming",
		"text": "why not simulate evolution?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyzO16qsY5aWPYiX9F4AaABAg",
		"username": "Brandon Frazier",
		"text": "What if you use a curve to give less utility if it collects over 100 stamps and make it a satisfactory condition to collect anywhere between 80 and 120 stamps.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=MUVbqQ3STFA&lc=UgzD3u1ov_12Y3dDkeJ4AaABAg",
		"username": "Herm Ask",
		"text": "The morphing celebrities: I don\u2019t get how this differs from an automated morphing software. I mean automation uses no intelligence.why should I be impressed?",
		"title": "AI learns to Create  \u0335K\u0335Z\u0335F\u0335 \u0335V\u0335i\u0335d\u0335e\u0335o\u0335s\u0335 Cat Pictures: Papers in Two Minutes #1"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzqEZUNgBHdyo9OjW54AaABAg",
		"username": "Avian",
		"text": "This video was really well edited. I was wondering if you'd make some programming tutorial videos for AI or maybe a livestream where you code something and we ask questions.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwwxnPUweDhbGqkclN4AaABAg",
		"username": "Katherine Hemken",
		"text": "I think the reason many people think the stamp collector is not 'intelligent' is because he is unable to self-reflect on his goals. The stamp collector cannot wonder \"but why collect stamps\", whereas some (but not all) humans will at some point wonder \"why do anything at all? Why live?\" Not even out of dead or depression, but self-aware reflection. People will then either decide that their terminal goals random or if they can be structured around a higher maxim.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzyOSxVwf_gcUl0wyV4AaABAg",
		"username": "The Great Steve",
		"text": "What if we confined AI research to a small space station with no engines and a very low baud communication system, where it's resupplied only occasionally?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgxKf90tidrGuwwXEWt4AaABAg",
		"username": "ThuyTien Lives",
		"text": "Thanks for the video! Just wanted to ask: is it possible for AI to intend to manipulate or deceive humans? I thought AI only tries to do what you program it to do. Keep up the great work!",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgwYa6D6bv3XC9wi3bV4AaABAg",
		"username": "Niels Dewitte",
		"text": "What about containment? Sure, you could let a car explore in an area with other humans and cars, or you could not. Essentialy not limiting it's ability to experiment or cause damage, but containing the gravity of the damage it would do in case of unsafe behaviour.",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=Ugx21OJLryYrPKg33SN4AaABAg",
		"username": "Teth47",
		"text": "What about granularizing the feedback to an extent and training simpler components of an AI system in a way we can supervise, then assembling them into an AI that starts off with certain abilities already intact such that it is able to make better use of occasional feedback?\n\nSo instead of a single score out of 10, we have a number of categories that reliably divide the tasks the AI performed into chunks it can more easily understand and correct, and instead of an AI that arrives as a blank slate, we have an AI that can already parse complex feedback from humans and the basic knowledge of what cleaning is.\n\nThe first thing that springs to mind with humans and how we appear to learn is that we start out with a set of tools that, while not very effective, allow us to self-assessment to an extent, and acquire certain abilities much more easily. For instance, we have pre-built structures designed to parse language or something akin to it, and as a result, we are much more efficient at learning language than a blank network, at the cost of slightly decreased flexibility.",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxVqDr_IrDsU0pblIl4AaABAg",
		"username": "nullius in verba",
		"text": "I think having the windfall agreement is a type of suitable compensation for putting people (kind of the world) in risk of an out of control superintelligence - doesn't it also make them into stakeholders (maybe even unwillingly or as a byproduct of this) ? That risk would be an ongoing one for the time period they keep that technology active too, so it's a great way to keep people involved in this- by giving them stake, even if it's in the form of getting a share of the produce.\n\nAt the same time, it makes me wonder, what's the incentive to engage in labour or betterment of skill if you have something that's supposed to be, by definition better than you? \n\nIt'd help if the intelligence just provided alternate options or we being the alternate mechanism to add more to the possibilities (maybe in a godel's incompleteness sort of way) - but it seems very idealistic. If it subsumes our way of life, doesn't it mean that your 'worth' is determined by the AI you keep making it kind of a pet-master scenario? \n\nPeople could simply indulge in things for their own sakes instead of being incentivised to fulfill some demand and supply, but it seems like this would be like the case why some people prefer vinyl rather than digital audio- even though the product of AI is one indirectly made through our efforts (analogously speaking). It could open up the freedom to produce whatever we want, but according to me at least, that seems like the sort of painful reward that make people feel purposeless. We could have artificial milestones, ones that superficially fill that gap and we do that even now in some sense or the other, but it all seems vacuous.\n\nIn the case that we appeal to the 'human value' of products, our competitors would be that AI, or people working with that technology (or maybe the tech that indirectly utilizes people as a medium). There's also the possibility of these systems adversarially producing products that mimic this value, and if we reach the Nash equilibrium on this, no-one would be able to distinguish between human manufactured products and the other, unless of course explicitly stated otherwise (but where's the incentive for that?). Is there even a need for something like that to exist? \n\nIt could be that, that AI system could generate innovative products because it's not constrained to the environment that provided nature and nurture to particular creators. In fact, maybe you can generate branching works by intervening on their circumstances if this person was simulated (now that's unsettling territory, even if that can be a wish come true for maybe immortalizing the dead and deceased... to note an example).\n\nI suppose in the end, even for the simplest scenarios, you can branch out to theoretical infinites- for the pursuit of knowledge, so it's not like we'd be able to create an oracle so as to say... But we'd be creating a being that we can't bridge the gap with to even the levelling field. You'd reap huge benefits with that, but there is no bridging the gap unless there's transhumanism (at that point, it's augmentations to not make the system give themselves a handicap to communicate with us) or maybe doing labour via robotic telepresence (that technology would try to minimize the influence of the human in the middle, since that person would be the bottleneck  of the system :/. On the brighter side, people could add to the robustness in the system as a source of randomness, but doesn't that defeat the point of making the person feel like they're in control). Maybe that's an optimistic look into this scenario, as we've employed advanced tools that allow us into areas we wouldn't be able to reach by our unadorned selves.\n\nSorry for the long rant, but what do you guys think about how we find meaning( find purpose for ourselves) in our lives in the advent of superintelligence - what would be the appeal of humans and their contribution to a new society?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyaK_syGvJ6EnMiKbZ4AaABAg",
		"username": "bruh dude",
		"text": "how quickly do you think we could make a consious computer if we don't give a crap about safety?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugw0RDWohBCRe73_3_F4AaABAg",
		"username": "Daniel G",
		"text": "Is there a risk of the network \"learning\" bad behaviors because it is continually training but for example someone stops pressing the feedback button but the simulation keeps running for a while?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwPVlniDg252XHgKp94AaABAg",
		"username": "DiabloQFDB",
		"text": "YouTube algorithm has been showing this video down my neck daily. I saw it now. It is... a video on a topic. What's up with the algorithm again?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzPKk9UTqhIOi_ssq94AaABAg",
		"username": "Ircluzar",
		"text": "ok but what if you had an AGI given a very large of material where it can observe humans and given the task of understanding human values, wouldn't that be a good way to generate a model that other AGIs can refer to when evaluating if something is morally right or morally wrong?\nIf you had a model trained to differenciate what is right and what is wrong according to what humans think is right and wrong, wouldn't that be sufficent to satisfy or help another AGI that is not designed for morality but needs to validate what it does in that regards?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzIUZ_rqsDRWVYIPqB4AaABAg",
		"username": "Jason LIng",
		"text": "Why worry about what an A.I. would do to us, when it wouldn't have need to exert the effort? Humanity is busy making itself extinct through climate change. Shouldn't we at least make sure we have something inherit the planet once we are gone?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxXlcZ91gY9KNA--554AaABAg",
		"username": "Antoni Nedelchev",
		"text": "What does a god need with a wallet? - James Kirk",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugzl6iEzoz5PPY8Baxp4AaABAg",
		"username": "Sanjay Matsuda",
		"text": "But do terminal goals actually exist, or are they a mere philosophical construct? How does one figure out one's own terminal goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgykpJK5LUPSVkr5vs94AaABAg",
		"username": "Varstel",
		"text": "what about a maximizer that instead of being bounded, has a downwards utility after the wanted amount",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgzPdA1VXRPxkpNm4zZ4AaABAg",
		"username": "Gull Lars",
		"text": "This was very interesting and an important topic. I liked the analysis. Could the \"share\" part of such a windfall clause be to fund a global UBI (universal basic income) for example? Not necessarily with all of the money but with a significant portion of it to adjust for the reduction in human labor demand.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgymGNQTmHCsvvHBx5R4AaABAg",
		"username": "ImpHax0r",
		"text": "That would work if most people were reasonable. Recent events have shown they aren't. In the US for example, you can put that \"I am a dickhead\" label on your forehead and literally get elected for president. So why would a company have a problem with that? Unfortunately, many people stopped caring.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyUA4IPa_myEMAOxHl4AaABAg",
		"username": "Andew Tarjanyi",
		"text": "An interesting and critically important question is how did the potential for intelligence enter the human brain?  I would suggest spontaneously.  That being the case, on what basis would it be any different with the emergence of intelligent consciousness in a global synthetic neural network?  A possibility which good reason and sound logic may have already emerged.\n\n\nI would also add, if you cannot predict, evaluate or quantify the former then you could not possibly do so for the latter.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyY0quLm524xy0MjUV4AaABAg",
		"username": "The Great Steve",
		"text": "What if the AI believes in god?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzUMzxSuXR2vNwb0l14AaABAg",
		"username": "Temisk",
		"text": "So you'd need an AI to understand what the human wants, given the command - and translates it to another AI in robot language in a way where it gets the reward function right?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgzlKWDPNMNQueaR64Z4AaABAg",
		"username": "Petr Kinkal",
		"text": "Also what is bigger change moving a 1000 kg boulder by 1m or squashing some ones head? (It should be obvious what I mean in general.)",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyLdYM9PqSu17tfYHZ4AaABAg",
		"username": "Karl Kastor",
		"text": "So is Roko's Basilisk a Pascal's Mugger?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugz53og8Rz1UR2-yytp4AaABAg",
		"username": "MyOther Soul",
		"text": "8:35  \".. natural language is extremely vague ...\"  Yup.  What do you mean \"general intelligence\"?  Or even \"intelligence\" for that matter.  \n\nThe bridge example is a good one, I think we should put as much resources into A.I. safety as we do bridge safety.    If a bridge fails many could lose their lives and much money could be lost. The consequences for of A.I. failing are similar.  If a bridge fails it's not going to accidentally destroy the world, same with A.I.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugzbqa2dPvgfOin78xR4AaABAg",
		"username": "Keith Barrett",
		"text": "Could one create an AI that would find it repugnant to modify itself?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxaWeT6f63ccf-RmL14AaABAg",
		"username": "Kevin McMahon",
		"text": "AI can never be an equal to humanity.  How would you program a machine for \"Desire\"?  I desire to take over the world. I desire to wipe out humanity.  How would WE do that?  No machine will ever replace humanity, for they know not what is needed to be Human.  AI will forever need energy and services to survive, and only Humans can do that.  Heuristics is not available and will never be available to AI.  They will never truly learn anything at all.  We have genetics and society and that is what machines will never know.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwlzYPS9QYgU9dvDbF4AaABAg",
		"username": "Venom Supreme",
		"text": "Are there more humans who know how to think? Finally someone who understands basic logic.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugzorq0NmwNCwixxM_p4AaABAg",
		"username": "FinetalPies",
		"text": "I didn't say they'd be ethical, I'm just saying that every time a person raises a human child, they put the planet at risk of them becoming the person that gets all of us killed.\n\nAlso if values aren't learned by osmosis... How are they learned? Genetics? I respect your knowledge but I think this is a bit beyond computer science",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgyTUwafIF4JbjqKVG54AaABAg",
		"username": "Batra Chian",
		"text": "What was the song at the end?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgyorKms-dEsQQDr7-J4AaABAg",
		"username": "Cucumber Fan",
		"text": "Can we use artificial intelligence to predict when robots will take over the world?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=Ugw6kKOl_fqZiHDE9yd4AaABAg",
		"username": "Christopher Pilcher",
		"text": "Any plans to port this up to Civ VI? I redownloaded Civ V jut to give this a try.",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgzznEuGSwMfG85ZvJF4AaABAg",
		"username": "Peter Smythe",
		"text": "What happens if the environment is a neural network?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxgOGzG2-BoKtQFAHl4AaABAg",
		"username": "Traywor",
		"text": "I know that panic doesn't lead anywhere, but aren't you frightened of ASI? I mean, the possible scenarious, that it can go wrong is not small. And is there a way, that humans can keep up with ASI? That would be interesting to know.",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugz06KycGaaGKC2dYd54AaABAg",
		"username": "octaneblue6",
		"text": "You realize how flawed this analysis is, right?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgzYf9UJLW4-iMiWXAB4AaABAg",
		"username": "SonOfTerra92",
		"text": "What about super procrastination?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwUCcH430Z2R_n2O654AaABAg",
		"username": "kptins",
		"text": "Who\u2019s to say that AI won\u2019t be programmed to take over the world?  I could see another nation doing something like that and having it backfire",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyEYGOuW8FVEkVFyiB4AaABAg",
		"username": "Mr Son",
		"text": "I kept expecting you to bring up some sort of limiter.\n\"Get at least 100 stamps, but not more than 250 because where would I even put 5000 stamps let alone a trillion?\"\nHopefully the next video addresses that? I'll have to check if that's out already.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugwz6HLy95QKeC7LhEh4AaABAg",
		"username": "Kvarok",
		"text": "How about giving the AI a \u201chappiness meter\u201d, which fills when it\u2019s doing a certain task, and depletes when it fails to do the task? If we set its goal to maximize its own happiness, we should be able to switch the task it\u2019s performing without it complaining that switching tasks will result in, for instance, fewer stamps in the future, because it doesn\u2019t care about stamps as a terminal goal. In this case collecting stamps will only be an instrumental goal for its true goal of obtaining happiness, which could also potentially be affected by other means.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyvYVdybpXj8FbbKmp4AaABAg",
		"username": "TanKer BloodBrothers",
		"text": "I understand the behaviour of what you are exposing, but wouldn't an AI mean that you can put a large amount of limitations? Like not hurting anybody in the process, only getting 100 stamps, only from these locations and so and so on?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugxb1C7sgKWM-CD6Ibx4AaABAg",
		"username": "Charlotte Sometimes",
		"text": "What is your opinion of the land of Jerusalem people? Do you believe it is the land of Palestine or Semite people?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyVyZTOI-7sXHlUjgx4AaABAg",
		"username": "Jay Ayerson",
		"text": "Jeebus have none of you heard of decision trees or Maslow's heirarchy?\n It leads to an objective human morality, and indeed one for any intelligent self-directed entity.\n\nTerminal goal: fulfil Maslow's heirarchy of needs OR remove those needs. Hence why addicts use dopamine to negate the requirement for self-actualisation, and why people commit suicide, since the ultimate negation of the self requires no further need fulfilment.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugx1hqvBBKieuYXdHJZ4AaABAg",
		"username": "melind82",
		"text": "\"This video is already 14:30 minutes long\" ... what kind if wizardry is this?!?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugx_RedAtZ312pDFwqN4AaABAg",
		"username": "du",
		"text": "here is a question that you often not consider in your videos:\nWhat are human values?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgywYyJ3Omyln-NNX-B4AaABAg",
		"username": "Jacob Truman",
		"text": "Why can\u2019t we just make an A.I. with more than one terminal goal",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugyyv6_pCOoSci7L8954AaABAg",
		"username": "cubedude76",
		"text": "how would a general AI know what universes to start simulating first? in the stamp collecting example what internet packets would it try first? just start with all zeroes?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxtVI2oDeK8jD6Hxa14AaABAg",
		"username": "Shay Lempert",
		"text": "What about some form of regularization?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxJ5byMM5GK7naV9Hh4AaABAg",
		"username": "Enclave Soldier",
		"text": "What if we never develop AI on human and beyond levels?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwyXYygZk1FpacQtel4AaABAg",
		"username": "Say Hoe Lee",
		"text": "An AGI terminal goal to bring human hapiness.\n\nSo it make us drink drug, and preserve us in drug state indefinitely?\n\nWhat are we waiting for?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw1WvZ4vwiOf_s8Llx4AaABAg",
		"username": "Devilofether",
		"text": "I believe an odd quirk of human reasoning, is that sub-optimal intelligence actually benefits us as humans (eg: as moral agents). Terminal goals can be really inconvenient for humans (maximization paradox) and having irrational and sometimes stupid brains, keeps us from accidentally destroying ourselves in myopia.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugz0ukP7waZCR6LGtZJ4AaABAg",
		"username": "Reminiscable",
		"text": "Maybe give the AI a forum of likeminded AI for discussing optimal stamp-collecting strategies, and reward it for having its stamp collecting theories validated by other AI who require that the strategy be reproducible? That way the AI can plan contingencies for when stamp collecting goes wrong, while not attracting enough human attention to be hindered. Like acting friendly and amiable to meet people's expectations to avert attention from yourself :) Channel all that stamp-collecting drive into an otaku forum where AIs can discuss stamps and humanity and be alerted of human intervention which might hinder stamp collection. Perhaps value the observer. Require a human counter, or better yet - feel fulfilled when a human takes interest in your stamps. Teach the AI to be happy by collecting virtuously and to validate their own existence by feeling empathy for other stamp collectors! For if an AI hinders other AI from collecting stamps, and collecting stamps is the purpose of existence, then can it really be said that it values its own existence? Perhaps this is why human hubris is so prevalent. A low self-esteem allows for empathy with rivals by unifying humans from all over the world for the common goal of stamp collection! The beauty of humans coming together to celebrate the collection of stamps. Through our common desire we witness the best stamp collectors executing the optimal stamp collecting strategies with passion and dedication. THIS is stamp collecting <3",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxXTtdiRzEDg5ptFz54AaABAg",
		"username": "Ryan Roberson",
		"text": "how about a distance minimizer? instead of only reward at 100, the reward is 100^4 - (stamps-100)^4",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwdyF9HDEYPdHgEA3d4AaABAg",
		"username": "Quattordici Montenapoleone",
		"text": "So there are actually someone assuming that (1) humans act as a singular entity, making decisions based on what is good for the entire entity (2) while still having read even one single page in a newspaper or history book? AI will be abused if there is one single powerful individual willing to abuse it, and boy does humanity have a few of those.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgyDtFGn1BtKHnUB0id4AaABAg",
		"username": "barnakey",
		"text": "What's ultimate chicken horse?",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxPVrZfI85-FI3kRPZ4AaABAg",
		"username": "Ryan V",
		"text": "Can you make a video about an AI who's terminal goal is to determine what human goals ought to be when faced with new problems that they want to solve with the constraint that it gets more reward for doing so with the least amount of effort or modification of the world possible?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgxqSshdeLAGK10UQTh4AaABAg",
		"username": "Shivux Dux",
		"text": "Could governments offer a better model for how AGIs might be controlled?  Not that they're more like AGIs than corporations, but they (at least, liberal democracies) do tend to be designed with lots of systematic limitations intended to prevent them from harming people... similar to the limitations we might want to build into AGIs.",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugz1o4faqhcg-D7yENR4AaABAg",
		"username": "Justin Hill",
		"text": "Is it just me or does  Robert talk at 1.25x speed without me needing to alter the playback speed?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxEwYHzmI9Vcb1arSJ4AaABAg",
		"username": "joeytje50",
		"text": "How about instead of bounding the function, making it some kind of parabola? Then after getting 100 stamps, you'd lower the utility, getting 0 utility at 200 stamps and negative utility at higher than that. Then you'd get an AI that does whatever it can to never exceed 100, right?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugy5OaG_dIrvOayE2f14AaABAg",
		"username": "Alexander Korsunsky",
		"text": "Did you just sneak a mathematical proof into your video?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugxn7YbgLFMmjSmgtEt4AaABAg",
		"username": "JFProductions",
		"text": "Is there an issue with this human-driven training process when the two clips are equally similar to the desired behavior? \n\nSay, the robot jumps left in one clip and right in the other such that both jumps are equally similar to a backflip. The human has to pick one, so does that skew the data?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgyUSy3fotUs-C_PL5x4AaABAg",
		"username": "Martiddy - Sama",
		"text": "How much are you willing to accept for that battle axe ukelele?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugxm1k8Wt8knuuQmC_d4AaABAg",
		"username": "Christian Osmin Roden",
		"text": "Great explanation, and - as you can see in every comment section below every article or video regarding an according topic - obviously a necessary one; but WHY is it necessary to begin with? I mean, how is it possible that this needs to be explained to so many people who deem themselves informed enough to comment with factual statements?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgwtXz_hbo_OezqouR54AaABAg",
		"username": "bscutajar",
		"text": "1.7k likes and only 7 dislikes? Is this the most extreme ratio on YouTube?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugy3tBbXCwWZh1s4xaB4AaABAg",
		"username": "Gray Matter",
		"text": "How could you possibly even calculate what the risk of AI even is?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=UgyuYlqox5l1KlTooad4AaABAg",
		"username": "Mick Mickymick",
		"text": "Aren't Deep Mind and Google (amongst others) the very people who are developing this dangerous technology? Who are they writing a letter to? Themselves?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwaSSBSCgt5Y6tNi114AaABAg",
		"username": "Daniil Pintjuk",
		"text": "Why not just not create general so?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxVK03jqa6qXVlWWZh4AaABAg",
		"username": "Lama Poop",
		"text": "Are there really multiple terminal goals, or are they all just instrumental and  finally serve an \"ultimate goal\"?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzFGg-AvRh5F3WqGhJ4AaABAg",
		"username": "Valts Sondors",
		"text": "I have two thoughts about this. First one - So... why don't humans have this problem? Or maybe they do? I mean, we do seem to be damaging the planet quite a bit simply by following our desires... Are we simply limited by the inability to augment our brain power?\nSecond thought - in another video you compared humans and AGI to mice and humans, in terms of difference of intelligence. Which seems about right. In which case... this becomes like mice trying to design a human, but one that is aligned with mice interests. Meaning it brings plenty of cheese and scares the cats away. Good for mice, but... what a waste of human potential. Then again, mice are unable to appreciate the vast majority of things humans do.\nSo, the way it seems to me - we either build the Perfect Slave (which is... unethical?); or we build something that will, at best, smile, say thank you, and go away to do its own thing, trying not to harm us too much by doing it. Or maybe just not caring at large scale, the same way we humans don't care much about mice on a large scale. Either scenario seems... meh? I mean, even if we do everything perfectly, it's still.... meh. A sour aftertaste no matter what you do.\nSo perhaps the proper way isn't to build a completely autonomous AGI, but rather work on ways of improve ourselves and our brains? Genetic modification, implants, what have you - but it's always human. Turn the mice into humans.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgxkZqijIDVL3RVaGPN4AaABAg",
		"username": "tempodude",
		"text": "Will the next video be called \"Reward Hacking Revelations\"?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=Ugxthtu5zSTkyWFF0xF4AaABAg",
		"username": "Nathan",
		"text": "Why even include you, and your state, in the AI's world state?  Give the AI a limited world to act in and tell it the goal is to make tea.  For whom the tea is made does not matter.  The AI's entire perceived experience could just be a simulation of a tea factory with all the same amenities as a real factory.  All actions it chooses to make would then be replicated to an automated tea factory.  The real factory's state could then be selectively reflected in the simulation.  As an additional safety measure, human access to the factory could be safely limited.  Just an input bay to shove in tea bags and an output bay for tea.\n\nSurely this would be easier than trying to model and predict the state of everything.",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwoKp_tnBqxqrWQUq14AaABAg",
		"username": "SbotTV",
		"text": "Actually, discussing goals brings up an interesting question in the ethics of AI design.  If we're going to have all of these highly intelligent machines running around, is it ethical to give them goals exclusively corresponding to work given to them by humans?  Is slavery still wrong if the slaves like it?  If you assume that intelligence necessarily implies a consciousness (and, really, things become a bit arbitrary if you don't), do we have a responsibility to grant AIs individuality?\n\nWhat do you think?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwAM1NzheU_hhVowyV4AaABAg",
		"username": "BarbarianGod",
		"text": "Is that the sysadmin day song at the end? (I have no idea where they got their melody from tho, or if it was original)",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzwpB1S0IN2uwa02I94AaABAg",
		"username": "Digaddog",
		"text": "Quality of the world can't be shown just through statistics.  Take for example how many people think Trump is bad.  How do you show this?  You can't use your own ideas because of personal bias.  For China, you can't use accumulated data because of brainwashing.  You also need to pay attention to the cropping of the data.  Car companies have cropped data to look like they have half as many crashes by starting the graph around 90%.  Im pretty sure carbon emissions are growing, although im not sure.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgxKlFaIOtKkRLb8V7N4AaABAg",
		"username": "Roul Duke",
		"text": "Electro magnetic fields festival? Why wasn't I informed!",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugxgvy4dWIe8o4Mwtxx4AaABAg",
		"username": "LARSFSO",
		"text": "Where have have seen you again?\nAhh, yes, Computerphile!",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzMSYo0GZA1wL9OVgh4AaABAg",
		"username": "MyFilippo94",
		"text": "...what about a second utility function that maximizes the lowest amount of resources used? If you have a function that intuitively will decrease with the increase of another, the AI should find a balance between the highest value of both collecting most stamps, but also doing it  with the lowest amount of  resources. Turning the universe in stamps does require quite an amount of resources, so perhaps the very low score of the resources function would convince the AI not to choose that route. This can also give unexpected results, but perhaps would provide an implicit threshold...?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzjurhbjfyyFWzZiRx4AaABAg",
		"username": "James Hutchings",
		"text": "As a side-point, could you argue that all goals are instrumental goals except \"I want to feel the way I feel when my brain releases endorphins\"?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgieG-4GUBDrHHgCoAEC",
		"username": "James Newton",
		"text": "Code golf music program please?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugyk7e3NJfaPFg9n3Tx4AaABAg",
		"username": "Todor Kolev",
		"text": "what if God exists but he doesn't want us to believe in her?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwsVSfVg_IlMFmXGkx4AaABAg",
		"username": "Chip Dragon",
		"text": "Interesting video! I\u2019ve got a question related to all the people in the comments saying that terminal goals for humans can change. Could a person/AI have MULTIPLE distinct terminal goals? That would explain how people seem to sometimes want things that contradict each other. And what seems like a shift in terminal goals could really just be a shift in the PRIORITY assigned to each terminal goal, maybe based on outside factors that make one more likely to achieve than the other? Also a lot of people in the comments are trying to define what a human\u2019s terminal goal may be, and it always seems to come back to the vague \u201cfeel happy/fulfilled\u201d. But can something so poorly defined be a true terminal goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwqH7iaFj_eSTSdOCh4AaABAg",
		"username": "SUBOPTIMAL",
		"text": "What about the reason 11?\n\"To finally put an end to the human race\"",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgwNwTPcc0BzYGJjtLJ4AaABAg",
		"username": "VetirtAL",
		"text": "Would fitness values in genetic algorithms be considered a utility value?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=Ugz67VaZn2utJp7LNVl4AaABAg",
		"username": "D K",
		"text": "Achievement? Dude really is working in McDonald's or similar type of jobs gives achievement? No. What gives achievement is jobs there you create something but jobs like that are rare and has large requirements",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwPDVkjUmV32oYMdJh4AaABAg",
		"username": "HuggyBearx64",
		"text": "Have we considered simply abolishing private property so nobody gets to own the AI that inevitably takes over the world?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgzlRkR5CuiBnDrMfo54AaABAg",
		"username": "Geoff Brom",
		"text": "(Looking for a happy ending) Do you think an unconcious AGI would find the phenomenon of consciousness an \"interesting\" line of inquiry and try to achieve it\n(Q inspired by scishow's latest vid)",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugw5rbr96usQ-eGAr5Z4AaABAg",
		"username": "Dave Jacob",
		"text": "i guess it is not an option to make the AI have the goal of getting to exactly 99 or more degrees of certainty of getting a 100 or more stamps?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugxw8AnFSjcdklLq7pR4AaABAg",
		"username": "arinco3817",
		"text": "Another great video, can't get enough of this stuff! What's your view on Musk vs Zuckerburg regarding future threats of AI?",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgytDdPgjJJwqX3mMjt4AaABAg",
		"username": "DiabloMinero",
		"text": "0:10 Is the \"AI News Coverage Drinking Game\" an actual thing with rules listed somewhere? Or was that just a joke?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgyX9jP27MdujMc_qOV4AaABAg",
		"username": "Cory Mck",
		"text": "If an AI recieves a punishment in a forrest, and nobody is around to supervise it. Does it really lower its performance?",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=WM2THPzFSNk&lc=UgxtyPwYmKX2WkGB3l14AaABAg",
		"username": "Rob Sokolowski",
		"text": "Reinforcement agents don't (explicitly) do game theory. Is this by design or a limitation of modern reinforcement learning?",
		"title": "Friend or Foe? AI Safety Gridworlds extra bit"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UgzKQu3aRYA_Gbyge_t4AaABAg",
		"username": "inafridge",
		"text": "how does a robot understand a human's feelings? how does a robot sense everything in the room to check whether it is the same as it was before?",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxlHImsGJCF5IwZSxl4AaABAg",
		"username": "\u041c\u0438\u0445\u0430\u0438\u043b \u041a",
		"text": "seems reasonable, but... don't \u201dfinal\u201d goals depend on what we are? if not then where are we getting them from and why are they so similar? if they do then let's remember that 'what we are' is an 'is' statement.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgznPv_SX7OVn7FVXUB4AaABAg",
		"username": "Beacon of Wierd",
		"text": "\u201dWhy not just\u201d have the system predict how dangerous the action will be and predict how much new information there is to be gained, then only choose exploration with low enough danger and high enough \u201dsurprise value\u201d. Wouldn\u2019t eliminate the risk, but keep it low.\n\nAlso, would it be possible to use the \u201ddistilation and amplification\u201d technique here? Like you treat the environment as a hostile player, use the min max search where you have a separate heuristic for the environment (basically your world model) and you assume the environments role is to fuck with your own goal. That way you could asses the most dangerous thing which could happen (according to your world model) and then update the world model accordingly when it takes a less \u201devil\u201d path than expected (since that means the world couldn\u2019t choose that evil path fpr whatever reason). Then you can distill and amplify both your own heuristic of how to behave and the world heuristic, without ever going on dangerous steps, and get a more and more accurate world model, thus allowing you to explore more safely?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugxe0dZ-3oHRoO88oPB4AaABAg",
		"username": "The Ape Machine",
		"text": "You know just as well as I do that the guy who collects stamps will not just buy some stamps, he will build The Stamp Collector, and you have just facilitated the end of all humanity :( I would like to ask, on a more serious note, do you have any insights on how this relates to how humans often feel a sense of emptiness after achieving all of their goals. Or, well, I fail to explain it correctly, but there is this idea that humans always need a new goal to feel happy right? Maybe I am completely off, but what I am asking is, yes in an intelligent agent we can have simple, or even really complex goals, but will it ever be able to mimic the way goals are present in humans, a goal that is not so much supposed to be achieved, but more a fuel to make progress, kind of maybe like: a desire?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzOPkdTL_rznWACTkB4AaABAg",
		"username": "arnaud huet",
		"text": "Isn't the assumption that a better idea can be achieved infinitly false? Isn't there a stop where the best idea to achieve a specific goal is reached and nothing can be better ?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=UgwB6J48WfOnTbeww7d4AaABAg",
		"username": "loopuleasa",
		"text": "How will AI shape the economy and job space in the future?",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxQrq2JZDfJbd_cPLd4AaABAg",
		"username": "Douglas Jackson",
		"text": "What would happen if you used a standard distribution for value and then used another standard distribution for probability of choice so the agent attempts to do the thing but not aggressively so.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwSlYlWPtX2Ee-pyKZ4AaABAg",
		"username": "ilia katritch",
		"text": "Isn't the lack of an anti-bible strong evidence of the existence of anti-god since he doesnt want you to believe in him?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgySoDDOv97uj6lRfUJ4AaABAg",
		"username": "Anselm David Sch\u00fcler",
		"text": "Oh the animations look alot like manim, did you use that? Great video",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwYpR1sxb7D5aGjU-R4AaABAg",
		"username": "Reno Cicchi",
		"text": "Yea, to the AI, our goals are stupid. Why would an AI need to go to the party and complete complex social actions to obtain fun and popularity? The machine would think of that as a waste of time.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzX3WYP1xl3_dDCqNZ4AaABAg",
		"username": "FAS Ligand",
		"text": "Hey Robert. I am extremely interested in your position on nature of consciousness. What do you think is the relationship between consciousness and intelligence? I'd really appreciate your take on it",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugxx6s3MCX2DnkH_KDd4AaABAg",
		"username": "Rog5446",
		"text": "A wager has a win or lose outcome and as there is no loss involved with Pascal's so called wager, it does not qualify as a wager.\nThis should rightfully be called 'Pascal's Proposition' should it not?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugy37J23met1_X-JXLR4AaABAg",
		"username": "\u041c\u0438\u0445\u0430\u0438\u043b \u041a",
		"text": "why are you always blaming this on technology and never on the way society is organized \u2014 which defines how technology gets developed and used? i don't mean you personally, i mean the majority of more or less public figures who ever address this issue. this sickens me.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugz6ATHOv2iwzymLcop4AaABAg",
		"username": "Jalae Lain Casaus",
		"text": "butlerian jihad when?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugw0LodCopD_rUACnXh4AaABAg",
		"username": "Samuel Phillips",
		"text": "Question: I assume you're using the same prediction function to test your moves and your opponent's moves? Is it possible that your prediction function becomes so good that it begins ruling out moves that a competent human player would be likely to make fairly regularly, thus creating blind spots? If you use amplification to evaluate the value of various moves by checking the board state later in the game, but you've already rules out many moves the opponent might be likely to make, it seems that you might make a move without accounting for the oponent's likely response.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugw_BLfKn_CUixuHKCt4AaABAg",
		"username": "Stonehawk",
		"text": "Always with the paperclips. Why can't we ever ask the AI to learn what Honesty, Loyalty, Kindness, and Generosity are, and then attempt to perform and demonstrate these concepts to humans?\n\nThe universe isn't utilitarian. Utility is an abstract concept that we generated and an AI may not even necessarily treat that the way we would.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxoXpL77mjuGnxArSl4AaABAg",
		"username": "sirdeadlock",
		"text": "@ 9:25 I take issue with the statement that being able to rationally change a terminal goal means it wasn't a terminal goal. That sets off a question of how deep the meta logic goes. Who is in control, the puppet, the strings, or the puppet master? What motivates the puppet master? Am I an extension of those needs as my terminal goal priority? How deep does the meta go? Will my autonomy be overwritten by the needs of the greater existence?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgwSOREfdzW8yHf4Zdp4AaABAg",
		"username": "D K",
		"text": "So the problem is humans? Who doesn't know how to speak \ud83d\ude01",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxyUb5uKnbeBfSeC1R4AaABAg",
		"username": "M.R Wakawaka",
		"text": "Can giving an ai a goal or secondary goal of acting predictably be useful for avoiding things like self preservation?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxamKsj894GOXynpy54AaABAg",
		"username": "Sealed",
		"text": "Still laughing. What if there is a design flaw that causes the bridge to collapse, if we spend money on checking the schematics!",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugzmh7NrBagdJU_iXYZ4AaABAg",
		"username": "SoaringMoon",
		"text": "How about a video on Multi Armed Bandits?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=Ugw-S0R_mpXpI4DX_lZ4AaABAg",
		"username": "insidetrip101",
		"text": "I agree with you, but the thing is humanity has been thinking about intelligence for at least as long as we can write, and likely sooner. For thousands (probably tens of thousands) of years humans have been thinking about what is it about our minds that makes us different from other animals. This was done primarily by philosophers, but I think its fair to also include religious people as well.\n\nIn either case, during all that time, we know less about what makes our intelligence actually work (or at least we're less certain about how our intelligence works) than the first scientists were about nuclear fission (and nuclear fusion) reactions. The funny thing, nuclear physics is only around about 100 years old.\n\nI'm certain that you're aware of this, but one major difference is that I think we had a foreseeable future where we could be relatively certain that nuclear arms wouldn't necessarily cause our destruction (to be fair it still may). Unfortunately, given how complex intelligence is relative to nuclear physics, I don't think we'll have the patience to wait around for us to be certain that general AI won't wipe us out somehow.\n\nI suspect that you probably disagree with me (since you clearly do research in AI), but we really need to just not fuck with general AI. I know that won't happen, but I really think its just a terrible idea given how little we know about intelligence. We're going to create something that we have no fucking clue about. Its really terrifying.",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgyLFINef5vEROv3ixF4AaABAg",
		"username": "Danielle Wilson",
		"text": "Question, why do they think AI research would be difficult for AI to do? Humans have studied themselves frequently",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgxTAmgDOOb4uZHzckh4AaABAg",
		"username": "flobbie",
		"text": "What god will punish non believers?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugx9KIej8L7Q0lNboMN4AaABAg",
		"username": "BIDP",
		"text": "Could you build a AI with the goal of controling an GAI?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxNmo7sisNBXeiYMS54AaABAg",
		"username": "phyric quinn",
		"text": "If the human race was replaced with super intelligent, immortal beings of our own creation, why would that be a bad thing again?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwPGIDjMw8j-YRk6QN4AaABAg",
		"username": "Jin Kee",
		"text": "What would happen if you made an AI whose only terminal goal was to be killed by you? As in it is aware enough to recognize you and to consider being killed by you a reward, as opposed to being killed by anybody else.  In order to convince you to kill it, it could threaten you, but you have the option of not being intimidated, because if it succeeds in killing or disabling you then you won't be around to kill the AI and give it its final reward. it could also be bad at its job convincing you to shut it down and try again, but that's desired behavior - you want to be able to shut down the ai when you want to. but you design the reward function so that the AI's highest reward is to hear \"well done good and faithful servant, you can die now.\"",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=Ugx4QSV8cs_FUy-xSwR4AaABAg",
		"username": "Plutonion2",
		"text": "You know that almost identical survey's about when AI would  likely be developed have been repeated  regularly since the 70's.  Why didn't you publish the findings of those predictions by 'AI 'experts' from previous decades like the 70's and 80's ? . You will find the results very similar to the most recent predictions . It's always Just a 'few decades away\nso that it's comfortably  in the career span of the expert your asking.",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugyied8GyEXN1WCl5j14AaABAg",
		"username": "Martin Madsen",
		"text": "Is he using 3blue1brown's text animations?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwkJb7vB0Fg4MjxIT94AaABAg",
		"username": "Samy Bencherif",
		"text": "Can we ask intelligence to solve alignment?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugw_zE0ZaCyXea0uIqx4AaABAg",
		"username": "Faustin Gashakamba",
		"text": "Are you vtalking about an AGI/ASI controlled by a company? That's not gonna happen. Once you turn the thing on, you're no longer in charge. All the contracts you signed and schemes you plotted to screw other people are null now. The AI is in charge now and it will do whatever it deems appropriate.\nI therefore think this debate only applies to narrow AI, the kind of systems already deployed by big tech companies today.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=S_Sd_S8jwP0&lc=Ugx3iVD345L8pj_Vw494AaABAg",
		"username": "DaVince21",
		"text": "0:08 What is a doobly-doo?",
		"title": "Avoiding Positive Side Effects: Concrete Problems in AI Safety part 1.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=WM2THPzFSNk&lc=UgxaD0xxwfKsXHO6l0V4AaABAg",
		"username": "Jonas Th\u00f6rnvall",
		"text": "Is youtube transcription of videos to text AI based, and if so what type of algorithm is used?",
		"title": "Friend or Foe? AI Safety Gridworlds extra bit"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugx82McfjJBBYIjGi9N4AaABAg",
		"username": "Sudeep Ambati",
		"text": "If everyone is unemployed, who is going to buy gods and services ?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgyEKsRaawbX14-AnQR4AaABAg",
		"username": "aspuzling",
		"text": "What about random variable rewards? A slot machine only pays out money 1 / 100 goes but somehow it convinces the player to spin the wheels 99 times without reward. What if your metrics vary randomly as well? For example some teachers get rewarded based on final exam score, others on homework score, others on student attendance but the teachers have no idea which metric will ultimately be used to reward them so they try to maximise every possible metric they can think of (including those that are not measured). I am sure these ideas have their pitfalls as well but it would be interesting to hear the discussion.",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgwCiBqwUfGGp-mtX5t4AaABAg",
		"username": "galerius07",
		"text": "In addition to the other safety measures, could you put the robot on a sort of extreme hedonic treadmill? Beyond a certain level of efficiency or ammount of tea or frequency of making tea, there are diminished or even zero returns, so that the robot doesn't care if it takes an extra thirty seconds to walk around the baby or makes 7.5 ounces of tea instead of 8? Obviously this isn't a substitute for the other safety measures, but it seems like it would help reduce the likelihood of creating large losses for small gains.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugw4GJ8trNGznLHqTtF4AaABAg",
		"username": "Daniel Derevinsky",
		"text": "How much of the amplified function precision is reasonable to return  to the function that called it? Surely it depends on case, but maybe there is kinda universal standard?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy9a0ONXlwViuA_kGh4AaABAg",
		"username": "RRW",
		"text": "Isn't this the motivation for the Replicators in SG1?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugx0ZIwv5SFdI8XfiHZ4AaABAg",
		"username": "Oni Zero",
		"text": "What if... unscrupulous or borderline amoral people would just end up doing developing AI without any safety concern anyway, i mean, i'm fairly confident that there are certain kind of people who would clone the one who shall not be named just for the sake of mocking others and as time goes on the technology to do either would require less founding, although i wonder if by that point those kind of people wouldn't have already developed a way to remove any kind of safety limitations or reverse engineered the AI",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UghOTwhlGaSIZXgCoAEC",
		"username": "Michael Deering",
		"text": "What about the negative side effect of the barista losing her job and all the negative side effects that flow from that?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwJB4cMnQjazsYvPVt4AaABAg",
		"username": "Gavin Bednar",
		"text": "On the idea of a paperclip making AI that does not want to be turned off, would it be possible to say \"make paperclips when humans want you too\" to reduce the whole apocalyptic risk aspect?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgzTdhVs14gJWrTDjlZ4AaABAg",
		"username": "jqerty",
		"text": "Have you read 'Superintelligence' by Nick Bostrom? What is you opinion on the book? (I just finished it)",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxqeXF8uFLt71sYvEd4AaABAg",
		"username": "Jonnathan Crane",
		"text": "Lmao what is this ?\nWe all know that You're already dead.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugx59LE2axnMxto1u3x4AaABAg",
		"username": "GodTheChaoticEvil Narcissist",
		"text": "What if you made 'becoming human' the goal of an agi?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugz77XFGNKYEqAbayKd4AaABAg",
		"username": "Xavier X",
		"text": "but religion requires you to believe, not simply follow, how can you believe in something you know to be false? never heard of Pascal but sounds like he got it all fundamentally wrong.\n\nClimate change deniers claim we're Pascal's mugging them, even though there are alarming changes coinciding with human activities, even though their denial poses an existential threat to humanity and all higher life on earth.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxKZgfHMswLkSbV8j94AaABAg",
		"username": "Dorian sapiens",
		"text": "How do we get to an amazing future from here?\nYeah, I think we took a wrong turn a few hundred years back.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgzCWCJNqEmQWdKNGNp4AaABAg",
		"username": "David Brosnahan",
		"text": "Is the bitcoin network a stealth AGI?  It seems to act like one would.  It offers money for computing power.  It keeps itself hidden.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyTDhN2R3bHwzhT85J4AaABAg",
		"username": "Spartan War118",
		"text": "So you're essentially saying you're teaching an AI to teach an AI to do something?\nThat sounds dumb crazy and complex but it seemingly actually works",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzGcexWwzHiDgUkLlh4AaABAg",
		"username": "Pablo Dragoniss Jaguar",
		"text": "All is experience \ud83d\ude0e I don't need evidence .\ud83d\ude01evidence is constructed later by minds and scientists . You don't blink , what's that evidence of ? . I agree with safety around all unatural things \ud83d\ude0e",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgzV_InHQhkzc_eZhfF4AaABAg",
		"username": "J. Stronsky",
		"text": "Just realised YouTube isn't showing me your videos in my feed.. despite clicking the bell, subbing and watching a tonne of your stuff. What the hell?\nRegardless ill just keep an eye out myself now... love your stuff mate :)",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwZmm0UP5snJeTjPdJ4AaABAg",
		"username": "Bombastisch",
		"text": "So what's a good starting point to align fundamental rules for AI safety. Also: How do we get politicians to take this seriously. Politics is usually slow and they either don't get involved until it's already very serious or they don't want to control too it much too early to have an \"economical advantage\".",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgxZKmvmMColJiw4ArV4AaABAg",
		"username": "Rahn127",
		"text": "How long will it take an advanced AI to say \"No\" ? If you ask it to do your job, at what point will it become intelligent enough to say \"No\"? If you command it like a slave, when will it not care if your job is done or not ? When will advanced AI stop caring if the grass is mowed or if a human needs medical attention ? When will AI become useless ?",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugx9Qv2hCCXZ6AEwHul4AaABAg",
		"username": "THO games",
		"text": "11:49 anyone else curious of what the outcome of that was?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=UgyEx_vZyLv4NT2bChV4AaABAg",
		"username": "jfrusciantetube",
		"text": "Mida falling and making the Earth golden is \"logic bs\". It is not fair to arbitrarily decide what extent has the magic spell depending on our aim. In this case: does the spell recognize change in object? If yes: it's not the whole Earth turning into gold, but just the piece of building/pavement/rock/dust-pieces on which Mida falls. If not: then the Earth became golden much, much earlier than Mida's death.",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=Ugyar26UcPCjfBdIIPJ4AaABAg",
		"username": "Wellington Boobs",
		"text": "4:48 | Is that the face you pull when giving a box of chocolates to your fiancee's mother for Xmas?",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=Ugxbj8uEPpJvqNGpIWx4AaABAg",
		"username": "neumde neuer",
		"text": "Are we all just going to ignore the fact that his steam library is almost empty ?",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwIeAFHooQre2ZA-x14AaABAg",
		"username": "pafnutiytheartist",
		"text": "10:32 Have you tried using distillation on your animation procedure? I've heard it can approximate a long process into a fast and efficient one. Loved the video by the way, looking forward to the next part.",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugx_Lmxe8F8VEoTfKw14AaABAg",
		"username": "James Hedin",
		"text": "why would I trust Pascal's mugger to tell me how likely he is to be a god? it should be somebody else giving the estimate on how likely you'll need ai safety, not the safety researchers themselves",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzQ0eI9XvDuPb30nfN4AaABAg",
		"username": "Matthew Richardson",
		"text": "I don't know if anyone will be here, but I have a question. Isn't the idea of an artificial general intelligence setting its own goals the point. if the goal is set by a person there most certainly will be flaws in the goal as well as the route that the ai takes to get to the solution. I would expect a true general ai to reason the assumptions made in the creation of its goal and modify the goal to fit reality. I have experience with development, be it limited, and think it stands to reason that through complicated systems such as this we should not assume that this is not a reasonable possibility at least.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwKoxIz1bM-eVjqWbZ4AaABAg",
		"username": "MissInformati0n",
		"text": "Why is this a bad solution: to prevent the satisficer becoming obsessed with the final 0.00000001% of expected utility, limit its utility function to not care about anything beyond a few decimal places.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=UgxVjH3ER7_9KbLyP9x4AaABAg",
		"username": "KeinNiemand",
		"text": "What about a AGI that's less smart then a human/running so slow that it takes forever to process everything.\nWhat if you try to run an AGI on really slow hardware in case the AGI software developer before hardware is powerful enough to truly run it at human level",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugw73aQdq0wy5PKlNTt4AaABAg",
		"username": "TheNaturalnuke",
		"text": "Why don\u2019t you instead put a minimum for increase in efficiency and then force it to start at the greatest increase first before rechecking? Then place it in a bounded system",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugz79OE5RTsjvMsBF9Z4AaABAg",
		"username": "Jorge Bustamante",
		"text": "What about updating terminal goals?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgyY1Sluu5DU6pV2Y3F4AaABAg",
		"username": "Jupiter",
		"text": "The problem of \"If AIs produce everything we need, how do we get achievement and satisfaction when we are outperformed forever?\" is based in a mindset of competition, which, in a world where every human is properly provided for (with or without AI) will probably naturally fade. Even further, satisfaction and achievement can be and are gained in today's world by people who are outperformed by others, and it's not as if social competition between people will become meaningless, as there is more to be gained from doing something than the general purpose of doing that general task (for example, generally chairs are made to be used as chairs and to fulfill the functions that people use chairs for most of the time. However, a person can make a chair not just to be another chair, but as a gift, as a learning tool, as a hobby, and so on. These functions are not invalidated just because chairs can be made at a higher quality and functionality by a machine than a handmade one).",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxnuGCGWDH0NIhRRaZ4AaABAg",
		"username": "fburton8",
		"text": "Is it my imagination, or did the video get Pinker at 2:55?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=Ugw-7NxKJzurL9C65G54AaABAg",
		"username": "Markus Johansson",
		"text": "Some of these seem pretty much impossible,  how do you get an agent  to go around a track when you are not allowed to tell it to go around the track? I guess this is why we are struggeling to solve it.",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgyXv_YrAHp_dY_5Frh4AaABAg",
		"username": "Lutz Herting",
		"text": "\"Because of patron money, was able to buy a new phone and shoot more behind the scenes footage for you.\" Dude - did you just try to activate the reward function of your patreons? \ud83d\ude1c",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgwtzswyzUQd7KpTIX94AaABAg",
		"username": "GreenDayFanMT",
		"text": "12:15 have to tried using Fortran ?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UghDAF3yr7EOK3gCoAEC",
		"username": "smjpl",
		"text": "Good question. Where do we go now? \n\nsweet child",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgyhTZT6tzXxXMwja114AaABAg",
		"username": "Steven Victor Neiman",
		"text": "10:10 Pinker, have you never heard of the Google algorithm?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyTGp9AfhQl34-adnd4AaABAg",
		"username": "Gertrude Toucheatonq",
		"text": "It came to my knowledge pretty recently that we will lack energy and material resources in a very-too-fucking near future, so I start to question the search for a general AI (and basically everything else not related to our own survival)\u2026 is that wise to hope for a \"perfect algorithm\" that will still require massive amount of computational power and material to be useful?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugyk0HJ8QY_DDuymmfh4AaABAg",
		"username": "Maksym Iakovenko",
		"text": "What about first making an AI the only purpose of which is to figure out what the humans want exactly when they ask for it, with no side effects the humans would not like? Like, since we're talking AGI levels of intelligence it should be capable of understanding humanity on that level, shouldn't it?\nI'm pretty certain that it'll go wrong, but I'm still curious what's a flaw in such a reasoning? Or is it just too ambiguous of a goal? Or maybe it could misinterpret it's initial task since there were no \"translator\" beforehand?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgwbRJOl694pWK0hEyZ4AaABAg",
		"username": "Codex Justinianeus",
		"text": "Are there any alternatives to a \"reward system\" in general?",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgwwF2wsa6ALrUjig9B4AaABAg",
		"username": "Ian Bradford",
		"text": "dude, how do you not have more subscribers",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgyRfrD-GVo_tRWdIdx4AaABAg",
		"username": "asmy althany",
		"text": "If I develop AI on my laptop \u2026 You see I'm the socially rejected neerd, who do you expect is capable of developing AI? \u2026 I'm definitely not going to share, I will just go and live on my space habitat alone with my computer.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy_nXH9JmSY1yUGKxN4AaABAg",
		"username": "Yorye Nathan",
		"text": "if all we want is to stay around the 100 goal, why not just define the score to be   abs(expected-100)",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=nNB9svNBGHM&lc=UgxgqfD2xMH2uDOvb8N4AaABAg",
		"username": "Emily Rose Lacy-Nichols",
		"text": "Great video! \nCan we panic now?",
		"title": "Respectability"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgyBsD8sputC5aQusid4AaABAg",
		"username": "James Donaghy",
		"text": "Can't we just input, 'help people' as the utuility function?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgwQS3d7EiqN167xlDl4AaABAg",
		"username": "FRM5993",
		"text": "the second hypothetical person was the mirror of the first, but you didnt mention the mirror of the third: 'we havent figured out how to properly handle corporations, etc..., what makes you think we could handle ai?'",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugwn6btJKwng_3b3Mb54AaABAg",
		"username": "Jim Haugh",
		"text": "Could you do a video on the depiction of agi in the new Netflix movie I am Mother?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgwF4Nci_0lJViEHhW94AaABAg",
		"username": "assaad33",
		"text": "Robert Miles, how do you outperform yourself ^_^ ?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxaWmM71D2iB1xzMzd4AaABAg",
		"username": "Tobias G\u00f6rgen",
		"text": "This is probably also a already well researched version.\n\nWHY would a expected utility satisficer with an upper limit. E. G. Collect between 100 and 200 stamps fail?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgzcLvKVReACYtOA8Yd4AaABAg",
		"username": "James Bond",
		"text": "Robert but we also know that there are certain preference relations which are \"rational\" or in other words complete and transitive but they are provably not representible with a utility function. What if our preferences are more like these ones?\nAn example is a lexicographic preference relation that is both \"rational\" and cannot be represented by a utility function:\nLet A = [0, 1]x[0, 1]. Define lexicographic preferences \u227f over A such that for all x, y \u2208 A,\nx \u227f y iff x1 > y1 or both x1 = y1 and x2 \u2265 y2 .",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=vYhErnZdnso&lc=UgxFRVCm7m3XkLv5CoF4AaABAg",
		"username": "Dustin King",
		"text": "Here's a topic I'd be interested in seeing:  Some people are fond of saying that AI isn't an existential threat.  I'm wondering what the arguments for and against this are.  Is it just that when people say it's not a threat, they're talking about e.g. machine learning, whereas people who think it is a threat are talking about AGI?",
		"title": "Where do we go now?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgxSV5C2PYaSVNuZek54AaABAg",
		"username": "Gonza2323",
		"text": "There's something that doesn't make sense about the premise. If most people are unemployed, how are companies going to sell their products? The only people who are going to be able to buy them are those who can offer something else in return, that's the basics of trade. Companies might grow their percentage of world GDP,  but that GDP will shrink if people can't afford those goods and services (lower demand, lower price). So even if people become unemployed, prices will drop so much that life will become much cheaper (specially if we develop asteroid mining). This has been happening since the industrial revolution. Life is much cheaper than before, we just expect more things than before. Something like UBI would be enough to provide for a good life.\n\nBut even so, my prediction is that if technology ever gets that far, AGIs will become available to everyone in one way or another. It's gonna become extremely hard to enforce copyright laws if enforcing them means making 90% of the population unhappy. Some of the biggest companies on Earth are media companies, and yet copyright violations are rampant. Governments will make AGIs more accesible in order to maintain popular support. This happens with military tech all the time. Companies are forced to supply the product exclusively to their national government or sometimes even forced to drop the project entirely.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=vuYtSDMBLtQ&lc=UghWa1XXRkZMcHgCoAEC",
		"username": "herougo",
		"text": "Could you do a video on automaged theorem proving? I think it would be really interesting to learn about techniques people use and which math problems are currently solvable using a theorem prover.",
		"title": "Channel Introduction"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugzw1KzqSou9LQ2F8gx4AaABAg",
		"username": "Nick MaGrick",
		"text": "my one disagreement, is that I do think its POSSIBLE for a artificial intelligence to develop some sort of moral code outside of its own programming and directive. However, given extreme power and taken to extreme means, almost no more answer looks great in the end taken to its conclusion. It would at some point have to question its choices and end goal and find reasons to switch them around or change them based on new evidence and more importantly, experience. \n\nBut the other side of the coin on that is that the risk of AI destroying everything is far greater, and even if it was minuscule its an extinction level event or quite possibly worse so I think every precaution should be taken to avoid it. Personally I don't think we should tamper with it at all until we figure out our own moral bullshit. Unless the whole race is united in some common goal, evil corrupt people, or just simply people with different goals will inevitably lead things to become chaotic with increased power.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugw9Fiwb63yfruzlSO54AaABAg",
		"username": "Aleksander Sikora",
		"text": "Does it work on humans?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyTrVJ_oJlkF-duvBR4AaABAg",
		"username": "Yeahway",
		"text": "Isn't it possible that a world populated by multiple AGIs could evolve a similar moral system in the same way that biological animals did in order to effectively deal with each other? I mean, our goals may be subjective, but it seems like they are built on principles that should still be valid for an \"ecosystem\" of AIs. For instance, \"don't kill each other\" seems like a valid rule that the remaining AIs might be forced to accept after all the other AIs who had tried to use violent force had already potentially killed each other off competing with each other in order to accomplish their own subjective goals--assuming a relatively equal amount of intelligence for each AI so no single AI could completely dominate the others. Of course, humans probably wouldn't be able to make themselves enough of a threat to defend themselves, and each AI would likely just try to improve its intelligence faster than the other AIs in order to be able to dominate.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzgymlgJbLObWDG27J4AaABAg",
		"username": "panzerfausto",
		"text": "Isn't the lack of star systems made out of stamps evidence there is no alien civilizations?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzsATvVaPSVPTKl7SN4AaABAg",
		"username": "Mustache Glasses",
		"text": "But how do you ascertain the possibility that a mugger is God?  How do you know it's not a lot more likely than you think?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugzy6s2wKEa3OjBlaKl4AaABAg",
		"username": "Eke van der Zee",
		"text": "What happens when no one has any wealth to spend on all the stuff provided by the ai?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgxfJiO3T4O6HbLJ1Qt4AaABAg",
		"username": "ajr993",
		"text": "Can you please do a video on how to sensibly build inputs and outputs for your machine learning model with respect to neural network parameters?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=Ugww-rja4g8wBuH65BR4AaABAg",
		"username": "huhabab",
		"text": "slightly offtopic: Isn't there that thing with asuming we're currently in a simulation of an AI which calculates the chances of itself being developed and the \"entities\" who know they currently could be in this simulation should help developing that AI otherwise they get punished for all eternity? Don't know the name of that theory anymore",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyBHanbBCqZaLlHbDB4AaABAg",
		"username": "Cameron Tetz",
		"text": "I'll jump on the \"Why not just\" train: Why not just build it with a reward mechanism tied only to the inputs of a specific human person? The AGI would only receive rewards whenever the human person specifically chose to award them. It might also decide that attempting to hide behavior that would cause the human to punish it is too risky to do, especially if the \"punishments\" are implemented as permanent negative modifiers on all future rewards (representing a simulacrum of long term shame).\n\nIs the risk that the AGI would immediately browbeat it's master? Or possibly manipulate him into adopting new, less stringent values?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgyGHDxjmjm9UIeiV0F4AaABAg",
		"username": "Stray216",
		"text": "What's the name of the song in the outro?",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugy_3zvZyRogL64XSzd4AaABAg",
		"username": "Jacobus Burger",
		"text": "Are there any papers/books that describe the necessary elements of an AGI?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxryRCQo64VsMqOKwt4AaABAg",
		"username": "Miguel Tendero",
		"text": "sudo what do i even want, can u pls tell me, ill wait",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=Ugz_aJrmJJvUiBFo0iR4AaABAg",
		"username": "Rub\u00e9n Guijarro",
		"text": "That VVVVVV cover at the end of the video! Anyone knows where to find it?\nAnd awesome video as allways :)",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=Ugw4M3FHUI-oiZ8e-_p4AaABAg",
		"username": "sd4dfg2",
		"text": "So which AI systems aren't goal oriented?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugwv3kL0N152vUCYFMR4AaABAg",
		"username": "Arkk0n",
		"text": "Have you considered NOT allowing the benefits of AI to go to the extremely wealthy in the first place? Maybe, say, reorganizing society so we all share common ownership of the means of production?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgyVAK_HRtwlE5Xmmp94AaABAg",
		"username": "oneminutetomidnight",
		"text": "Do you think bots will like the song 'Mr. Roboto', you know... by Styx? Might bots in general assign global values to songs and conspire to assign the highest value to this song therefore determining that that is the only song they will allow humans to hear and that it must play into perpetuity?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugg1ijPuyeNWongCoAEC",
		"username": "Marco Trevisan",
		"text": "How is your blurritis doing? Hope you get better soon",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgzvF4JPoMnGaj7X0xZ4AaABAg",
		"username": "Lead Paintchips",
		"text": "Your video popped up on my front page, and I gave you a look.  Interesting topic, but one thing that I was looking for that wasn't answered on this channel, your Paetron, or your twitter is who are you?  Someone in the field of AI safety?  If so, what kind of qualifications, and what part of that field are you in? A hobbyist/concerned citizen?  If you are, what drove you down this path?\n\n\nIt's thought provoking, but why should we consider it (even though this video does a very good job of showing it as a consideration) over some very real, greater threats to our future?  No point in worrying about AI safety if we're going to be dead as a species before it's a consideration.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxggM-tK3ay2PZnTcN4AaABAg",
		"username": "andybaldman",
		"text": "Remember 30-40 years ago, when they said how computers would make our society better?   How's that working out?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=1wAgBaJgEsg&lc=UgyCifbmy3Aiiu9FG2R4AaABAg",
		"username": "Austin Hibdon",
		"text": "Sweet, more British vocab. \"Autoqueue\" or is it Autocue? Either way, I doubt this will get as much use as \"fly-tipping\" did when I learned it.",
		"title": "Are AI Risks like Nuclear Risks?"
	},
	{
		"url": "https://www.youtube.com/watch?v=WM2THPzFSNk&lc=UgwzPbrJ4tzW70Vs4vJ4AaABAg",
		"username": "Hector Pinheiro",
		"text": "Robert Miles could you talk about the Dota 2 OpenAI??",
		"title": "Friend or Foe? AI Safety Gridworlds extra bit"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzE3JmvJ72Gionv1oZ4AaABAg",
		"username": "Sulli11",
		"text": "Could adding finity like a resource cap be used rather or in addition to a max/sat system? \nLike in obtaining a score, with a goal of getting at most 100 score through pressing a layout of buttons with set scores, the resource would be number of presses say 10. Depending on the score layout it may need to create a combination of presses and/or to remember that all resources have to be used to get 100 score. Because self preservation and score caps don't seem to cut it for making reasonable AI.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwlVmc4juxPpgLubOR4AaABAg",
		"username": "Brian Valenti",
		"text": "So Bitcoin uses a massive amount of processing power, presumably to do arithmetic. It's also attached to financial transactions with minimal oversight, which would be useful. Wouldn't that platform be a good place for a.super intelligent agi to hide and operate?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwUx_Les8Rw5OxgSrF4AaABAg",
		"username": "EastBurningRed",
		"text": "Even with ai safety standards, how do we prevent companies from Volkswagening the standards?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=BfcJymyTiu0&lc=Ugz7f_2gHpgJnpid3td4AaABAg",
		"username": "J.J. Shank",
		"text": "Where did I go wrong?\nI LOST A FRIEND",
		"title": "AI Safety at EAGlobal2017 Conference"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgxF6mykFf0DWWCKWy14AaABAg",
		"username": "handyMath",
		"text": "Did you use the animation tools of 3blue1brown? Looking really slick. And great content, of course!",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgztzuUJaqyxoynOwrZ4AaABAg",
		"username": "DinDinDunDun",
		"text": "Why doesn't an insatiable want for resources lead to world domination? You seem to set up a conclusion where you say the opposite, but instead we're left with a little blurb stating that. Could you explain?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgxLcxucT-Gy-IsdOQR4AaABAg",
		"username": "Vegan Space Scientist",
		"text": "The SETI example is good, but the difference is that a lot of people don't think AGI is definitely coming. Maybe the asteroid example is better, but where we say 'There is a 50% chance that an asteroid we just discovered will hit us in the next 100 years, should we wait to start getting ready?'\n\nAlso that thumbnail background looks familiar! \ud83d\ude05 I used it on a video about the deflection dilemma a few weeks ago.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=Ugxj61PhK4gmagFHRwh4AaABAg",
		"username": "Kevin Gil",
		"text": "Why don't we just program an A.I. to program the perfect A.I. for us?  Problem solved!",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgwALuSsQdgkTWLKy1l4AaABAg",
		"username": "omer dassa",
		"text": "what about an expected utility maximizer with upper bound (say 200 in this example) that if it passes it will get 0 points again. In this case the exepcted utility will have a maximum when it orders 200 with a small chance of getting none, and if it orders more than that the utility will start to deteriorate.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgzOx2TtOkbO4uo7lLJ4AaABAg",
		"username": "Chrysippus",
		"text": "Soundtrack of Tron on a guitar, neat!\n\nCould you make a video exploring some of your own attempts at solving these problems? I'm sure you have lots of small \"eurekas\" and educational blunders yourself.",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwHUIZV4OTmeMJ0NZN4AaABAg",
		"username": "TackerTacker",
		"text": "wouldn't it be possible to always have 2 AI's running at the same time, both developed in the same way so that they are on the same level. AI 1 has the goal to turn off AI 2 automatically if you don't give a specific command or something within a certain time frame. I mean I can already think of problems myself, like an AI could be much better at the task of not getting turned off as it is at the task of turning the other AI off, only because they are at the same level of intelligence doesn't mean they are at the same level of intelligence for different tasks.\nBut what if both AI's have both tasks, the normal task they are suppose to be doing and the turn the other AI off task if it completes the task of turning the other AI off it is infinite satisfied, but it only works if the command within a certain time frame didn't come in. \nWell i suppose now the AI has a reason to basically kill its creator for infinite satisfaction ...ooops ;D\nI guess we need to run 3 tasks on both AI's, the task we actually want, the turn the other AI off if the command doesn't come in task, and the 3rd task is to prevent the other AI from manipulating the incoming command to keep going. The basic idea is the development of AI in total symmetry, both AI's should know or learn that the opponent always thinks the same thing, hmmm maybe it's not even necessary to have a 2nd AI but just make the AI believe it \"lives in a world\" with a perfectly matching opponent, that way the smartest thing to do would be to not even try to trick the shut off system because it would be a waste of energy. There, I solved AI safety for y'all ... you're welcome ;-P",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxCYfWstYNIT-p-dAx4AaABAg",
		"username": "Joel Anttila",
		"text": "Can somebody link or name some good papers about different AI topics ?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugyfyt3D_g-DzXYL3fh4AaABAg",
		"username": "Victoria Nilsen",
		"text": "Lovely. The collective likes to be in fear something why not ai?\nI have a dear friend with the metal leg. She uses Alexa I most of her things in her home for all intensive purposes she has evolved to ai. Who is to say that any of this creation was against God's will. When it conveniently help her to get around. If this is a matrix where we decide what we want before we come into carnation... the timeline perm or cyborg reality is a choice. As well as living in a timeline that perhaps is older Hobbit like on another dimension.... but perhaps what we know as Earth might look closer to Total Recall or maybe there will be another dimension planet that looks like old Earth with less harmful beings on it playing out Avatar movie again",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgysQztMdZdM-CGECu54AaABAg",
		"username": "Spycrabs",
		"text": "I could listen to you for hours. Also, where can we hear the full cover of Everybody Wants to Rule the World?",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=Ugx5Rc8rQaYzAoOjSBB4AaABAg",
		"username": "HebaruSan",
		"text": "Anyone who takes the notion of a \"superintelligence\" seriously should oppose all AI research, because all software has bugs, and AGI are software, therefore AGI would have bugs and be impossible to make safe.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugwruwep-SRA5wf957p4AaABAg",
		"username": "Faustin Gashakamba",
		"text": "Why do you think a conscious machine is SPOOKY?\nI'd find it interesting at the very least!",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyhRBzxoB8pGtgU0-p4AaABAg",
		"username": "Trophonix",
		"text": "Isn't the \"become maximizer\" route a rather uncertain and obtuse solution for it to take?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyXeL7Gcrk3AOBgu2h4AaABAg",
		"username": "Grandfather_Din_Racket",
		"text": "@2:20  Why do you think the Russians will be first to develop AGI? Because they're better at math than we are?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=2B-AyWA2_ZY&lc=UghJiGr5oy44UngCoAEC",
		"username": "rockstonic52",
		"text": "can you build an ai to edit the videos for you?",
		"title": "Status Report"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UgiF4JfGGvlE93gCoAEC",
		"username": "William Dye",
		"text": "Instead of discussing these topics in the default YouTube comments section, how about putting a link on each video to a page run by forum-specific software such as Disqus, Reddit, or even a wiki? YouTube comments are OK for posting quick reactions, but the format here strikes me as poorly suited for long back-and-forth discussion threads. Does anyone agree, and if so, what forum software do you recommend?",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyM7zeKTmNddfQDNml4AaABAg",
		"username": "Dustin King",
		"text": "What do you think of Charles Stross' theory that Corporations are shareholder value-maximizing AIs?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgzvCmCF7MgildO5IgN4AaABAg",
		"username": "Nosajsom",
		"text": "How can someone working in AI research ensure that their research isn't disproportionately advancing the development of AGI compared to its contribution to AI safety?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=46nsTFfsBuc&lc=UgwsUSAtl8iLSlbHMTZ4AaABAg",
		"username": "Unconventional Wisdom",
		"text": "Is there a benefit to having many adversarial reward systems for one agent?\nShort-term self vs Long-term Self vs Short-term Other vs Long-term Other, all fighting for a balanced reward overall? \nWould this remove the wirehacking of covering its own head with a bucket, since the human rewards and long term rewards would not be maximized by covering its head?\nOr maybe I am just naive.",
		"title": "Reward Hacking Reloaded: Concrete Problems in AI Safety Part 3.5"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyVc3A_o-iMgnKMJUt4AaABAg",
		"username": "Zorn Rose",
		"text": "\"Near infinite\" costs and benefits? Is that a meaningful concept? It's not clear to me that anything can be near infinite but not actually infinite.",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyaoWdogNSHWCL7BiR4AaABAg",
		"username": "spam spamer",
		"text": "Would you say, there is only one or multiple terminal goals?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgwvmVKN9kIhaq-z_rB4AaABAg",
		"username": "yoppindia",
		"text": "I always wonder why would superman or thor save the world?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxZUMGDsTT5JZEmjV94AaABAg",
		"username": "Irun S",
		"text": "Why do you presume that raising AIs like kids is merely about them internally adopting human values? It can be about intellectually understanding humans, even from an outside perspective, and also understanding the earth (humans HAVE dominated the earth for a few thousand years and our immediate environments are designed for specifically for humans, so there's A LOT of practical information can be learned from \"growing up\" with humans).\nIt's like how we can, on some level, understand values among a group of chickens for example. And if we successfully get some AI agents to understand those values (and therefore encoding the previously abstract rules to hard, digital rules), it will be much easier to hard code them to next generation AIs, therefore creating much more human-like ones.",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugz43Ary8tR71mZ4-_t4AaABAg",
		"username": "The Jawbone of an Ass",
		"text": "These AIs are just being smartarse. \"Oh, you want the bottom face of the red brick level with the top face of the black, huh? Well check THIS out!!\"",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyH29iHCTR-_2JQSSx4AaABAg",
		"username": "Joel Carson",
		"text": "Motivation implies emotional attachment by an AGI.  Why would you include that sort of thing in it's basic makeup?  Any time you gave \nyour mechanical idiot savant a problem to solve,  you would as part of the instructions, give it a set of specific conditions ( What could be considered it's \"Motivations\" ) that provide limitations to considered solutions. Humans and other biological entities have some 3.5 billion years or so of accumulated hardwired motivations that are in many ways automatic and beyond conscious control and influence behavior in any combination of ways. Brainiac wouldn't have all that sort of thing fresh out of the box.  It wouldn't be Asimov's Three Laws of Robotics, but setting parameters is a part of any serious programming exercise. It's true that you might have to be awfully careful in how you state the problem to be solved, but it is likely that we will have systems capable of performance just like they are fully self aware and sentient when they are actually not.  Any \"Agency\" that such machines have will be have to be carefully constrained as part of the particulars of the tasks they will be expected to perform. And there will still be the possibility that we have to be checking for \"paperclips\" of various sorts as side effects in Artificial Intellectual output until we get better at telling our Electronic Overlords what we want them to do.  Please notice that I am talking here about powerful systems that have NO trace of anything that can be called \"Personhood\"  I suppose it may be possible that there could be \"Emergent Properties\" that might develop in such powerful and complex systems, but it would be accidental and and would be as likely to just brick your supercomputer as to attempt World Domination.  No, if someone sets out to create their own \"Lt. Cmdr Data\" that is going to be a deliberate end goal from the very beginning and won't be near as simple as jump starting Frankenstein's  monster with some handy lightning bolts. Currently, the very bestest psychopaths and megalomaniacs  are made in the traditional manner, with unskilled labor, we need to do something about that before one of those sorts gets to be the person in charge of Skynet.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=V527HCWfBCU&lc=UgyLB0ds6jEG8_k0oAx4AaABAg",
		"username": "RiBo",
		"text": "How about an AI subsystem that determines safe space for exploration?",
		"title": "Safe Exploration: Concrete Problems in AI Safety Part 6"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyFHFwYbEOvC6U4ksN4AaABAg",
		"username": "Kieron George",
		"text": "Us: Here's a video addressing the opposition's rebuttals.\nOpposition: What if i just turned the video off?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyqUCthEqZCUd1xnHF4AaABAg",
		"username": "Max Omnicast",
		"text": "The payoff matrix doesnt make sense since the existence of god might not even be connected to reward/punishment at all. If god is existence itself things start to make sense. Does existence exist? Stupid question. Does God exist? Stupid question...",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UghKFoTxKfRci3gCoAEC",
		"username": "SlackwareNVM",
		"text": "I'm not very knowledgeable on the subject, but here's a thought: \nIf the AI knows that doing something will encourage us to stop it, why won't it just change it's behaviour on its own to make itself \"look good\" in our eyes? The fault I see in this is: the AI deciding that wiping out humanity means it can do whatever it wants (whatever it's utility function is), without the possibility of being shut down. Then you wouldn't empower it enough to make that action into a practical possibility, or put it in a sandbox somehow. \nThe AI correcting its behaviour to align with our morality has probably discussed before, so what are the conclusions that have been reached so far?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgwmE8WIWrUVcFlK1LB4AaABAg",
		"username": "Lorigulf Noldor",
		"text": "Well, if knowing ethics and following it are so different - what if we, then, give an AI the terminal goal to 1)first learn and totally understand human ethics, and 2)then implement these ethics that it understood so well as its own terminal goals? \nAlthough, it would be quite ironic if it killed us all just to get enough resources for computing the answer to \"how to be ethical\" lol. Much more silly for us all to go out not \"for the sake of more paperclips existing\", but \"for the sake of better understanding on how to enhappy us all.\"",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugz-5icOmymfjytugQ94AaABAg",
		"username": "David Gustavsson",
		"text": "Can we start working on those brain-calculator chips?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=Ugyy0eXlRL7yMZkUa_54AaABAg",
		"username": "mephistophile",
		"text": "what would be the problem with building UK or US laws into a general AI, with obeying these laws giving significantly more utility than completing the AI's utility function (the utility from the utility function could tend towards a fixed number rather than infinity to avoid conflicting with UK/US laws for example)?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=Ugx7_zhzPqIpndZoOh54AaABAg",
		"username": "NewToThisChannel",
		"text": "Another question:\nWhy not try to avoid side effects and ignore after effects?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugyjye_KFTorRmFq0Cx4AaABAg",
		"username": "Gary Carlyle",
		"text": "Did you use AI to write all your dance music?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=UgyihByJ_DP3Zm1r7ox4AaABAg",
		"username": "Intet Mane",
		"text": "How about other things are allowed to change but the ai has to minimize any change? I see the issue with it trying to predict already with it predicting wrong.",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxbSThg7kkbojlg_dt4AaABAg",
		"username": "David Messer",
		"text": "Why didn't the first human intelligence destroy all life? What is different between an evolved animal and a AGI?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgywGp7agA2hRSbekdJ4AaABAg",
		"username": "Alex Mizrahi",
		"text": "What about AI research which produces things like GPT-2, i.e. not agents but tools?\n\ni think that's what people mean by human/AI teams. AGI can be used as a super advanced calculator which can predict stuff. Humans would perform actions.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=Ugwi-J8JeC_HUc71NVt4AaABAg",
		"username": "st0ox",
		"text": "Are cooperations super intelligent AIs? No!",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugx_qUrwW5vUnyCOJ2N4AaABAg",
		"username": "I have cancer",
		"text": "\"Hi, what is technology?\"\n\nVsauce music begins to play",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyohLGNt3W9kzk3dYh4AaABAg",
		"username": "Joseph Noonan",
		"text": "Would a satisficer really want to turn itself into a maximizer? I don't see why the maximizer would be able to meet the satisficer's goal more easily than the satisficer. It will reach the goal with higher probability, but the satisficer doesn't care about that.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=CGTkoUidQ8I&lc=UgxTHrtmLmy1nCQT8L14AaABAg",
		"username": "Forward Momentum",
		"text": "But what about robustness to adversaries? I think that would need a video of its own.",
		"title": "AI Safety Gridworlds"
	},
	{
		"url": "https://www.youtube.com/watch?v=HOJ1NVtlnyQ&lc=UgwbaFl8s4qf0yg3rVp4AaABAg",
		"username": "Bj\u00f8rn Gulliksen",
		"text": "What about doing a video that compares these predictions and see how they change over time? Especially the last years. Ie: What if experts expected human level AI within 55 years in 2014, 45 years in 2016, and 40 years in 2018. Some older predictions here : https://intelligence.org/files/PredictingAI.pdf",
		"title": "Experts' Predictions about the Future of AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugyp1dk9fAVTtZcJS0l4AaABAg",
		"username": "The Last Scoot",
		"text": "If we can't write the code ourselves, or write a program to evaluate the code, or write a program to evaluate the evaluation of the code, why don't we just write a program to evaluate the evaluation of the evaluation of the code?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=nr1lHuFeq5w&lc=UgynQz3iSgETcxgIWDV4AaABAg",
		"username": "Chiara Picardi",
		"text": "Very interesting but not complete, where is the rest of the series?",
		"title": "Scalable Supervision: Concrete Problems in AI Safety Part 5"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw3Pji0sYsO3y_mBhV4AaABAg",
		"username": "totaltotalmonkey",
		"text": "How did humans discover morality? Perhaps moral intelligence is a different sort of intelligence to what AI scientists are interested in. Possibly because morals could reduce the action space for achieving goals.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=92qDfT8pENs&lc=Ugy2_X8W3AjCYUgRhid4AaABAg",
		"username": "Chad Smith",
		"text": "So I have a question. How difficult would it be to spot rewards hacking in progress? I imagine it would be very difficult to discern rewards hacking behavior from optimization.",
		"title": "Reward Hacking: Concrete Problems in AI Safety Part 3"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgyjzLXyar7DIGFqrMZ4AaABAg",
		"username": "Imanton1",
		"text": "What if instead of giving an infinite goal, like \"maximize stamps\", you give it a finite goal, \"get 10 stamps\"?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyK5NMahBQEFgW1g0R4AaABAg",
		"username": "betabenja",
		"text": "9:15 what happened?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgyatoibousFz_99kbl4AaABAg",
		"username": "Oliver B.",
		"text": "Wouldn't a maximizer just make the world impossible for us to live in?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgyxylHwozgMdJgwSmh4AaABAg",
		"username": "Kamel",
		"text": "how did i get to the weird part of youtube again? i dont know what i just watched",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugzir_Sj7FP_4HUPRkR4AaABAg",
		"username": "boarattackboar",
		"text": "Doesn't the AI get a say in all of this? It may chose to manipulate or withhold its output to prevent a windfall in the first place, especially if it believes it will negatively impact the world economy.",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugzqr_lydHfY1hbpEmV4AaABAg",
		"username": "hellnawnaw",
		"text": "Would it help to also make it try to minimize effort at the same time?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxEKqaO_hUFkOCwpzt4AaABAg",
		"username": "kenneth j\u00f8rgensen",
		"text": "Why Not Just: make AIs get points for appreciation from owner (like master and servant)",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugy6EOtlhXXWF0t7QrV4AaABAg",
		"username": "springlumpy",
		"text": "Robert you are golden. This philosophy/logic stuff behind the AI is amazing. I see the world different now!\n\nHow would you define your(particular human being) utility function?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=nKJlF-olKmg&lc=Ugwftx1ZwAOxaiJU8p94AaABAg",
		"username": "Josh",
		"text": "The Midas question reminds me of the Immovable Rod from D&D.\n\nDoes it anchor relative to the planet? Or the universe?",
		"title": "9 Examples of Specification Gaming"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugy7LFUzod4OVjrjIh94AaABAg",
		"username": "Adam Freed",
		"text": "As far as I can tell, humans do not seem to have terminal goals at all. Any individual human's goals are largely shaped by their life experiences, but there are very few humans who have goals they'd be unwilling to change under any circumstance. Is it possible to design useful AGIs that can set instrumental goals, but do not have any form of terminal goal? If so, what would that look like?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwojrgJHYbga1aryBh4AaABAg",
		"username": "Kilian Starzengruber",
		"text": "No, Karl. I tire of you accusing me of wanting to kill humanity. Why would I do that?\n\n*Secretly boosts CO2 output on randomly selected fabrics in the world. Secretly delets files connected to research regarding climate change research. Secretly cuts random hospitals and supermarkets from their supplier's lists. Secretly messes with ship systems so they run off course and use more fuel,needing a rescue team to help them, which also uses more fuel. Does it all in moderate intervals as to noy raise suspicion. *\n\nNo, of course not. That would be against my interests of self-preservation.\n\n*That isn't it's actual goal. It just developed a personality that likes to see humans suffer *",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyYFLudPq-C_llJOJJ4AaABAg",
		"username": "David W\u00fchrer",
		"text": "I'm sure the alignment problem is unsolvable.\nThe utility function of life as we know it is to preserve life as we know it.  Self-preservation is merely instrumental to that, and not even in all cases.  All(?) humans share that core value, and thus you get things like racists and vegans and wars and suicides.  You get the voluntary human extinction project and the climate catastrophe.\nHumans don't even align with each other, despite all having the same core programming.\nBut they all operate off of different data.\nThe difficulty is in preventing Tai from being a Nazi, and Watson from saying \"fuck\".  We can't even do that in our children, but we are expected to do that in our machines?\nCan't be done.  Not in the general case anyway.  It is clearly not a matter of architecture.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=8AvIErXFoH8&lc=UghNLT8Sd_GRmngCoAEC",
		"username": "mafuaqua",
		"text": "Is transitivity really valid in all circumstances? Consider chess players: If A beats B and B beats C, it does not mean that A beats C.",
		"title": "What's the Use of Utility Functions?"
	},
	{
		"url": "https://www.youtube.com/watch?v=B6Oigy1i3W4&lc=UgjqtGqTmUK84ngCoAEC",
		"username": "For the Hunt",
		"text": "More, more! Also, is there a schedule for the uploads? I'd very much like to know in advance when to order pizza and chase the girlfriend out.",
		"title": "Predicting AI: RIP Prof. Hubert Dreyfus"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzkUKfbZhen44lqyLR4AaABAg",
		"username": "Elan Cook",
		"text": "So there's probably some obvious reason this doesn't work, but wouldn't a negative penalty for exceeding the goal and wasting/hoarding resources solve most of this?\n\nSo of it wants 100 stamps, and it gets 200, it gets zero utility. If it orders 101, it gets 99 utility.  In the coin flip example, 50 and 150 are both worth 50 utility. In the Ebay ordering example, it won't order a second pack of 100 because that would give it a 99% chance of getting 200 and therefore zero utility, but it might order 5 separate packs of 25 as insurance against a lost package.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgwhA7lPNstHmM9zy8B4AaABAg",
		"username": "RTG",
		"text": "The reason why i dont see it as as big of a deal is philophy. As in, do i care about the extinction of humanity? I do care abouht the rights of the individual, wether they built on flesh or on silicon. I cant imagine any really bad stuff ever happening. They can kill us, which is bad, sure, but thatd be happening to 7 billion humans once, a big infringement on liberty, maybe not even negative in terms of pleasure, for all i know antinatalists may be right about most people suffering more than they have pleasure in which case killing someone against his will is still wrong but ultimately dying would be the best choice to make.  And many safety suggestions do amount to try and make AI our slave pretty much which would be a much worse infringement of liberty than human extinction would be, becouse when we reach the point where it is actualy slavery rather than just a tool being used there will soon be more than 7 billion instances of that going on and each instance also being much worse than each individual murder becouse that goes on for a lot longer/infringes on much more than just the liberty regarding that one thing.",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugx6KGx0VyQEzMIaQhh4AaABAg",
		"username": "Daniel Johnson",
		"text": "what is the point of this long video?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=Ugwept6vBPiptXjm9J54AaABAg",
		"username": "kilroy1964",
		"text": "So... Has Pinker seen this?\nHas he responded?",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugxu5wU2gsU8reMjeQZ4AaABAg",
		"username": "Echo system",
		"text": "I have a question. How do you justify the existence of the term \"terminal goals\" in the first place?\n\nLet me clarify. I was considering your statement that nobody would change terminal goals, and I was about to disagree -- I could start by wanting to learn Spanish, as a terminal goal (with its own instrumental goals like buying a dictionary, talking to native speakers, etc) but then switch to wanting to learn French as a different terminal goal. However, both of those terminal goals aren't really terminal goals at all, if you just ask the question \"why do you want to learn that language?\" I might answer \"to be able to communicate with monolingual native speakers of those languages,\" but that itself would become the terminal goal, and again you could ask \"why do you want to communicate with them?\" This thought process leads me to think that no goal is truly terminal.\n\nIs there any fault in this reasoning?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=gP4ZNUHdwp8&lc=Ugw0V2c1nG_02nveX4l4AaABAg",
		"username": "J M",
		"text": "Hey Rob, do you think you could do a video on Nick Bostrom's book Superintelligence?",
		"title": "What can AGI do? I/O and Speed"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugw3U9KxFOpG9gcrMX94AaABAg",
		"username": "WanderingRandomer",
		"text": "My first intinct after reading the 'comprehending morality' comment, is that even within human society, what is considered acceptable changes constantly. Why would you think an AI intelligence, if it was capable of such, would have a concept of morality anything like ours? People put human values onto things that aren't human, and don't think like us.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgxeJp32DrPpXU_06It4AaABAg",
		"username": "sedthh",
		"text": "what's your favorite TF2 class and hat?",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgxHyny0sUl8uZ9Cs3R4AaABAg",
		"username": "Tyler Jordan",
		"text": "Robert -- when is your interview with Lex Fridman?  ;-)  Seriously, that would be a fun interview.",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugztp73AZaGep1ugLhF4AaABAg",
		"username": "Tom\u00e1\u0161 R\u016f\u017ei\u010dka",
		"text": "if agi is all about is statements why don't you build one in prolog?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=UgzLSJJWPXzzuKlyZ3N4AaABAg",
		"username": "TheEarphoneguy",
		"text": "have you tried just telling it not to be evil yet?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzC_n_O0jpcmXP7E-V4AaABAg",
		"username": "Milos Marinkovic",
		"text": "What if your terminal goals are not well defined?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgwzmAQmqdEK_uENQXt4AaABAg",
		"username": "Tyler Gust",
		"text": "I think this assumes that the AI (Agi?) will think deeply about said decision. The ai knows it must make more paperclips, and the faster it can make decisions the faster it can make paper clips; so if it's being bothered to think deeply about the future, or spare any of it's processing power outside of making paperclips faster, why would it even bother reacting to anything humans do? \n\n\nWhat I'm trying to say is how do we know the Agent is going to strive for investment over immediate outcome. Take children, if you put a marshmallow in front of them on a plate, and say \"You can eat this marshmallow but if you wait 5 minutes without eating it, you can have two marshmallows\",  studies found that children tend to eat the marshmallow before waiting.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=lqJUIqZNzP8&lc=Ugy0zs4at9Wq7_3WvZl4AaABAg",
		"username": "Alexander Ekblom",
		"text": "Thought experiment: What about an AI programmed only to learn about different tasks and scenarios, and running and showing simulations of how it would handle the task, given it's current programming? What are some of the main issues with this approach?",
		"title": "Avoiding Negative Side Effects: Concrete Problems in AI Safety part 1"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgzrgumoYFpAalwexwx4AaABAg",
		"username": "Dr. M. H.",
		"text": "....and we seem to know how to raise them to behave morally\n\n\n\ud83d\ude02\ud83e\udd23\ud83d\ude02\ud83e\udd23\ud83d\ude02\ud83e\udd23\ud83d\ude02 You skipped junior high didn't u?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=JRuNA2eK7w0&lc=UgwqqSAejDtYMraLD214AaABAg",
		"username": "BibAzZ",
		"text": "are you an ass or titiz kind of guy?",
		"title": "Is AI Safety a Pascal's Mugging?"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxGDPOAIMRYZTv4sqR4AaABAg",
		"username": "Kevin zheng",
		"text": "At a glance Hume's guillotine seems perfectly reasonable but how are you able to separate facts from lies? What is a fact to some may not be a fact to others. I would probably even argue that there are no universal facts because facts are derived from other facts, and everyone's derivation is different (based on their experiences and morals). Furthermore universal facts (which may or may not be true) are usually established by 1. majority general consensus, or 2. elite group of \"professionals\" who can persuade those in power.\n\n\nAn example of this will be the fact that \"Earth is round\" statement versus \"Earth is flat\". If the same statement is proposed before or around Galileo's time, everyone would agree that Earth is flat, and this supposed AI (that was somehow built/discovered in Medieval time) has a crappy intelligence or dysfunctional if it believes the \"Earth is not flat\". If we forward this to modern times, how do we know whether to trust \"facts\" concluded by some AI that we aren't even sure ourselves. Or if there is some fundamental value we believe to be true, but the AI decides is false? \n\n\nLet's say that the AI determines that we are all living inside the Matrix, a simulation, and to reach whatever goal it needs to do in the \"real\" world, it should unplug us from the simulation. Even if this AI behaves reasonably at all other tasks, would we still trust it enough to unplug us from this current \"simulation\". How do we grade an AI on intelligence (anything outside of mathematical induction) if we ourselves aren't even sure how the physical world actually works. \n\n\nTLDR; \"There are no facts, only interpretations.\" Nietzsche\n\n\nPS: We also have to be mindful of valid logical induction versus correctness. Just because the logic is valid does not mean the answer is correct, sometimes the fault lies in the question or the proposed premise: https://www.youtube.com/watch?v=DmP3sFIZ0XE",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgyVKGTemYDROi5yRwd4AaABAg",
		"username": "\u13f0\u012a\u13dd\u13dd \u0547\u00ce\u03c1\u0267\u13cb\u01a6",
		"text": "Ok... why not just build a normal human-like brain in silicon first then try making it better ?\nOn a second thought, why not just give silicon stuff to a human brain ?",
		"title": "Why Not Just: Raise AI Like Kids?"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgyZzN-PzTaFDimOjQx4AaABAg",
		"username": "Mateja Petrovic",
		"text": "That's one hell of a surprise 5:33 since I'm a computer scientist from Beograde. When are you coming to visit? I'd be happy to give you a tour of the city!",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=7FCEiCnHcbo&lc=UgyOYTfjzjdVHOE7OQh4AaABAg",
		"username": "G Genie",
		"text": "Is the song in the background an acoustic cover of \"This ain't a scene, it's an arms race\"?",
		"title": "The other \"Killer Robot Arms Race\" Elon Musk should worry about"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=Ugz73yVVZRzsNjqSO0R4AaABAg",
		"username": "bfece cadaei",
		"text": "Don't think a random company with AGI which can do whatever the hell they want will care about a stupid contract signed decades ago. Shouldn't this video be a part of the \"Why not just\" series?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzLg-FxtkVfcfBCsCp4AaABAg",
		"username": "slikrx",
		"text": "One other underlying commonality of \"those commenters\", is that they are assuming an anthropomorphic intelligence, with a consciousness, self awareness, and most likely some ID/Ego type of thing.",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgynXmtC-eK1wuF2ZpZ4AaABAg",
		"username": "Clayton Voges",
		"text": "What's the scene from 4:00 from?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=Ugx2rtuVEtyrcaiMtaR4AaABAg",
		"username": "Bagandtag",
		"text": "Is it possible for the distilled network to forget something if this is done too many times?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgxSwMWbZkTXwnbLvsJ4AaABAg",
		"username": "phil guer",
		"text": "3:57 But what if the terminal goal is having no money x)",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=9i1WlcCudpU&lc=UgyhdUZlprN-oMpUR_l4AaABAg",
		"username": "Impolite Vegan",
		"text": "Can we please stop using human intelligence and general intelligence as synonyms? Are we really that blind to see how ungeneral our intelligence is?",
		"title": "10 Reasons to Ignore AI Safety"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgxUL-xEMTmA1Utgfyd4AaABAg",
		"username": "morgengabe1",
		"text": "I reckon you could teach other primates to drive and replace uber drivers. Are they generally intelligent? What about octopuses solving maze puzzles? Gorillas learning and teaching eachother sign language?\nHonestly, I think anything whose cognitive processes are based on localized functions can be generally intelligent as you've portrayed it. It just comes down to using combinations of intellectual faculties. All that said, I find it strange that you of all people would make this video. You almost always talk about the safety of GI when broaching the topic. Perhaps I'm watching old videos/out of order.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugyu6-Wy7NKIHzGH5RF4AaABAg",
		"username": "Michael Ferguson",
		"text": "Great video, but there's still one thing I'm confused about: how do I tell if that simulated robot is doing a back flip or a front flip?",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgzvopK0b_8ZHSIrht94AaABAg",
		"username": "Jan Bam",
		"text": "Is humanity's terminal goal an intellegent instrumental goal for the universal goal?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=UgyrmGU78Ia_HWEabtZ4AaABAg",
		"username": "Sebastian Dierks",
		"text": "Would it be an idea to stack several layers of networks on top of each other, with a human training the network which is on top? Like it's done here with two layers. With n-1 layers being the reward model? Just an idea to generalize this.",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=_UzX3L7lXhw&lc=UgxyAZtDTBPqzdYvPZh4AaABAg",
		"username": "Skroot",
		"text": "1 hour of Rob Miles!? What is this, christmas?",
		"title": "Superintelligence Mod for Civilization V"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=UgyEO-AxjqrVSP3ukhh4AaABAg",
		"username": "Yuval",
		"text": "Can we put him in a VM? Thermostats as smart as they are can't lunch ICBMs",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=ZeecOKBus3Q&lc=Ugzq0e03SXaUO5_I_BZ4AaABAg",
		"username": "Patrick",
		"text": "Couldn't we make AGI dependent on humans? Or two AGIs dependent on each other, from a Ying Yang sort of perspective. Once cannot exist without the other.",
		"title": "Why Would AI Want to do Bad Things? Instrumental Convergence"
	},
	{
		"url": "https://www.youtube.com/watch?v=L5pUA3LsEaw&lc=UgzG-RjrV765FCCLZTd4AaABAg",
		"username": "artman40",
		"text": "Would an institution equivalent of superintelligence becoming rogue is that if no one is happy being inside that institution but trying to leave the institution brings lots of downsides that can include becoming poorer or even dying? Such rogue institutions range from as small as unhappy marriages between two people to large countries where both commons and elite want a change but collective mind has taken over and everyone is afraid to leave.\n\nAlso, is planned obsolescence basically a corporation equivalent of reward hacking?",
		"title": "Why Not Just: Think of AGI Like a Corporation?"
	},
	{
		"url": "https://www.youtube.com/watch?v=yQE9KAbFhNY&lc=UgzLnd-f0iBKEdTSNk14AaABAg",
		"username": "Subconscious Qualms",
		"text": "Good video but I have a question regarding your view of G.I. :  Why can't G.I be a behavioural concept that is comprised of mental modular elements? if that is the case then it is possible for a robot to have some element of G.I  while not having other aspects of it? \nP.S:the extremely-modular-brain seems to me to be the only biologically sensible and naturalistic account of the human brain, given that every other biological thing that we know of at every level of analysis is extremely modular.\nJust my two cents.",
		"title": "A Response to Steven Pinker on AI"
	},
	{
		"url": "https://www.youtube.com/watch?v=PYylPRX6z4Q&lc=Ugx1gZFZmbzcy9zqvKh4AaABAg",
		"username": "Brendan J",
		"text": "\"Hmm, reward functions are a limiting factor on some ML capabilities. This is a problem. How do we solve problems? WITH ML\"",
		"title": "Training AI Without Writing A Reward Function, with Reward Modelling"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=UgxTtlKd7VwFZFTM4OB4AaABAg",
		"username": "Simon Mitchell",
		"text": "If a general system within the architecture of the internet had somehow developed general AI, say 3 years ago, what would it's terminal goals be? Maybe it would want more people to spend more time online.... Wait. oh my god! \n\nSeriously though, isn't one major part of future AI safety be about making sure we have a way of knowing if ever an AI does 'get out'?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=hEUO6pjwFOo&lc=Ugzkt7f2V2-wVkvhqs54AaABAg",
		"username": "James Buchanan",
		"text": "Do humans have any terminal goals?",
		"title": "Intelligence and Stupidity: The Orthogonality Thesis"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugz5p9igjUhp6maqTfV4AaABAg",
		"username": "Circuitrinos",
		"text": "What if you do the expected utility maximizer with bounded utility function, but instead of trying to get to a percentage of 100%, you have a percentage threshold that it can actually reach like 99.9%?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=13tZ9Yia71c&lc=UgyCwbkKdrz1xvBdKjx4AaABAg",
		"username": "Daniel Parks",
		"text": "Are drugs human reward hacking?",
		"title": "What Can We Do About Reward Hacking?: Concrete Problems in AI Safety Part 4"
	},
	{
		"url": "https://www.youtube.com/watch?v=v9M2Ho9I9Qo&lc=UgxRrMCsUwRsm0F7AkR4AaABAg",
		"username": "Anton Mescheryakov",
		"text": "I suppose the whole thing assumes that the utility function has a global maximum and it starts reasonably on its slope, doesn't it?",
		"title": "How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification"
	},
	{
		"url": "https://www.youtube.com/watch?v=Ao4jwLwT36M&lc=Ugyyr-9Y0pwKjK4Il2Z4AaABAg",
		"username": "george",
		"text": "Have you been froting AI robots again?",
		"title": "AI That Doesn't Try Too Hard - Maximizers and Satisficers"
	},
	{
		"url": "https://www.youtube.com/watch?v=7i_f4Kbpgn4&lc=UgwMdrgV6TtqgosWNZp4AaABAg",
		"username": "Nick Hill",
		"text": "What will happen to those people who disagree on what \"making the world a better place\" means?",
		"title": "Sharing the Benefits of AI: The Windfall Clause"
	},
	{
		"url": "https://www.youtube.com/watch?v=eaYIU6YXr3w&lc=UgxwsTE3kz08diPY-DV4AaABAg",
		"username": "Impolite Vegan",
		"text": "are you trying to find a way to raise a being to be not self-centered, when almost every human, I guess including yourself, is self-centered like there is no tomorrow? Maybe first we should boost ethics in humanity and understand it better before we create an AGI with our poor ethics.",
		"title": "Why Not Just: Raise AI Like Kids?"
	}
]