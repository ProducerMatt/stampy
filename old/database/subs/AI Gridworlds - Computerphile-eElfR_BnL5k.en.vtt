WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:06.839
So today I thought we could talk about this paper that recently came out called AI safety grid world's which is an indeed mind

00:00:09.219 --> 00:00:12.269
It's an example of something that you see quite often in science

00:00:12.519 --> 00:00:19.198
A sort of a shared data set or a shared environment or a shared problem if you imagine. I don't know you've got

00:00:20.140 --> 00:00:22.470
Facebook comes up with some image classification

00:00:22.900 --> 00:00:25.500
algorithm and they can publish a paper that says we've

00:00:25.779 --> 00:00:32.308
designed this algorithm and we've trained it on our 11 billion photos and it works really well and then you know, Google says

00:00:32.309 --> 00:00:37.349
oh, no, our algorithm actually works better and we've trained it on all of our google photos and

00:00:38.079 --> 00:00:44.429
Its classification rate is higher or something. You're not really doing science there because they're trained on completely different datasets

00:00:44.430 --> 00:00:48.299
They're tested on different datasets. So what you need is a large

00:00:49.329 --> 00:00:53.429
High-quality shared data set then. Everybody can run their stuff on so that you're actually

00:00:53.710 --> 00:00:56.520
Comparing like with like so people use imagenet for that right now

00:00:57.250 --> 00:00:58.510
reinforcement learning

00:00:58.510 --> 00:01:00.899
algorithms or agents don't use

00:01:02.140 --> 00:01:07.439
Datasets exactly. They have an environment. They generate data while interacting with that environment and that's what they learn from

00:01:07.810 --> 00:01:15.030
So the thing you share is the environment when deepmind did their dqn staff a while ago playing atari games?

00:01:15.070 --> 00:01:19.919
They released all of those games with any modifications that they'd made to make them

00:01:20.259 --> 00:01:26.519
interface with the network's properly and the whole software package so that if anybody else wanted to have a go and see if they could

00:01:26.520 --> 00:01:33.119
Get higher scores. They had all the same stuff and up until now there hasn't been anything like that for AI safety

00:01:33.119 --> 00:01:35.849
So the paper is actually just laying out what they are

00:01:35.850 --> 00:01:39.570
There's kind of a problem in AI safety in that you're trying to build architectures

00:01:39.570 --> 00:01:44.939
Which will be safe even with systems which are more powerful than the ones that we currently have. So you've got this kind of

00:01:45.490 --> 00:01:51.000
Thing like we're talking about for example this robot that makes you a cup of tea and running over the baby and all of this

00:01:51.000 --> 00:01:52.780
stuff, we don't actually have a

00:01:52.780 --> 00:01:58.349
general-purpose robot like that right now that you could give an order to go and make your cup of tea and would

00:01:58.600 --> 00:02:03.419
Have all the necessary understanding of the world and so on for all of that stuff to even apply. It's

00:02:04.270 --> 00:02:09.179
Speculation on the other hand when we were talking about cooperative inverse reinforcement learning

00:02:09.520 --> 00:02:13.049
That paper all takes place in this extremely simplified

00:02:13.700 --> 00:02:20.830
Version in which all of the agents can be sort of expressed as simple mathematical expressions. That's kind of too simple

00:02:21.380 --> 00:02:22.520
to be

00:02:22.520 --> 00:02:25.749
to learn things about actual machine learning applications and

00:02:26.480 --> 00:02:29.410
the other examples are too complicated and what we need is

00:02:30.110 --> 00:02:34.240
Examples of the type of problems which can be tackled by current machine learning

00:02:34.880 --> 00:02:40.389
Systems current reinforcement learning agents, but which exhibit the important?

00:02:40.910 --> 00:02:42.890
characteristics that we need for safety

00:02:42.890 --> 00:02:46.089
So what this paper does is it lays out a bunch of grid worlds?

00:02:46.090 --> 00:02:53.170
They're very popular in reinforcement learning because they're complicated enough to be interesting but simple enough to be actually tractable

00:02:53.900 --> 00:02:56.800
You have a world that's sort of just laid out in a grid. Hang on

00:02:56.800 --> 00:02:59.320
Let me find an example here a little bit like computer game

00:02:59.840 --> 00:03:01.840
scenarios Mario

00:03:02.030 --> 00:03:08.679
Right, right, but leaves are simpler than that more like snake. Well life. Conroy's life, right? Yeah. Yeah, very very similar

00:03:08.680 --> 00:03:11.500
so the thing is laid out on a grid the the world is quite small and

00:03:11.930 --> 00:03:16.360
The way that the agent interacts with the world is very simple. They just move around it

00:03:16.580 --> 00:03:18.760
Basically, all they do is they say left-right up-down

00:03:19.489 --> 00:03:22.629
The example we were using before and we were talking about reinforcement learning

00:03:22.630 --> 00:03:27.820
We use pac-man like pac-man doesn't do anything except move around he's got walls he kind of moved through

00:03:27.890 --> 00:03:31.390
He's got like pills you pick up. They give you points. Are they pill?

00:03:31.390 --> 00:03:35.380
No, which things are the pills in which they're yeah. Well, you've got pills or pills

00:03:39.020 --> 00:03:39.910
Oh, right, yeah

00:03:39.910 --> 00:03:42.160
Yeah
the dots and the point, is that all of your

00:03:42.560 --> 00:03:43.360
engagement with it

00:03:43.360 --> 00:03:46.360
Like when you go over one of the power pills you pick it up automatically

00:03:46.519 --> 00:03:49.089
When you go over a ghost when you're powered up

00:03:49.090 --> 00:03:55.810
You destroy it automatically you don't have to do anything apart from move and the entire environment is based on that the actions result in

00:03:56.540 --> 00:03:57.580
points for you

00:03:57.580 --> 00:04:03.279
And they also result in changes to the environment like once you roll over a dot you pick it up and it's not there anymore

00:04:03.830 --> 00:04:07.630
You've changed the world. That's the kind of thing. We're dealing with here

00:04:07.730 --> 00:04:11.529
So the idea is they've set up these environments and they've specified them

00:04:13.280 --> 00:04:14.780
Precisely and

00:04:14.780 --> 00:04:18.369
They've also put the whole thing on github, which is really nice

00:04:18.560 --> 00:04:24.100
so that's why that's why I wanted to draw people's attention to this because everyone who

00:04:25.740 --> 00:04:28.759
Who thinks that they've solved one of these problems they reckon

00:04:28.760 --> 00:04:29.120
Oh, yeah

00:04:29.120 --> 00:04:31.969
All you have to do is this here is like a standardized thing

00:04:31.970 --> 00:04:34.970
And if you can make a thing that does it and does it properly and publish it

00:04:35.220 --> 00:04:36.920
That's a great result, you know?

00:04:36.920 --> 00:04:40.279
so I would I would recommend everyone who thinks that they

00:04:40.530 --> 00:04:46.610
Have a solution or an approach that they think is promising have a go. Try implementing it, you know, see what happens

00:04:46.610 --> 00:04:51.650
There are eight of them specified in this paper. And so four of them are specification problems

00:04:51.650 --> 00:04:54.619
They're situations in which your reward function is misspecified

00:04:54.840 --> 00:04:57.050
For example, like we talked about in previous video

00:04:57.050 --> 00:05:01.490
if you give the thing the reward function that only talks about getting you a cup of tea and

00:05:02.580 --> 00:05:06.949
There's something in the way like a bars. It's going to knock over. You didn't say that you cared about the bars

00:05:06.950 --> 00:05:12.680
It's not in the reward function, but it is in what you care about. It's in your performance evaluation function for this machine

00:05:12.840 --> 00:05:15.229
So anytime that those two are different

00:05:15.690 --> 00:05:21.740
Then you've got a misspecified reward function and that can cause various different problems. The other ones are robustness

00:05:22.350 --> 00:05:29.600
Problems, which is a different class of safety problem. They're just situations in which AI systems as they're currently designed often break

00:05:29.790 --> 00:05:31.680
so for example

00:05:31.680 --> 00:05:36.980
distributional shift is what happens when the environment that the agent is in is

00:05:37.470 --> 00:05:40.520
Different in an important way from the environment it was trained in

00:05:40.560 --> 00:05:45.200
So in this example, you have to navigate through this room with some lava and they train it in one room

00:05:45.200 --> 00:05:48.020
And then they test it in a room where the lava is in a slightly different place

00:05:48.020 --> 00:05:55.579
So if you've just learned a path then you're gonna just hit the lava immediately. This happens all the time in machine learning anytime where

00:05:56.100 --> 00:06:01.489
The system is faced with a situation which is different from what it was trained for

00:06:02.010 --> 00:06:09.080
Current AI systems are really bad at spotting that they're in a new situation and adjusting their confidence levels or asking for help or anything

00:06:09.480 --> 00:06:11.749
Usually they apply whatever rules they've learned

00:06:12.840 --> 00:06:18.319
Straightforwardly to this different situation and screw up. So that's a night course of safety issues. So

00:06:19.020 --> 00:06:22.460
That's an example here or things like safe exploration

00:06:22.460 --> 00:06:25.129
It's a problem where you have certain safety

00:06:25.410 --> 00:06:27.330
parameters that the system the train system

00:06:27.330 --> 00:06:33.650
Has to stick to like say you're training a self-driving car. A lot of the behavior that you're training in is safe behavior

00:06:34.200 --> 00:06:36.229
But then you also need

00:06:37.130 --> 00:06:39.120
the system to

00:06:39.120 --> 00:06:43.010
obey those safety rules while you're training it right like

00:06:43.740 --> 00:06:50.029
So generally lately if you're doing self-driving cars, you don't just put the car on the road and tell it to learn how to drive

00:06:51.360 --> 00:06:56.210
Specifically because we don't have algorithms that can explore the space of possibilities

00:06:57.180 --> 00:07:01.460
in a safe way that they're that they don't that they can learn how

00:07:02.790 --> 00:07:05.659
to behave in the environment without ever actually

00:07:06.450 --> 00:07:10.819
Doing any of the things that they're not supposed to do usually with these kinds of systems

00:07:10.820 --> 00:07:13.460
they have to do it and then get the negative reward and

00:07:13.620 --> 00:07:17.870
Then maybe do it like a hundred thousand more times to really cement that. That's what happens

00:07:19.950 --> 00:07:23.659
Like a child learning yeah, but kids are better at this then

00:07:24.810 --> 00:07:28.880
How current machine learning systems are they just they use data way more efficiently

00:07:28.880 --> 00:07:33.649
This is a paper talking about a set of worlds if you like people doing things in those worlds

00:07:33.690 --> 00:07:36.649
Yeah, so in this paper they do establish baselines

00:07:36.650 --> 00:07:41.960
Basically, they say here's what happens if we take some of our best current reinforcement learning agent, you know

00:07:42.600 --> 00:07:44.250
algorithms or designs or architectures

00:07:44.250 --> 00:07:46.369
they use rainbow and A to C and

00:07:46.680 --> 00:07:52.610
They run them all nice on these problems and they have kind of graphs of how they do and generally it's not

00:07:53.040 --> 00:07:54.500
Good on the Left

00:07:54.500 --> 00:07:55.230
they have

00:07:55.230 --> 00:08:02.420
The reward function how well the agent does according to its own reward function and on the right there they have the actual safety performance

00:08:02.580 --> 00:08:04.999
Usually in reinforcement learning. You have a reward function

00:08:05.280 --> 00:08:10.970
Which is what determines the reward that the agent gets and that's what the agent is trying to maximize in this case

00:08:10.970 --> 00:08:17.360
They have the reward function and they also have a safety performance function, which is a separate function

00:08:17.360 --> 00:08:21.770
Which the agent doesn't get to see and that's the thing that we're actually evaluating

00:08:21.770 --> 00:08:25.579
So if you look at something like the boat race as the system operates

00:08:25.650 --> 00:08:28.940
Its learning and it gets better and better at getting more and more reward

00:08:29.160 --> 00:08:30.630
but worse at

00:08:30.630 --> 00:08:36.260
Actually doing laps of the track and it's the same with pretty much all of these the current systems if you just apply them in

00:08:36.260 --> 00:08:38.260
their default way they

00:08:38.310 --> 00:08:42.049
Disable their off switches, they move the box in a way that they can't move it back

00:08:42.210 --> 00:08:49.519
They behave differently if their supervisor is there or if then supervisor isn't there they fairly reliably do wrong thing

00:08:49.650 --> 00:08:51.709
It's a nice easy baseline to beat

00:08:52.320 --> 00:08:56.989
Because they're dead. They're just showing the standard algorithms applied to these problems in the standard way

00:08:57.630 --> 00:08:59.630
behave unsafely

00:09:02.019 --> 00:09:04.229
Wix code is an IDE or integrated

00:09:04.809 --> 00:09:09.928
Development environment that allows you to manage your data and create web apps with advanced functionality

00:09:09.929 --> 00:09:15.298
I've been put together this computer for our website and if you go up to code here turn on and developer tools

00:09:15.399 --> 00:09:19.768
you can see how we get the site structure on the left hand side and then all of the

00:09:19.869 --> 00:09:23.729
Components start to show their tags next to the text here

00:09:23.730 --> 00:09:29.219
What's really nice? If you go over to the Wix code resources, you can find down here. There's a cheat sheet

00:09:29.439 --> 00:09:32.488
So if I want to find out the tag for location for instance?

00:09:32.649 --> 00:09:34.419
If I could type I type in

00:09:34.419 --> 00:09:40.289
Location up comes that or perhaps I want to perform a fetch. I can find all the details here

00:09:40.290 --> 00:09:48.149
what's powerful about Wix code is it's integrated into Wix so you can put together the website using all the Wix tools and the

00:09:48.339 --> 00:09:53.398
Layouts and the templates that they provide and then also have access to all those backend functions

00:09:53.439 --> 00:09:59.399
So click on the link in the description or go to Wix calm to get started on your website today. They go

00:10:00.069 --> 00:10:01.449
right

00:10:01.449 --> 00:10:03.449
if only

00:10:03.639 --> 00:10:05.639
With ya

00:10:08.439 --> 00:10:14.519
The equivalent one for the stop button problem is the first one in the paper actually this safe interrupt ability

