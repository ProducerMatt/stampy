WEBVTT
Kind: captions
Language: en-GB

00:00:00.000 --> 00:00:05.400
The stamp collecting machine we talked about last time is a physical impossibility effectivly.

00:00:05.400 --> 00:00:09.240
Because it has a perfect model of reality, or an extremely good model or realtity,

00:00:09.240 --> 00:00:11.760
that we just gave it by specifying it.

00:00:11.840 --> 00:00:15.760
It looks through every possible sequence of output data within a year,

00:00:15.760 --> 00:00:19.240
which is far too large a search space to search exhaustively.

00:00:19.240 --> 00:00:22.800
And it is able to evaluate

00:00:22.800 --> 00:00:25.680
for each of this extremely huge search space

00:00:25.680 --> 00:00:28.600
a detailed, year-long simulation of reality

00:00:28.600 --> 00:00:29.960
to figure out how many stamps it gets.

00:00:29.960 --> 00:00:32.520
So clearly an actual computer to run this algorithm

00:00:32.560 --> 00:00:34.720
is significantly larger than the universe.

00:00:34.720 --> 00:00:36.880
So why is it even worth thinking about?

00:00:37.080 --> 00:00:37.580
Right?

00:00:38.460 --> 00:00:39.060
Umm..

00:00:39.300 --> 00:00:42.140
And the reason it's worth thinking about is...

00:00:42.460 --> 00:00:43.820
Self Improvement

00:00:46.740 --> 00:00:49.620
It is reasonable to expect

00:00:49.620 --> 00:00:53.100
that a general intelligence which is not this powerful

00:00:53.540 --> 00:00:55.820
would improve itself over time.

00:00:55.940 --> 00:00:58.420
and eventually become very powerful.

00:00:58.420 --> 00:00:59.820
Not as powerful as this,

00:00:59.820 --> 00:01:04.060
but closer to this than our intuitive understandings of intelligence.

00:01:04.540 --> 00:01:05.040
Umm

00:01:05.480 --> 00:01:07.680
And the reason for that is...

00:01:07.680 --> 00:01:11.240
that intelligence, in this context,

00:01:11.240 --> 00:01:14.320
is an instrumental value.

00:01:14.640 --> 00:01:16.640
Whatever you're trying to achieve,

00:01:16.640 --> 00:01:19.080
it's valuable to be more intelligent.

00:01:19.080 --> 00:01:21.920
If you sense that you're not intelligent enough

00:01:21.920 --> 00:01:24.440
to come up with the best possible stamp-collecting plan,

00:01:25.040 --> 00:01:27.760
the first part of your plan might be spent

00:01:27.760 --> 00:01:30.080
designing improvements to yourself

00:01:30.080 --> 00:01:31.840
to allow you to come up with a better plan.

00:01:31.840 --> 00:01:35.040
[Interviewer] Is the machine realizing it might need to modify itself

00:01:35.040 --> 00:01:37.680
a bit like somebody saying "I wanna be a doctor"

00:01:37.680 --> 00:01:39.600
so they decide to go off and get a medical degree?

00:01:39.600 --> 00:01:40.680
In a sense, yeah,

00:01:40.680 --> 00:01:44.680
although people are not really able to

00:01:44.680 --> 00:01:47.480
change our intelligence, our core intelligence.

00:01:47.480 --> 00:01:49.520
We can acquire more knowledge,

00:01:49.920 --> 00:01:50.420
Umm

00:01:50.420 --> 00:01:51.740
and we can acquire more skills,

00:01:51.740 --> 00:01:54.420
which is in a sense increasing your effective intelligence

00:01:54.740 --> 00:01:55.460
but...

00:01:55.940 --> 00:01:57.180
if you could have a...

00:01:57.780 --> 00:02:02.100
if you could take an action which would actually increase your ability to think,

00:02:02.100 --> 00:02:04.100
in all senses,

00:02:04.100 --> 00:02:04.600
uh,

00:02:04.600 --> 00:02:06.960
that action would be worth taking.

00:02:06.960 --> 00:02:08.960
even at fairly significant cost.

00:02:08.960 --> 00:02:12.240
And this is something which is true

00:02:12.240 --> 00:02:14.240
if you're trying to collect stamps,

00:02:14.240 --> 00:02:17.680
but also if you're trying to do almost anything else.

00:02:19.200 --> 00:02:20.920
Being better at modeling reality,

00:02:20.920 --> 00:02:23.560
being better at picking options,

00:02:23.560 --> 00:02:26.760
better at making plans, better at enacting those plans,

00:02:26.760 --> 00:02:28.840
Whatever your plan is, that's worth going for.

00:02:28.840 --> 00:02:31.040
And we may also have reason to believe

00:02:31.040 --> 00:02:34.640
that a general intelligence might...

00:02:34.640 --> 00:02:37.040
...be quite successful at improving itself.

00:02:38.720 --> 00:02:41.640
Human beings designed it,

00:02:41.640 --> 00:02:44.440
but it's quite easy,

00:02:44.440 --> 00:02:47.560
it's quite common for humans to design things

00:02:47.560 --> 00:02:50.840
that are better at what they do than humans are.

00:02:51.040 --> 00:02:51.540
Right?

00:02:51.620 --> 00:02:53.620
I am a very weak chess player.

00:02:53.620 --> 00:02:56.220
But I could, given some time and effort,

00:02:56.220 --> 00:02:59.180
write a computer that would beat me at chess.

00:02:59.180 --> 00:03:03.180
So it's perfectly plausible that we might write,

00:03:03.180 --> 00:03:07.620
we might design an AI that is actually better than us at AI design.

00:03:07.620 --> 00:03:11.140
So that it's able to make improvements to itself they that we haven't thought of.

00:03:11.140 --> 00:03:13.140
Machine self-improvement...

00:03:13.140 --> 00:03:15.340
We don't really have a sense of the time scale.

00:03:15.340 --> 00:03:17.340
And it could be extremely short.

00:03:17.340 --> 00:03:17.840
Right?

00:03:17.840 --> 00:03:22.400
If it's just a software change, you write the piece of software, you run it,

00:03:22.400 --> 00:03:24.160
It might think faster than a human being,

00:03:24.160 --> 00:03:28.360
I mean, computer clock speeds are much faster than human... brain speeds,

00:03:28.360 --> 00:03:28.860
Um

00:03:28.860 --> 00:03:32.420
It might discover improvements that it could make to itself very quickly,

00:03:32.420 --> 00:03:34.820
rewrite its own code, that's all in software,

00:03:34.820 --> 00:03:36.020
It could do that very quickly.

00:03:36.020 --> 00:03:40.140
I mean it could do that within a day, it could do that within the first second of being turned on.

00:03:40.660 --> 00:03:41.160
Um

00:03:42.400 --> 00:03:45.080
And then it becomes very interesting because...

00:03:45.880 --> 00:03:49.560
There's a possibility that this process could be recursive.

00:03:49.560 --> 00:03:53.880
That is to say, that once it's redesigned itself, it's now more intelligent,

00:03:53.880 --> 00:03:56.480
It's possible that at this new level of intelligence,

00:03:56.480 --> 00:03:58.960
It is better at AI design than it was.

00:03:58.960 --> 00:04:02.920
And it's able to come up with more improvements that it can make to itself.

00:04:03.000 --> 00:04:03.500
Umm

00:04:04.380 --> 00:04:05.660
And so on and so on...

00:04:05.660 --> 00:04:09.680
And so the thing continually increases in its intelligence.

00:04:09.880 --> 00:04:10.380
Umm...

00:04:11.180 --> 00:04:13.180
And this could be quite rapid.

00:04:13.180 --> 00:04:16.020
And then the question becomes:

00:04:16.020 --> 00:04:19.980
Is this subcritical or supercritical?

00:04:19.980 --> 00:04:24.180
So there's kind of an analogy here with a nuclear material, right?

00:04:24.180 --> 00:04:27.460
If you've got some fissile material, say it's uranium or something,

00:04:27.460 --> 00:04:31.300
every time the atom, uh, decays,

00:04:31.300 --> 00:04:36.180
the nucleus splits, releases some energy, it releases three fast neutrons I think.

00:04:36.180 --> 00:04:39.380
And those three neutrons go off into the material,

00:04:39.380 --> 00:04:42.260
and may hit other nuclei and cause them to fuse, [probably a mistake, they decay rather than fuse]

00:04:42.260 --> 00:04:44.260
each of which gives off three of their own.

00:04:44.260 --> 00:04:46.060
So the question is:

00:04:46.060 --> 00:04:49.860
For each decay of a nucleus,

00:04:49.860 --> 00:04:53.020
how many further decays happen as a consequence of that?

00:04:53.020 --> 00:04:58.060
If the number is less than one, then you'll get a bit of a chain reaction that then fizzles out,

00:04:58.060 --> 00:05:01.540
because on average, for everyone that splits, it's creating less than one new one.

00:05:01.540 --> 00:05:04.660
If it's exactly one you'll get a self-sustaining reaction,

00:05:04.660 --> 00:05:07.820
where the thing is like a nuclear power plant,

00:05:07.820 --> 00:05:11.620
where each thing sets off one other one on average,

00:05:11.620 --> 00:05:13.980
and so the thing just keeps going steadily.

00:05:13.980 --> 00:05:17.740
If it's more than one, you'll get a runaway reaction

00:05:17.740 --> 00:05:20.220
in which the level of activity increases exponentially.

00:05:20.220 --> 00:05:23.900
If each fission event results in two more,

00:05:23.900 --> 00:05:26.300
then you've got 2,4,8,16,32...

00:05:26.300 --> 00:05:30.580
and you've got at best a melt-down, and at worst a nuclear bomb.

00:05:31.060 --> 00:05:31.560
Umm...

00:05:32.760 --> 00:05:41.800
And so, we don't know... what the shape... of the landscape is

00:05:41.800 --> 00:05:45.760
around any general intelligence that we might design.

