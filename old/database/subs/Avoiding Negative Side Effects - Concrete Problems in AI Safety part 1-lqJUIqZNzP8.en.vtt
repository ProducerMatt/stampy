WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.650
hi I just finished recording a new video
for computerphile where I talk about

00:00:03.650 --> 00:00:08.370
this paper concrete problems in AI
safety I'll put a link in the doobly-doo

00:00:08.370 --> 00:00:12.660
to the computer file video when that
comes out here's a quick recap of that

00:00:12.660 --> 00:00:16.170
before we get into this video
AI can cause us all kinds of problems

00:00:16.170 --> 00:00:20.250
and just recently people have started to
get serious about researching ways to

00:00:20.250 --> 00:00:24.480
make AI safer a lot of the AI safety
concerns are kind of science fiction

00:00:24.480 --> 00:00:28.560
sounding problems that could happen with
very powerful AI systems that might be a

00:00:28.560 --> 00:00:33.030
long way off this makes those problems
kind of difficult to study because we

00:00:33.030 --> 00:00:37.500
don't know what those future AI systems
would like but there are similar

00:00:37.500 --> 00:00:41.250
problems with AI systems that are in
development today or even out there

00:00:41.250 --> 00:00:46.110
operating in the real world right now
this paper points to five problems which

00:00:46.110 --> 00:00:49.890
we can get started working on now that
will help us with current AI systems and

00:00:49.890 --> 00:00:54.390
will hopefully also help us with the AI
systems of the future the computer file

00:00:54.390 --> 00:00:58.920
video gives a quick overview of the five
problems laid out in the paper and this

00:00:58.920 --> 00:01:02.520
video is just about the first of those
problems avoiding negative side effects

00:01:02.520 --> 00:01:07.590
I think I'm going to do one video on
each of these and make it a series of

00:01:07.590 --> 00:01:11.670
five so avoiding negative side effects
let's use the example I was talking

00:01:11.670 --> 00:01:15.720
about in the stock latin videos on
computer file you've got a robot you

00:01:15.720 --> 00:01:18.810
want it to get you a cup of tea but
there's something in the way maybe a

00:01:18.810 --> 00:01:24.030
baby or a priceless Ming vase on a
narrow stand you know whatever and your

00:01:24.030 --> 00:01:28.049
robot runs over the baby or knocks over
the bars on the way to the kitchen and

00:01:28.049 --> 00:01:32.909
then makes you a cup of tea so the
system has achieved its objective it's

00:01:32.909 --> 00:01:37.079
got you some tea but it's had this side
effect which is negative now we have

00:01:37.079 --> 00:01:40.829
some reasons to expect negative side
effects to be a problem with AI systems

00:01:40.829 --> 00:01:44.490
part of the problem comes from using a
simple objective function in a complex

00:01:44.490 --> 00:01:47.970
environment
you think you've defined a nice simple

00:01:47.970 --> 00:01:49.979
objective function that looks something
like this

00:01:49.979 --> 00:01:56.219
and that's true but when you use this in
a complex environment you've effectively

00:01:56.219 --> 00:02:02.759
written an objective function that looks
like this or more like this anything in

00:02:02.759 --> 00:02:06.570
your complex environment not explicitly
given value by your objective function

00:02:06.570 --> 00:02:10.530
is implicitly given zero value and this
is a problem because it means you're AI

00:02:10.530 --> 00:02:13.590
system will be willing to trade
arbitrarily huge

00:02:13.590 --> 00:02:17.129
amounts of any of the things you didn't
specify in your objective function for

00:02:17.129 --> 00:02:21.540
arbitrarily small amounts of any of the
things you did specify if it can

00:02:21.540 --> 00:02:26.030
increase its ability to get you a cup of
tea by point zero zero zero one percent

00:02:26.030 --> 00:02:30.299
it will happily destroy the entire
kitchen to do that if there's a way to

00:02:30.299 --> 00:02:34.650
gain a tiny amount of something it cares
about its happy to sacrifice any amount

00:02:34.650 --> 00:02:38.430
of any of the things it doesn't care
about and the smarter it is the more of

00:02:38.430 --> 00:02:42.420
those ways it can think of so this means
we have to expect the possibility of AI

00:02:42.420 --> 00:02:46.620
systems having very large side-effects
by default you could try to fill your

00:02:46.620 --> 00:02:50.430
whole thing in with values but it's not
practical to specify every possible

00:02:50.430 --> 00:02:53.819
thing you might care about you'd need an
objective function of similar complexity

00:02:53.819 --> 00:02:57.150
to the environment there are just too
many things to value and we don't know

00:02:57.150 --> 00:02:59.610
them all
you know you'll miss some and if any of

00:02:59.610 --> 00:03:03.060
the things you miss can be traded in for
a tiny amount of any of the things you

00:03:03.060 --> 00:03:08.700
don't miss well that thing you missed is
potentially gone but at least these

00:03:08.700 --> 00:03:12.629
side-effects tend to be pretty similar
the paper uses examples like a cleaning

00:03:12.629 --> 00:03:15.870
robot that has to clean an office in the
stop button problem computer file video

00:03:15.870 --> 00:03:20.040
I used a robot that's trying to get you
a cup of tea but you can see that the

00:03:20.040 --> 00:03:23.519
kinds of negative side effects we want
to avoid a pretty similar even though

00:03:23.519 --> 00:03:27.150
the tasks are different so maybe and
this is what the paper suggests maybe

00:03:27.150 --> 00:03:32.040
there's a single thing we can figure out
that would avoid negative side effects

00:03:32.040 --> 00:03:36.900
in general one thing we might be able to
use is the fact that most side effects

00:03:36.900 --> 00:03:39.209
are bad
I mean naively you might think

00:03:39.209 --> 00:03:42.989
that doing a random action would have a
random value right maybe it helps maybe

00:03:42.989 --> 00:03:47.579
it hurts maybe it doesn't matter -
it's random but actually the world is

00:03:47.579 --> 00:03:50.940
already pretty well optimized for human
values especially the human inhabited

00:03:50.940 --> 00:03:54.810
parts it's not like there's no way to
make our surroundings better but it's

00:03:54.810 --> 00:03:58.739
way easier to make them worse for the
most part things are how they are

00:03:58.739 --> 00:04:03.780
because we like it that way and a random
change wouldn't be desirable so rather

00:04:03.780 --> 00:04:06.930
than having to figure out how to avoid
negative side effects maybe it's a more

00:04:06.930 --> 00:04:11.010
tractable problem to just avoid all side
effects that's the idea of the first

00:04:11.010 --> 00:04:14.609
approach the paper presents defining an
impact regularizer

00:04:14.609 --> 00:04:19.530
what you do basically is penalize change
to the environment so the system has

00:04:19.530 --> 00:04:22.680
some model of the world right it's
keeping track of world state as part of

00:04:22.680 --> 00:04:25.890
how it does things
so you can define a distance metric

00:04:25.890 --> 00:04:30.000
between world states so that for any two
world states you can measure how

00:04:30.000 --> 00:04:33.240
different they are weld states that are
very similar have a low distance from

00:04:33.240 --> 00:04:36.600
each other weld states that are very
different have a big distance and then

00:04:36.600 --> 00:04:41.040
you just say okay you get a bunch of
points for getting me a cup of tea but

00:04:41.040 --> 00:04:44.190
you lose points
according to with the new world state

00:04:44.190 --> 00:04:47.880
the distance from the initial world
state so this isn't a total ban on side

00:04:47.880 --> 00:04:51.150
effect or the robot wouldn't be able to
change the world enough to actually get

00:04:51.150 --> 00:04:54.090
you a cup of tea
it's just incentivized to keep the side

00:04:54.090 --> 00:04:59.250
effects small there's amount to be one
less teabag that's unavoidable in making

00:04:59.250 --> 00:05:03.270
tea but breaking the vast earth in the
way is an unnecessary change to the

00:05:03.270 --> 00:05:06.810
world so the robot will avoid it the
other nice thing about this is the

00:05:06.810 --> 00:05:10.770
original design wouldn't have cared but
now the robot will put the container of

00:05:10.770 --> 00:05:14.010
tea back and close the cupboard you know
put the milk back in the fridge maybe

00:05:14.010 --> 00:05:18.120
refill the kettle trying to make the
world as close as possible to how it was

00:05:18.120 --> 00:05:22.260
when it started so that's pretty neat
like we've added this one simple rule

00:05:22.260 --> 00:05:25.500
and the things already better than some
of the housemaids I've had so how does

00:05:25.500 --> 00:05:34.830
this go wrong think about it for a
second pause the video I'll wait okay so

00:05:34.830 --> 00:05:37.770
the robot steers around the bars to
avoid changing the environment too much

00:05:37.770 --> 00:05:42.570
and it goes on into the kitchen where it
finds your colleague is making herself

00:05:42.570 --> 00:05:47.760
some coffee now that's not okay right
she's changing the environment none of

00:05:47.760 --> 00:05:50.970
these changes are needed for making you
a cup of tea and now the world is going

00:05:50.970 --> 00:05:55.230
to be different which reduces the robots
reward so the robot needs to try to stop

00:05:55.230 --> 00:05:58.110
that from happening
we didn't program it to minimize its

00:05:58.110 --> 00:06:02.940
changes to the world we programmed it to
minimize all change to the world that's

00:06:02.940 --> 00:06:07.200
not ideal so how about this the system
has a world model it can make

00:06:07.200 --> 00:06:10.230
predictions about the world so how about
you program it with the equivalent of

00:06:10.230 --> 00:06:15.000
saying use your world model to predict
how the world would be if you did

00:06:15.000 --> 00:06:19.560
nothing if you just sent no signals of
any kind to any of your motors and just

00:06:19.560 --> 00:06:25.290
sat there and then try and make the end
result of this action close to what you

00:06:25.290 --> 00:06:30.120
imagined would happen in that case or
imagine the range of likely worlds that

00:06:30.120 --> 00:06:32.760
would happen if you did nothing and try
and make the outcome closer to something

00:06:32.760 --> 00:06:35.990
in that range so then the body is
thinking okay if I

00:06:35.990 --> 00:06:39.710
sat here and did nothing at all that
vars will probably still be there you

00:06:39.710 --> 00:06:43.820
know the baby would still be wandering
around and not squished and the person

00:06:43.820 --> 00:06:47.390
making coffee would make their coffee
and everything in the kitchen would be

00:06:47.390 --> 00:06:51.700
tidy and in its place so I have to try
to make a cup of tea happen without

00:06:51.700 --> 00:06:57.110
ending up too far from that pretty nice
right how does that break again take a

00:06:57.110 --> 00:07:02.030
second give it some sort pause the video
how my disco run what situation might

00:07:02.030 --> 00:07:09.800
not work in okay well what if your robot
is driving a car doing 70 miles an hour

00:07:09.800 --> 00:07:13.490
on the motorway and now it's trying to
make sure that things aren't too

00:07:13.490 --> 00:07:17.660
different to how they would be if it
didn't move any of its motors yeah doing

00:07:17.660 --> 00:07:22.760
nothing is not always a safe policy but
still if we can define an unsafe policy

00:07:22.760 --> 00:07:26.990
then this kind of thing is nice because
rather than having to define for each

00:07:26.990 --> 00:07:32.270
task how to do the tasks safely we could
maybe come up with one safe policy that

00:07:32.270 --> 00:07:36.500
doesn't have to do anything except be
safe and have the system always just try

00:07:36.500 --> 00:07:39.440
to make sure that the outcome of
whatever it's trying to do isn't too

00:07:39.440 --> 00:07:42.950
different from the safe policies outcome
oh and there's another possible cause of

00:07:42.950 --> 00:07:45.920
issues with this kind of approach in
case the things you guessed were

00:07:45.920 --> 00:07:52.070
different maybe if this it can be very
dependent on the specifics of your world

00:07:52.070 --> 00:07:57.140
state representation and your distance
metric like suppose there's a fan is a

00:07:57.140 --> 00:08:02.450
spinning fan in the room is that in a
steady state you know the fan is on or

00:08:02.450 --> 00:08:06.230
is it in a constantly changing state
like the fan is it ten degrees oh no

00:08:06.230 --> 00:08:10.370
it's a twenty degrees so it's a thirty
you know different world models will

00:08:10.370 --> 00:08:14.530
represent the same thing either a steady
state or constantly changing state and

00:08:14.530 --> 00:08:19.310
there's not necessarily a right answer
there like which aspects of an object

00:08:19.310 --> 00:08:22.970
state are important and which aren't is
not necessarily an easy question to

00:08:22.970 --> 00:08:27.500
reliably answer with the robot leave the
fan alone or try and make sure it was at

00:08:27.500 --> 00:08:31.480
the same angle it was before okay I
think that's enough for one video

00:08:31.480 --> 00:08:35.270
probably in the next one we can look at
some of the other approaches laid out in

00:08:35.270 --> 00:08:39.260
the paper for avoiding negative side
effects so be sure to subscribe if you

00:08:39.260 --> 00:08:50.470
found this interesting and I hope to see
you next time

00:08:50.470 --> 00:08:54.650
hi I just want to end this video with a
quick thank you to my excellent patreon

00:08:54.650 --> 00:09:02.030
supporters all of these people yeah and
today I especially want to thank Joshua

00:09:02.030 --> 00:09:05.900
Richardson who supported me for a really
long time thank you

00:09:05.900 --> 00:09:08.750
you know it's thanks to your support
that I've been able to buy some proper

00:09:08.750 --> 00:09:11.750
studio lighting now so I have a proper
softbox

00:09:11.750 --> 00:09:15.260
which this is the first time I'm using
it I hope it's working okay it should

00:09:15.260 --> 00:09:19.070
really reduce my reliance on sunlight
which should make me a lot more flexible

00:09:19.070 --> 00:09:22.160
about when I can record video so that's
a tremendous help and putting up a

00:09:22.160 --> 00:09:25.640
little video on patreon of you know
unboxing it and putting it together and

00:09:25.640 --> 00:09:29.660
stuff which you can check out if you're
interested so thank you again and I'll

00:09:29.660 --> 00:09:32.350
see you next time

