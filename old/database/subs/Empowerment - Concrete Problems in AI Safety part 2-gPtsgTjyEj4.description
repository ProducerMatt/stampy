Maybe AI systems would be safer if they avoid gaining too much control over their environment? How might that work?

This is a follow-up to this earlier video: https://youtu.be/lqJUIqZNzP8

The paper 'Concrete Problems in AI Safety': https://arxiv.org/pdf/1606.06565.pdf
A book chapter about Empowerment: https://arxiv.org/pdf/1310.1863.pdf

Prof Brailsford's Information Theory Videos: https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6

Thanks to my amazing Patreon Supporters:
Sara Tjäder
Jason Strack
Chad Jones
Ichiro Dohi
Stefan Skiles
Katie Byrne
Ziyang Liu
Jordan Medina
James McCuen
Joshua Richardson
Fabian Consiglio
Jonatan R
Øystein Flygt
Björn Mosten
Michael Greve
robertvanduursen 
The Guru Of Vision
Fabrizio Pisani
Alexander Hartvig Nielsen
Volodymyr 
Peggy Youell
Konstantin Shabashov
Almighty Dodd
DGJono 
Matthias Meger
Scott Stevens
Emilio Alvarez
Benjamin Aaron Degenhart
Michael Ore
Robert Bridges
Dmitri Afanasjev
Brian Sandberg
Einar Ueland
Lo Rez
C3POehne
https://www.patreon.com/robertskmiles