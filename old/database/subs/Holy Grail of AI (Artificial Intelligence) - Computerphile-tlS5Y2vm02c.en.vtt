WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.600
Right. So, last time, which was quite a while ago, we were talking about intelligence in general

00:00:06.600 --> 00:00:10.880
and the way that you can model intelligence as an optimization process

00:00:10.880 --> 00:00:14.400
- This is the hill climbing algorithm.
- Yeah, that was an example we gave.

00:00:14.400 --> 00:00:22.700
We were using evolution as an example of an optimizing algorithm, or an optimizing system anyway.

00:00:22.700 --> 00:00:26.380
And then we were using that as a way of talking about other types of intelligence.

00:00:26.380 --> 00:00:30.200
We talked about chess AI very briefly. That kind of thing.

00:00:30.300 --> 00:00:36.280
So then the question is:  What's the difference between the type of AI that we have now--

00:00:36.280 --> 00:00:42.740
the type of AI that might play chess, drive a car, or win jeopardy or whatever--

00:00:42.880 --> 00:00:54.560
versus the ideas that we have of AI in the future? The kind of science fiction AI's that are what you would call true AI.

00:00:57.740 --> 00:01:02.960
What is it that really makes the difference? Is it just a matter of power, or is there something else?

00:01:04.800 --> 00:01:16.040
And one real distinguishing factor is generality. And what that means is how broad a set of domains

00:01:16.280 --> 00:01:27.620
can it optimize in. So if you take a chess AI, it's very intelligent in the domain of chess, and it is absolutely

00:01:27.620 --> 00:01:34.320
useless in almost any other domain. If you put a chess AI in a google self driving car not only can it not

00:01:34.380 --> 00:01:40.260
drive the car it doesn't have the concept of what a car is. It doesn't have any of the necessary architecture

00:01:40.460 --> 00:01:45.840
cognitive architecture to drive a car. And vice versa right? The google car can't play chess.

00:01:45.840 --> 00:01:47.280
And it can't win at jeopardy.

00:01:47.280 --> 00:01:52.000
Where as we have a working example of a general intelligence.

00:01:52.000 --> 00:01:54.000
Which is human intelligence.

00:01:54.020 --> 00:01:54.520
Right?

00:01:54.520 --> 00:01:57.080
Human brains can do a lot of different things.

00:01:57.080 --> 00:01:59.080
In a lot of different domains.

00:01:59.080 --> 00:01:59.740
Gulp.

00:02:00.020 --> 00:02:03.060
Including brand new domains.

00:02:03.060 --> 00:02:06.400
The domains we didn't evolve for particularly.

00:02:06.400 --> 00:02:07.480
So in fact chess ,right?

00:02:07.480 --> 00:02:09.480
We invented chess, we invented driving.

00:02:09.480 --> 00:02:11.480
And then we learned to become good at them.

00:02:11.480 --> 00:02:16.840
So, a general intelligence is in a sense a different class of thing.

00:02:16.860 --> 00:02:23.900
Because it's a single optimization system that's able to optimize in a very broad variety of different domains.

00:02:24.560 --> 00:02:28.980
And if we could build an artificial general intelligence.

00:02:28.980 --> 00:02:31.740
That's kind of the holy grail of AI research

00:02:31.740 --> 00:02:37.900
That you have a single program or a single system that is able to solve any problem that we throw at it

00:02:37.900 --> 00:02:40.060
or at least tackle any problem that we throw at it.

00:02:40.060 --> 00:02:43.980
-Recently Pr Brailsford ... the idea of the Turing test

00:02:43.980 --> 00:02:47.800
That strikes me from what you're saying is that's a very specific domain

00:02:47.800 --> 00:02:49.900
pretending to be human talking.

00:02:49.900 --> 00:02:53.380
-Yes, in a sense it's a very specific domain.

00:02:53.560 --> 00:02:59.600
The Turing test is necessary but not sufficient test for general intelligence.

00:02:59.600 --> 00:03:01.600
Hum.

00:03:01.600 --> 00:03:04.520
You could, it depends how you format your test, right

00:03:04.520 --> 00:03:09.580
because you could say well, if the AI has to pretend to be human, convincingly

00:03:09.580 --> 00:03:14.740
Turing's original test was only in a brief conversation using on a text

00:03:14.740 --> 00:03:21.780
But you could say, to convince me you're human : tell me what move I should make in this chess game.

00:03:21.780 --> 00:03:26.340
To convince me you're human, tell me how I would respond in this driving situation

00:03:26.340 --> 00:03:27.900
or what's the answer to this jeopardy question?

00:03:27.900 --> 00:03:32.140
So you can in a Turing test deliberately test a wide variety of other domains.

00:03:32.140 --> 00:03:36.220
But in general, conversation is one domain

00:03:36.220 --> 00:03:40.560
Hum, yeah you could formulate a true Turing test, in that way

00:03:40.560 --> 00:03:43.920
but it would get longer and be more, sort of, regressive.

00:03:43.920 --> 00:03:49.320
One more way of thinking about general intelligence is a domain specific intelligence

00:03:49.320 --> 00:03:53.820
but where the domain is the world or physical reality.

00:03:53.820 --> 00:03:58.120
And if you can reliably optimize the world itself.

00:03:58.120 --> 00:04:00.760
That is in a sense what general intelligence does.

00:04:00.760 --> 00:04:05.100
-Is that like humans have been changing the world to meet their needs?

00:04:05.100 --> 00:04:09.060
-Absolutely, so when you say changing the world

00:04:09.060 --> 00:04:12.460
Obviously we've been changing the world on a very grand scale

00:04:12.560 --> 00:04:20.240
but everything that humans do in the real world is in the sense changing the world to be better optimized to them,

00:04:20.240 --> 00:04:20.900
right.

00:04:20.900 --> 00:04:25.960
Like if I'm thirsty and there's a drink over there then picking it up and putting it to my lips and drinking.

00:04:25.960 --> 00:04:29.740
I'm changing the world to improve my hydration levels which is something that I value

00:04:29.740 --> 00:04:32.360
So I'm, sort of, optimizing

00:04:32.360 --> 00:04:35.640
I am using my intelligence to optimize the world around me

00:04:35.640 --> 00:04:39.180
in a very abstract sense. But also quite practically.

00:04:39.180 --> 00:04:42.360
-But on bigger scale, as you say on a grander scale,

00:04:42.360 --> 00:04:49.320
building it down and irrigating a field , putting a pipe to your house and then I'll need to have a tab.

00:04:49.320 --> 00:04:51.720
-Yep. -It's doing the same thing but on a grander scale.

00:04:51.720 --> 00:04:54.700
-Right, and there's no hard boundary between these two things.

00:04:54.700 --> 00:04:59.640
It's the same basic mechanism at work.

00:04:59.640 --> 00:05:03.020
The idea that you want things to be in somewhere different from where they are

00:05:03.020 --> 00:05:06.820
So you use your intelligence to come up with a series of actions or a plan,

00:05:06.820 --> 00:05:10.680
that you can implement, that will better satisfies your values.

00:05:10.680 --> 00:05:12.680
And that's

00:05:13.480 --> 00:05:17.180
that's what a true AI, a general AI would do as well.

00:05:17.920 --> 00:05:19.800
So you can see the

00:05:19.820 --> 00:05:22.920
the metaphore to optimization is still there, right.

00:05:22.920 --> 00:05:29.560
You've got, this vast state space, which is all possible states of the world

00:05:29.560 --> 00:05:31.940
Remember before, we were talking about dimensionality

00:05:31.940 --> 00:05:34.340
and how it's kind of a problem if you have too many dimensions.

00:05:34.340 --> 00:05:36.340
(So when we have a two-dimensional space...)

00:05:37.020 --> 00:05:42.520
This is what kills basic implementation of general AI off the bat

00:05:42.520 --> 00:05:45.260
because the world is so very very complicated.

00:05:45.260 --> 00:05:47.880
It's an exceptionally high dimensional space.

00:05:48.020 --> 00:05:51.160
With the "I'm drinking a drink" example, you've got the same thing again.

00:05:51.880 --> 00:05:55.000
You've got a state of the world which is a place in this space

00:05:55.000 --> 00:05:58.720
and you've got another state of the world which is the state in which I've just had a drink.

00:05:58.720 --> 00:06:01.700
And one of them is higher in my utility function.

00:06:01.700 --> 00:06:05.860
It's higher in my ordering, my preference ordering of the world states.

00:06:05.860 --> 00:06:10.860
So I'm going to try and move, I'm going to try to shift the world

00:06:10.860 --> 00:06:14.020
from places that are lower in my preference ordering  to places that are higher.

00:06:16.140 --> 00:06:19.820
And that gives you a way to express

00:06:19.820 --> 00:06:26.160
the making of plans and the implementing of actions and intelligent behavior in the real world

00:06:26.160 --> 00:06:28.440
in mathematical terms.

00:06:29.860 --> 00:06:33.700
It's not, you can't just implement it, because hum

00:06:35.000 --> 00:06:37.060
because of this enormous dimensionality problem.

00:06:37.060 --> 00:06:41.320
-All these dimensions, if you try to break force infinite dimensions, you're going to fall out very quickly.

00:06:41.320 --> 00:06:43.200
-Yeah, yeah, immediately.

00:06:43.300 --> 00:06:45.240
-Changing the world.

00:06:45.500 --> 00:06:46.240
Right,

00:06:46.980 --> 00:06:48.900
and if that sounds a little bit threatening

00:06:49.400 --> 00:06:50.660
uh it is.

00:06:50.660 --> 00:06:52.780
(laughs)

00:06:56.040 --> 00:06:59.280
We'd like to thank audible.com for sponsoring this computerphile video

00:06:59.280 --> 00:07:01.280
and if you like books go over to :

00:07:03.420 --> 00:07:05.520
There's a chance to try out a book for free.

00:07:05.520 --> 00:07:10.560
Now I spoke to Rob who's in this computerphile video and asked him what book he would recommand

00:07:10.560 --> 00:07:15.280
and he says "Superintelligence" by Nick Bostrom is the one to check out.

00:07:15.280 --> 00:07:18.040
Particularly on this subject of artificial intelligence

00:07:18.040 --> 00:07:21.180
We've got more to come on that subject on computerphile as well, so visit :

00:07:21.180 --> 00:07:23.420
audible.com/computerphile

00:07:23.440 --> 00:07:29.260
Check out "Superintelligence" and thanks once again to audible.com for sponsoring this computerphile video.

